{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "State Regularized RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOSRGNgywMr6Bdi6vOPzEyR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelly-hateva/tardis/blob/master/notebooks/State_Regularized_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbBftCsLWw_p"
      },
      "source": [
        "Check runtime resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYo9XQBnWYp0"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y0Pyq0G0A4W"
      },
      "source": [
        "Prerequisities: Access data in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvCTwouDz-zs"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "MOUNT_POINT = \"/content/drive/\"\n",
        "DATA_DIR = MOUNT_POINT + \"My Drive/Colab Notebooks/Thesis-Data\"\n",
        "MODELS_DIR = MOUNT_POINT + \"My Drive/Colab Notebooks/Thesis-Models\"\n",
        "drive.mount(MOUNT_POINT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqVIc7pe0Xxw"
      },
      "source": [
        "# Used, if we use local environment\n",
        "# DATA_DIR = \"Thesis-Data\"\n",
        "# MODELS_DIR = \"Thesis-Models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDQZ5n1R0aWe"
      },
      "source": [
        "Import libraries, set random seed and device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u56lsr5U0W7h"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import pickle\n",
        "import copy\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "from torch import nn, optim, utils\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def seed_torch(seed=666):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  numpy.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_torch()\n",
        "\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIdpnf0yerS5"
      },
      "source": [
        "State Regularized RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ed6ar_Jc8dM"
      },
      "source": [
        "class SRRNN(nn.Module):\n",
        "\n",
        "  r\"\"\" https://arxiv.org/pdf/1901.08817.pdf\n",
        "       https://github.com/deepsemantic/sr-rnns\n",
        "  Applies a single-layer State Regularized RNN to an input sequence.\n",
        "\n",
        "  If :attr:`mode` is ``'rnn'``, then for each element in the input sequence computes the function\n",
        "\n",
        "    .. math::\n",
        "        h_t' = \\text{tanh}(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})\n",
        "\n",
        "    where :math:`x_t` is the input at time `t`, :math:`h_{(t-1)}` is the hidden state\n",
        "    at time `t-1` or the initial hidden state at time `0`.\n",
        "    If :attr:`nonlinearity` is ``'relu'``, then `ReLU` is used instead of `tanh`.\n",
        "\n",
        "  If :attr:`mode` is ``'gru'``, then for each element in the input sequence computes the function\n",
        "\n",
        "    .. math::\n",
        "      \\begin{array}{ll} \\\\\n",
        "        r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
        "        z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
        "        n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)} + b_{hn})) \\\\\n",
        "        h_t' = (1 - z_t) * n_t + z_t * h_{(t-1)}\n",
        "      \\end{array}\n",
        "\n",
        "    where :math:`x_t` is the input at time `t`, :math:`h_{(t-1)}` is the hidden state\n",
        "    at time `t-1` or the initial hidden state at time `0`.\n",
        "    :math:`r_t`, :math:`z_t`, :math:`n_t` are the reset, update, and new gates, respectively.\n",
        "    :math:`\\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.\n",
        "\n",
        "  In both modes, if :attr:`number_of_states` is ``None``, then the hidden state\n",
        "  at time `t` :math:`h_t` equals :math:`h_t'`. \n",
        "\n",
        "  Otherwise,\n",
        "\n",
        "    .. math::\n",
        "      \\begin{array}{ll} \\\\\n",
        "        \\alpha_{i} = \\frac{\\exp(- \\Vert{h_t' - s_i}\\Vert / \\tau)}{\\sum_{i=1}^{k} \\exp(- \\Vert{h_t' - s_i}\\Vert / \\tau)}\n",
        "        h_t = {\\sum_{i=1}^{k} \\alpha_{i} s_i}\n",
        "      \\end{array}\n",
        "\n",
        "    where :math:`\\{s_1, s_2, ..., s_k\\}` are the k learnable states. \n",
        "    :math:`\\alpha_{i}` is the probability of the RNN to transition to state i\n",
        "    given the vector :math:`h_t'` for which we write :math:`p_{h_t'}(i) = \\alpha_{i}`\n",
        "    :math:`\\tau` is a temperature parameter that can be used to anneal\n",
        "    the probabilistic state transition behavior. The lower :math:`\\tau` the\n",
        "    more :math:`\\alpha` resembles the one-hot encoding of a centroid. The\n",
        "    higher :math:`\\tau` the more uniform is :math:`\\alpha`\n",
        "\n",
        "    Args:\n",
        "        input_size: The number of expected features in the input `x`\n",
        "        hidden_size: The number of features in the hidden state `h`\n",
        "        mode: The RNN mode to use.\n",
        "          Can be either ``'rnn'`` or ``'gru'``. Default: ``'rnn'``\n",
        "        nonlinearity: The non-linearity to use if :attr:`mode` is ``'rnn'``.\n",
        "          Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``\n",
        "        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
        "            Default: ``True``\n",
        "        number_of_states: The number of learnable finite states.\n",
        "          If ``None`` then the stohastic component is not used. Default: ``None``\n",
        "        temperature: The temperature parameter. Default: ``1.00``\n",
        "\n",
        "    Inputs: input, h_0\n",
        "        - **input** of shape `(batch, seq_len, input_size)`:\n",
        "          tensor containing the features of the input sequence.\n",
        "          The input can also be a packed variable length sequence.\n",
        "          See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
        "          :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
        "        - **h_0** of shape `(batch, hidden_size)`: tensor\n",
        "          containing the initial hidden state for each element in the batch.\n",
        "\n",
        "          If `h_0` is not provided, it defaults to zero.\n",
        "\n",
        "    Outputs: output, h_n\n",
        "        - **output** of shape `(batch, seq_len, hidden_size)`: tensor\n",
        "          containing the output features `(h_t)` from the RNN for each `t`.\n",
        "          If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
        "          given as the input, the output will also be a packed sequence.\n",
        "        - **h_n** of shape `(batch, hidden_size)`: tensor\n",
        "          containing the hidden state for `t = seq_len`.\n",
        "\n",
        "    Attributes:\n",
        "        If :attr:`mode` is ``'rnn'``\n",
        "          weight_ih: the learnable input-hidden weights,\n",
        "              of shape `(hidden_size, input_size)`\n",
        "          weight_hh: the learnable hidden-hidden weights,\n",
        "              of shape `(hidden_size, hidden_size)`\n",
        "          bias_ih: the learnable input-hidden bias,\n",
        "              of shape `(hidden_size)`\n",
        "          bias_hh: the learnable hidden-hidden bias,\n",
        "              of shape `(hidden_size)`\n",
        "        If :attr:`mode` is ``'gru'``\n",
        "          weight_ih : the learnable input-hidden weights\n",
        "              (W_ir|W_iz|W_in), of shape `(3*hidden_size, input_size)`\n",
        "          weight_hh : the learnable hidden-hidden weights\n",
        "              (W_hr|W_hz|W_hn), of shape `(3*hidden_size, hidden_size)`\n",
        "          bias_ih : the learnable input-hidden bias\n",
        "              (b_ir|b_iz|b_in), of shape `(3*hidden_size)`\n",
        "          bias_hh : the learnable hidden-hidden bias\n",
        "              (b_hr|b_hz|b_hn), of shape `(3*hidden_size)`\n",
        "        If :attr:`number_of_states` is not ``None``\n",
        "          states: the learnable finite number of states, of shape\n",
        "              `(number_of_states, hidden_size)\n",
        "\n",
        "    .. note::\n",
        "        All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n",
        "        where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n",
        "\n",
        "    Examples::\n",
        "        >>> rnn = SRRNN(10, 20)\n",
        "        >>> input = torch.randn(6, 3, 10)\n",
        "        >>> h_0 = torch.randn(6, 20)\n",
        "        >>> output, h_n = rnn(input, h_0)\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "    self, input_size, hidden_size, bias=True, mode='rnn', nonlinearity='tanh',\n",
        "    number_of_states=None, temperature=1.00\n",
        "  ):\n",
        "    super(SRRNN, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "\n",
        "    if mode == 'rnn':\n",
        "      self.rnn_cell = nn.RNNCell(\n",
        "        input_size, hidden_size, bias=bias, nonlinearity=nonlinearity\n",
        "      )\n",
        "    elif mode == 'gru':\n",
        "      self.rnn_cell = nn.GRUCell(\n",
        "        input_size, hidden_size, bias=bias\n",
        "      )\n",
        "    else:\n",
        "      raise ValueError(\"Unknown mode '{}'\".format(mode))\n",
        "\n",
        "    self.stohastic_component = False\n",
        "\n",
        "    if number_of_states:\n",
        "      self.stohastic_component = True\n",
        "      self.number_of_states = number_of_states\n",
        "\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "      self.states = nn.Parameter(\n",
        "        torch.Tensor(\n",
        "            self.number_of_states, hidden_size\n",
        "        )\n",
        "      )\n",
        "      self.temperature = temperature\n",
        "\n",
        "  def extra_repr(self):\n",
        "    s = ''\n",
        "    if 'number_of_states' in self.__dict__:\n",
        "      s = 'number_of_states={number_of_states}'\n",
        "      if 'temperature' in self.__dict__ and self.temperature != 1.00:\n",
        "        s += ', temperature={temperature}'\n",
        "\n",
        "    return s.format(**self.__dict__)\n",
        "\n",
        "  def permute_hidden(self, hidden, permutation, dim=0):\n",
        "    if permutation is None:\n",
        "      return hidden\n",
        "    return hidden.index_select(dim, permutation)\n",
        "\n",
        "  def forward(self, input, h_0=None):\n",
        "    orig_input = input\n",
        "\n",
        "    if isinstance(orig_input, nn.utils.rnn.PackedSequence):\n",
        "      input, batch_sizes, sorted_indices, unsorted_indices = input\n",
        "      max_batch_size = int(batch_sizes[0])\n",
        "    else:\n",
        "      max_batch_size, sorted_indices = input.size(0), None\n",
        "\n",
        "    if h_0 is None:\n",
        "      h_0 = torch.zeros(\n",
        "        max_batch_size, self.hidden_size, dtype=input.dtype, device=input.device\n",
        "      )\n",
        "    else:\n",
        "      # Each batch of the hidden state should match the input sequence that\n",
        "      # the user believes he/she is passing in.\n",
        "      h_0 = self.permute_hidden(h_0, sorted_indices)\n",
        "\n",
        "    if isinstance(orig_input, nn.utils.rnn.PackedSequence):\n",
        "      output, hidden, transition_probabilities = self.forward_packed(input, batch_sizes, h_0)\n",
        "\n",
        "      if self.stohastic_component:\n",
        "        transition_probabilities = nn.utils.rnn.PackedSequence(\n",
        "          transition_probabilities, batch_sizes, sorted_indices, unsorted_indices\n",
        "        )\n",
        "\n",
        "      hidden = self.permute_hidden(hidden, unsorted_indices)\n",
        "      output = nn.utils.rnn.PackedSequence(\n",
        "        output, batch_sizes, sorted_indices, unsorted_indices\n",
        "      )\n",
        "\n",
        "      return output, hidden, transition_probabilities\n",
        "\n",
        "    return self.forward_tensor(input, h_0)\n",
        "\n",
        "  def forward_tensor(self, input, h_0):\n",
        "    output, h_t = [], h_0\n",
        "\n",
        "    if self.stohastic_component:\n",
        "      transition_probabilities = []\n",
        "\n",
        "    for t in range(input.size(1)):\n",
        "      result = self.forward_impl(input[:,t,:], h_t)\n",
        "      if self.stohastic_component:\n",
        "        transition_probs, h_t = result\n",
        "        transition_probabilities.append(transition_probs)\n",
        "      else:\n",
        "        h_t = result\n",
        "      output.append(h_t)\n",
        "\n",
        "    output = torch.stack(output).permute(1, 0, 2)\n",
        "\n",
        "    if self.stohastic_component:\n",
        "      return output, h_t, torch.stack(transition_probabilities).permute(1, 0, 2)\n",
        "    return output, h_t, None\n",
        "\n",
        "  def forward_packed(self, input, batch_sizes, h_0):\n",
        "    output, h_n, t, h_t = [], [], 0, h_0\n",
        "\n",
        "    if self.stohastic_component:\n",
        "      transition_probabilities = []\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "      batch_size = int(batch_size)\n",
        "\n",
        "      h_t, h_n_ = h_t[:batch_size], h_t[batch_size:]\n",
        "      h_n.append(h_n_)\n",
        "      result = self.forward_impl(input[t : t + batch_size], h_t)\n",
        "\n",
        "      if self.stohastic_component:\n",
        "        transition_probs, h_t = result\n",
        "        transition_probabilities.append(transition_probs)\n",
        "      else:\n",
        "        h_t = result\n",
        "\n",
        "      output.append(h_t)\n",
        "      t += batch_size\n",
        "\n",
        "    h_n.append(h_t)\n",
        "    h_n.reverse()\n",
        "\n",
        "    output, h_n = torch.cat(output), torch.cat(h_n)\n",
        "\n",
        "    if self.stohastic_component:\n",
        "      return output, h_n, torch.cat(transition_probabilities)\n",
        "    return output, h_n, None\n",
        "\n",
        "  def forward_impl(self, input, h_t):\n",
        "    h_t_ = self.rnn_cell(input, h_t)\n",
        "    if self.stohastic_component:\n",
        "      # print(\"self.states \", self.states.size(), self.states)\n",
        "      # print(\"h_t_ \", h_t_.size(), h_t_)\n",
        "      # print(\"h_t_.unsqueeze(1) \", h_t_.unsqueeze(1).size(), h_t_.unsqueeze(1))\n",
        "      # print(\"self.states - h_t_.unsqueeze(1) \", (self.states - h_t_.unsqueeze(1)).size(), self.states - h_t_.unsqueeze(1))\n",
        "      # print(\"torch.pow(self.states - h_t_.unsqueeze(1), 2) \", torch.pow(self.states - h_t_.unsqueeze(1), 2).size(), torch.pow(self.states - h_t_.unsqueeze(1), 2))\n",
        "      # print(\"- torch.pow(self.states - h_t_.unsqueeze(1), 2).sum(2) \", - torch.pow(self.states - h_t_.unsqueeze(1), 2).sum(2))\n",
        "      # print(\"(- torch.pow(self.states - h_t_.unsqueeze(1), 2).sum(2)) / self.temperature\",(- torch.pow(self.states - h_t_.unsqueeze(1), 2).sum(2)) / self.temperature)\n",
        "      transition_probs = self.softmax(\n",
        "        (- torch.pow(self.states - h_t_.unsqueeze(1), 2).sum(2)) / self.temperature\n",
        "      )\n",
        "      return transition_probs, torch.matmul(transition_probs, self.states)\n",
        "    return h_t_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnLM-uoyjSDJ"
      },
      "source": [
        "Test for pack padded sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kndaEv8LjTB7"
      },
      "source": [
        "with torch.no_grad():\n",
        "  for _ in range(1024):\n",
        "\n",
        "    batch_size = random.randint(1, 15)\n",
        "    seq_length = random.randint(1, 15)\n",
        "    input_size = random.randint(1, 5)\n",
        "    hidden_size = random.randint(1, 5)\n",
        "    bias = bool(random.getrandbits(1))\n",
        "    nonlinearity = random.choice(['tanh', 'relu'])\n",
        "    mode = random.choice(['rnn', 'gru'])\n",
        "    number_of_states = random.randint(1, 15)\n",
        "    temperature = random.choice([1.00, 0.5, 0.1])\n",
        "\n",
        "    input = torch.rand(batch_size, seq_length, input_size).to(device)\n",
        "    h_0 = torch.rand(batch_size, hidden_size).to(device)\n",
        "\n",
        "    lengths = torch.randint(1, seq_length + 1, (batch_size,)).to(device)\n",
        "    packed_sequence = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "      input, lengths, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "\n",
        "    # without stohastic component\n",
        "    model = SRRNN(\n",
        "      input_size, hidden_size, bias=bias, mode=mode, nonlinearity=nonlinearity\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    output, h_n, _ = model(input, h_0)\n",
        "\n",
        "    assert h_n.size() == (batch_size, hidden_size)\n",
        "    assert output.size() == (batch_size, seq_length, hidden_size)\n",
        "\n",
        "    output_packed, h_n_packed, _ = model(packed_sequence, h_0)\n",
        "\n",
        "    assert h_n_packed.size() == (batch_size, hidden_size)\n",
        "    assert output_packed.data.size() == (lengths.sum().item(), hidden_size)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      assert torch.allclose(h_n_packed[i,:], output[i, lengths[i].item() - 1, :], atol=1e-07)\n",
        "\n",
        "    # with stohastic component\n",
        "    model = SRRNN(\n",
        "      input_size, hidden_size, bias=bias, mode=mode, nonlinearity=nonlinearity,\n",
        "      number_of_states=number_of_states, temperature=temperature\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    output, h_n, transition_probabilities = model(input, h_0)\n",
        "\n",
        "    assert h_n.size() == (batch_size, hidden_size)\n",
        "    assert output.size() == (batch_size, seq_length, hidden_size)\n",
        "    assert transition_probabilities.size() == (batch_size, seq_length, number_of_states)\n",
        "\n",
        "    output_packed, h_n_packed, transition_probabilities_packed = model(packed_sequence, h_0)\n",
        "\n",
        "    assert h_n_packed.size() == (batch_size, hidden_size)\n",
        "    assert output_packed.data.size() == (lengths.sum().item(), hidden_size)\n",
        "    assert transition_probabilities_packed.data.size() == (lengths.sum().item(), number_of_states)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      assert torch.allclose(h_n_packed[i,:], output[i, lengths[i].item() - 1, :], atol=1e-07)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qAaRQ6-0LOD"
      },
      "source": [
        "class NLNN(nn.Module):\n",
        "\n",
        "  def __init__(self, params):\n",
        "    super(NLNN, self).__init__()\n",
        "\n",
        "    # + 1 because of the padding with 0s\n",
        "    num_embeddings = params['num_embeddings'] + 1\n",
        "\n",
        "    if 'embedding_dim' in params:\n",
        "      embedding_dim = params['embedding_dim']\n",
        "      self.embeddings = nn.Embedding(\n",
        "        num_embeddings,\n",
        "        embedding_dim,\n",
        "        padding_idx = 0\n",
        "      )\n",
        "      input_size = embedding_dim\n",
        "    else:\n",
        "      # one hot encoding\n",
        "      self.embeddings = nn.Embedding(\n",
        "          num_embeddings,\n",
        "          num_embeddings,\n",
        "          padding_idx = 0\n",
        "      )\n",
        "      nn.init.eye_(self.embeddings.weight.data)\n",
        "      self.embeddings.weight.requires_grad = False\n",
        "      input_size = num_embeddings\n",
        "\n",
        "    if 'number_of_states' in params:\n",
        "      self.rnn = SRRNN(\n",
        "        input_size, params['hidden_size'], bias=params['bias'],\n",
        "        mode=params['mode'], nonlinearity=params['nonlinearity'],\n",
        "        number_of_states=params['number_of_states'],\n",
        "        temperature=params['temperature']\n",
        "      )\n",
        "    else:\n",
        "      self.rnn = SRRNN(\n",
        "        input_size, params['hidden_size'], bias=params['bias'],\n",
        "        mode=params['mode'], nonlinearity=params['nonlinearity']\n",
        "      )\n",
        "\n",
        "    self.linear = nn.Linear(\n",
        "      in_features=params['hidden_size'], out_features=2, bias=True\n",
        "    )\n",
        "\n",
        "  def forward(\n",
        "    self, x, length,\n",
        "    h_0=None, return_rnn_output=False, return_probabilities=False\n",
        "  ):\n",
        "    embedding_output = self.embeddings(x)\n",
        "    packed_sequence = nn.utils.rnn.pack_padded_sequence(\n",
        "      embedding_output, length, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "\n",
        "    if h_0 is not None:\n",
        "      rnn_output, h_n, transition_probabilities = self.rnn(packed_sequence, h_0=h_0)\n",
        "    else:\n",
        "      rnn_output, h_n, transition_probabilities = self.rnn(packed_sequence)\n",
        "\n",
        "    linear_output = self.linear(h_n)\n",
        "\n",
        "    if return_rnn_output and return_probabilities:\n",
        "      return linear_output, rnn_output, transition_probabilities\n",
        "    elif return_rnn_output:\n",
        "      return linear_output, rnn_output\n",
        "    elif return_probabilities:\n",
        "      return linear_output, transition_probabilities\n",
        "    else:\n",
        "      return linear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G01xTmxuj9Cr"
      },
      "source": [
        "Define the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOiy1Kys02Wf"
      },
      "source": [
        "class NLDataset(utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, dataset):\n",
        "    data, length, labels = dataset\n",
        "    self.data = torch.tensor(data).long().to(device)\n",
        "    self.length = torch.tensor(length).long().to(device)\n",
        "    self.labels = torch.tensor(labels).long().to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "      'x': self.data[idx],\n",
        "      'length': self.length[idx],\n",
        "      'y': self.labels[idx]\n",
        "    }\n",
        "\n",
        "def load_data(filename_data, filename_length, filename_labels):\n",
        "  return numpy.load(filename_data, allow_pickle=True), \\\n",
        "    numpy.load(filename_length, allow_pickle=True), \\\n",
        "    numpy.load(filename_labels, allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Mu0GuRvx84"
      },
      "source": [
        "Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUX8FKwB1SyD"
      },
      "source": [
        "def __evaluate(predictions, labels):\n",
        "  assert(len(predictions) == len(labels))\n",
        "\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "  for prediction, label in zip(predictions, labels):\n",
        "    if label == 1:\n",
        "      if prediction == 1:\n",
        "        tp += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        fp += 1\n",
        "      else:\n",
        "        tn += 1\n",
        "\n",
        "  if tp == 0:\n",
        "    if fn == 0 and fp == 0:\n",
        "      pr, r, f1 = 1, 1, 1\n",
        "    else:\n",
        "      pr, r, f1 = 0, 0, 0\n",
        "  else:\n",
        "    pr = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    f1 = 2 * ((pr * r) / (pr + r))\n",
        "\n",
        "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "  return tp, tn, fp, fn, pr, r, f1, accuracy\n",
        "\n",
        "def _evaluate(data_loader, model):\n",
        "  model.eval()\n",
        "\n",
        "  predictions, labels = [], []\n",
        "\n",
        "  for data in data_loader:\n",
        "    t0 = time.time()\n",
        "    result = model(data['x'], data['length'])\n",
        "    # print(\"res \", result)\n",
        "    # print(\"res + soft \", nn.Softmax(dim=1)(result))\n",
        "    argmax = result.argmax(dim=1).detach().cpu().numpy()\n",
        "    #print(\"argmax \", argmax)\n",
        "    predictions.extend(list(argmax))\n",
        "    labels.extend(list(data['y'].detach().cpu().numpy()))\n",
        "\n",
        "  return __evaluate(predictions, labels)\n",
        "\n",
        "def evaluate_model(data_loader, model, set_name):\n",
        "  tp, tn, fp, fn, pr, r, f1, acc = _evaluate(data_loader, model)\n",
        "  print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "    set_name, tp, tn, fp, fn, pr, r, f1, acc\n",
        "  ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1nCE4O7kqj2"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwx4fUK7kVkE"
      },
      "source": [
        "def model_summary(model):\n",
        "  print (model)\n",
        "  print()\n",
        "\n",
        "  print(\"Trainable parameters:\")\n",
        "  for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print (\" \", name)\n",
        "  print()\n",
        "\n",
        "  number_of_trainable_parameters = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad\n",
        "  )\n",
        "  print(\"Number of trainable parameters {0:,}\".format(\n",
        "    number_of_trainable_parameters\n",
        "  ))\n",
        "  print()\n",
        "\n",
        "def train_model(\n",
        "    model, params=None, params_path=None, model_path=None,\n",
        "    train_loader=None, dev_loader=None\n",
        "):\n",
        "  model.to(device)\n",
        "  model_summary(model)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "\n",
        "  best_accuracy, best_state_dict, best_epoch = 0, dict(), -1\n",
        "\n",
        "  # first_batch = next(iter(train_loader))\n",
        "  # print (first_batch)\n",
        "\n",
        "  for epoch in range(params['num_epochs']):\n",
        "    print('Epoch {}/{} : '.format(epoch, params['num_epochs']))\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.train() # set the model to training mode\n",
        "    #for data in [first_batch] * 1:\n",
        "    for data in train_loader:\n",
        "      model.zero_grad()\n",
        "      output = model(data['x'], data['length'])\n",
        "      batch_loss = loss_function(output, data['y'])\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    _, _, _, _, _, _, _, dev_accuracy = _evaluate(dev_loader, model)\n",
        "\n",
        "    if dev_accuracy > best_accuracy:\n",
        "      best_accuracy = dev_accuracy\n",
        "      best_state_dict = copy.deepcopy(model.state_dict())\n",
        "      best_epoch = epoch\n",
        "\n",
        "    print(\n",
        "      \"dev accuracy:{}\\ttime:{:.2f}s\"\n",
        "      .format(dev_accuracy, time.time() - t0)\n",
        "    )\n",
        "\n",
        "    # _, _, _, _, _, _, _, train_accuracy = _evaluate(train_loader, model)\n",
        "\n",
        "    # if train_accuracy > best_accuracy:\n",
        "    #   best_accuracy = train_accuracy\n",
        "    #   best_state_dict = copy.deepcopy(model.state_dict())\n",
        "    #   best_epoch = epoch\n",
        "\n",
        "    # print(\n",
        "    #   \"train accuracy:{}\\ttime:{:.2f}s\"\n",
        "    #   .format(train_accuracy, time.time() - t0)\n",
        "    # )\n",
        "\n",
        "    if params[\"save_after_each_epoch\"]:\t\n",
        "      torch.save(\t\n",
        "        {\t\n",
        "          \"epoch\": epoch,\t\n",
        "          \"model_state_dict\": model.state_dict(),\t\n",
        "          \"optimizer_state_dict\": optimizer.state_dict(),\t\n",
        "          \"loss\": loss_function,\t\n",
        "          \"best_accuracy\": best_accuracy,\t\n",
        "          \"best_epoch\": best_epoch,\t\n",
        "          \"best_state_dict\": best_state_dict\t\n",
        "        },\t\n",
        "        MODELS_DIR + model_path + str(epoch)\t\n",
        "      )\t\n",
        "      if params_path:\t\n",
        "        with open(MODELS_DIR + params_path, 'wb') as f:\t\n",
        "          pickle.dump(params, f)\n",
        "\n",
        "  model.load_state_dict(best_state_dict)\n",
        "  print(\"Best epoch {} and accuracy {}\".format(best_epoch, best_accuracy))\n",
        "\n",
        "  if model_path:\n",
        "    torch.save(model.state_dict(), MODELS_DIR + model_path)\n",
        "    if params_path:\n",
        "      with open(MODELS_DIR + params_path, 'wb') as f:\n",
        "        pickle.dump(params, f)\n",
        "\n",
        "  #evaluate_model([first_batch] * 1, model, \"train\")\n",
        "  evaluate_model(train_loader, model, \"train\")\n",
        "  evaluate_model(dev_loader, model, \"dev\")\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-anSMTog1LSb"
      },
      "source": [
        "train_data = load_data(\n",
        "  DATA_DIR + \"/numeral/train.data.npy\",\n",
        "  DATA_DIR + \"/numeral/train.length.npy\",\n",
        "  DATA_DIR + \"/numeral/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/numeral/dev.data.npy\",\n",
        "  DATA_DIR + \"/numeral/dev.length.npy\",\n",
        "  DATA_DIR + \"/numeral/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/numeral/test.data.npy\",\n",
        "  DATA_DIR + \"/numeral/test.length.npy\",\n",
        "  DATA_DIR + \"/numeral/test.labels.npy\"\n",
        ")\n",
        "\n",
        "alphabet = {}\n",
        "with open(DATA_DIR + \"/numeral/alphabet.dict\", \"rb\") as f:\n",
        "  alphabet = pickle.load(f)\n",
        "\n",
        "params = {\n",
        "  'batch_size': 20,\n",
        "  'lr': 0.001,\n",
        "  'weight_decay': 1e-5,\n",
        "  'num_epochs': 100,\n",
        "  'num_embeddings': len(alphabet),\n",
        "  #'embedding_dim': 10,\n",
        "  'mode': 'rnn',\n",
        "  'nonlinearity': 'relu',\n",
        "  'hidden_size': 10,\n",
        "  'bias': False,\n",
        "  'save_after_each_epoch': False\n",
        "}\n",
        "\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "model = NLNN(params)\n",
        "train_model(\n",
        "  model,\n",
        "  params=params,\n",
        "  params_path=\"/numeral/remove-softmax-rnn.model.params\",\n",
        "  model_path=\"/numeral/remove-softmax-rnn.model.pt\",\n",
        "  train_loader=train_loader, dev_loader=dev_loader\n",
        ")\n",
        "#evaluate_model(dev_loader, model, \"dev\")\n",
        "evaluate_model(test_loader, model, \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuAHR09WBXtD"
      },
      "source": [
        "def load_model(params_path=None, model_path=None):\n",
        "  with open(MODELS_DIR + params_path, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "  model = NLNN(params)\n",
        "  checkpoint = torch.load(MODELS_DIR + model_path, map_location=device)\n",
        "  if 'model_state_dict' in checkpoint:\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  else:\n",
        "    model.load_state_dict(checkpoint)\n",
        "  model.to(device)\n",
        "  return model, params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HVfo4wbNPWD"
      },
      "source": [
        "Get vectors from pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgFdeeanNONL"
      },
      "source": [
        "def get_model_vectors(model, params, dataset_loader):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    vectors, dataset_size = [], 0\n",
        "\n",
        "    for data in train_loader:\n",
        "      x, length = data['x'], data['length']\n",
        "      dataset_size += x.size(0)\n",
        "      _, rnn_output = model(x, length, return_rnn_output=True)\n",
        "      softmax_output =  model.softmax(model.linear(rnn_output.data))\n",
        "      vectors.extend(\n",
        "        torch.cat((rnn_output.data, softmax_output), dim=1).cpu().detach().numpy()\n",
        "      )\n",
        "\n",
        "    h_0 = torch.zeros((1, params['hidden_size']), device=device)\n",
        "    h_0 = torch.cat((h_0, model.softmax(model.linear(h_0))), dim=1)\n",
        "    vectors.extend(\n",
        "      h_0.expand(dataset_size, params['hidden_size'] + 2).cpu().detach().numpy()\n",
        "    )\n",
        "\n",
        "    vectors, counts = numpy.unique(numpy.array(vectors), axis=0, return_counts=True)\n",
        "\n",
        "    print(\"Number of unique vectors {0:,}\".format(vectors.shape[0]))\n",
        "    print(\"Number of total vectors {0:,}\".format(numpy.sum(counts)))\n",
        "\n",
        "  return vectors, counts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT6dazLMrjxe"
      },
      "source": [
        "model, params = load_model(\n",
        "    params_path=\"/numeral-50/rnn.model.params\",\n",
        "    model_path=\"/numeral-50/rnn.model.pt\"\n",
        ")\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/numeral-50/train.data.npy\",\n",
        "  DATA_DIR + \"/numeral-50/train.length.npy\",\n",
        "  DATA_DIR + \"/numeral-50/train.labels.npy\"\n",
        ")\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=500, shuffle = False\n",
        ")\n",
        "\n",
        "vectors, counts = get_model_vectors(model, params, train_loader)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "fig = plt.figure(figsize=(50, 50))\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(vectors)\n",
        "\n",
        "t = reduced.transpose()\n",
        "\n",
        "plt.scatter(t[0], t[1])\n",
        "plt.show()\n",
        "\n",
        "# numpy.savetxt(MODELS_DIR + \"/words/rnn.model.vectors.npy.gz\", vectors)\n",
        "# numpy.savetxt(MODELS_DIR + \"/words/rnn.model.vectors.counts.npy.gz\", counts, fmt='%d')\n",
        "# numpy.save(MODELS_DIR + \"/words/rnn.model.vectors.npy\", vectors)\n",
        "# numpy.save(MODELS_DIR + \"/words/rnn.model.vectors.weights.npy\", counts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fprBS8yOteJS"
      },
      "source": [
        "def read_centroids(file):\n",
        "  centroids = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        centroids.append([float(p) for p in line.split()])\n",
        "\n",
        "  print(\"Number of centroids: {0:,}\".format(len(centroids)))\n",
        "\n",
        "  return torch.tensor(centroids).cuda()\n",
        "\n",
        "centroids = read_centroids(MODELS_DIR + \"/numeral-50/rnn.model.vectors.txt.centroids.txt\")\n",
        "print (centroids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfL_MZ3N1rtx"
      },
      "source": [
        "train_data = load_data(\n",
        "  DATA_DIR + \"/numeral/train.data.npy\",\n",
        "  DATA_DIR + \"/numeral/train.length.npy\",\n",
        "  DATA_DIR + \"/numeral/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/numeral/dev.data.npy\",\n",
        "  DATA_DIR + \"/numeral/dev.length.npy\",\n",
        "  DATA_DIR + \"/numeral/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/numeral/test.data.npy\",\n",
        "  DATA_DIR + \"/numeral/test.length.npy\",\n",
        "  DATA_DIR + \"/numeral/test.labels.npy\"\n",
        ")\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path=\"/numeral/rnn2.model.params\",\n",
        "    model_path=\"/numeral/rnn2.model.pt\"\n",
        ")\n",
        "\n",
        "params['batch_size'] = 10\n",
        "params['lr'] = 0.0001\n",
        "params['weight_decay'] = 0.005\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params['batch_size'], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "\n",
        "vectors, counts = get_model_vectors(model, params, train_loader)\n",
        "\n",
        "clusters = KMeans(\n",
        "    n_clusters=500,\n",
        "    init=\"k-means++\",\n",
        "    random_state=0\n",
        ").fit(vectors, None, counts).cluster_centers_\n",
        "numpy.save(MODELS_DIR + \"/numeral/st-debug-rnn2.model.centers.npy\", clusters)\n",
        "\n",
        "params['num_epochs'] = 100\n",
        "params['number_of_states'] = 500\n",
        "params['temperature'] = 0.1\n",
        "\n",
        "\n",
        "stohastic_model = NLNN(params)\n",
        "stohastic_model.load_state_dict(model.state_dict(), strict=False)\n",
        "stohastic_model.embeddings.weight.requires_grad=False\n",
        "stohastic_model.rnn.rnn_cell.weight_ih.requires_grad=False\n",
        "stohastic_model.rnn.rnn_cell.weight_hh.requires_grad=False\n",
        "\n",
        "stohastic_model.rnn.states.data = torch.from_numpy(clusters[:,:-2]).float().to(device)\n",
        "stohastic_model.to(device)\n",
        "\n",
        "# alphabet = {}\n",
        "# with open(DATA_DIR + \"/numeral-50/alphabet.dict\", \"rb\") as f:\n",
        "#   alphabet = pickle.load(f)\n",
        "\n",
        "# inv_alphabet = { v : k for k, v in alphabet.items() }\n",
        "# torch.set_printoptions(precision=4)\n",
        "# torch.set_printoptions(threshold=5000)\n",
        "# torch.set_printoptions(sci_mode=False)\n",
        "# x = torch.tensor(list(inv_alphabet.keys()), device=device).unsqueeze(1)\n",
        "# length = torch.ones(len(inv_alphabet), device=device)\n",
        "\n",
        "# h_0 = torch.zeros((len(inv_alphabet), params['hidden_size']), device=device)\n",
        "\n",
        "# softmax_output, rnn_output, transition_probabilities = stohastic_model(x, length, h_0=h_0, return_probabilities=True, return_rnn_output=True)\n",
        "# transition_probabilities, _ = nn.utils.rnn.pad_packed_sequence(transition_probabilities, batch_first=True)\n",
        "# next_states = torch.argmax(transition_probabilities[:, 0, ], dim=1)\n",
        "\n",
        "# print(torch.max(transition_probabilities[:, 0, ], dim=1))\n",
        "# print(inv_alphabet)\n",
        "# softmax_output, rnn_output = model(x, length, h_0=h_0 , return_rnn_output=True)\n",
        "# print(rnn_output)\n",
        "\n",
        "# model_summary(stohastic_model)\n",
        "stohastic_model = train_model(\n",
        "  stohastic_model,\n",
        "  params=params,\n",
        "  train_loader=train_loader, dev_loader=dev_loader,\n",
        "  params_path=\"/numeral/st-debug-rnn2.model.params\",\n",
        "  model_path=\"/numeral/st-debug-rnn2.model.pt\",\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZOWoW4tbo-e"
      },
      "source": [
        "Train stohastic model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EokKg0_Xu43i"
      },
      "source": [
        "model, params = load_model(\n",
        "    params_path=\"/words/rnn.model.params\",\n",
        "    model_path=\"/words/rnn.model.pt\"\n",
        ")\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/words/train.data.npy\",\n",
        "  DATA_DIR + \"/words/train.length.npy\",\n",
        "  DATA_DIR + \"/words/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/words/dev.data.npy\",\n",
        "  DATA_DIR + \"/words/dev.length.npy\",\n",
        "  DATA_DIR + \"/words/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/words/test.data.npy\",\n",
        "  DATA_DIR + \"/words/test.length.npy\",\n",
        "  DATA_DIR + \"/words/test.labels.npy\"\n",
        ")\n",
        "params['batch_size'] = 20\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params['batch_size'], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "\n",
        "params['num_epochs'] = 20\n",
        "params['number_of_states'] = 60000\n",
        "params['temperature'] = 0.01\n",
        "\n",
        "stohastic_model = NLNN(params)\n",
        "stohastic_model.load_state_dict(model.state_dict(), strict=False)\n",
        "stohastic_model.embeddings.weight.requires_grad=False\n",
        "print(params)\n",
        "model_summary(stohastic_model)\n",
        "\n",
        "centroids = numpy.load(MODELS_DIR + \"/words/mini-clusters.npy\")\n",
        "stohastic_model.rnn.states.data = torch.from_numpy(centroids[:,:-2]).float().to(device)\n",
        "stohastic_model.to(device)\n",
        "\n",
        "train_model(\n",
        "  stohastic_model,\n",
        "  params=params,\n",
        "  params_path=\"/words/stohastic.rnn.model.params\",\n",
        "  model_path=\"/words/stohastic.rnn.model.pt\",\n",
        "  train_loader=train_loader, dev_loader=dev_loader\n",
        ")\n",
        "evaluate_model(test_loader, stohastic_model, \"test\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8Ds6I7bjm6"
      },
      "source": [
        "train_data = load_data(\n",
        "  DATA_DIR + \"/numeral/train.data.npy\",\n",
        "  DATA_DIR + \"/numeral/train.length.npy\",\n",
        "  DATA_DIR + \"/numeral/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/numeral/dev.data.npy\",\n",
        "  DATA_DIR + \"/numeral/dev.length.npy\",\n",
        "  DATA_DIR + \"/numeral/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/numeral/test.data.npy\",\n",
        "  DATA_DIR + \"/numeral/test.length.npy\",\n",
        "  DATA_DIR + \"/numeral/test.labels.npy\"\n",
        ")\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path=\"/numeral/rnn2.model.params\",\n",
        "    model_path=\"/numeral/rnn2.model.pt\"\n",
        ")\n",
        "\n",
        "params['batch_size'] = 2\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params['batch_size'], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "\n",
        "vectors, counts = get_model_vectors(model, params, train_loader)\n",
        "\n",
        "n_unique_vectors = vectors.shape[0]\n",
        "results = []\n",
        "n_experiments = 3\n",
        "\n",
        "for n_clusters in [500, 400, 350, 300, 250, 200, 100]:\n",
        "  print (\"n_clusters \", n_clusters)\n",
        "\n",
        "  clusters = KMeans(\n",
        "      n_clusters=n_clusters,\n",
        "      init=\"k-means++\",\n",
        "      random_state=0\n",
        "  ).fit(vectors, None, counts).cluster_centers_\n",
        "\n",
        "  params['num_epochs'] = 35\n",
        "  params['number_of_states'] = n_clusters\n",
        "  params['temperature'] = 0.0000001\n",
        "\n",
        "  accuracy = []\n",
        "\n",
        "  for _ in range (0, n_experiments):\n",
        "    stohastic_model = NLNN(params)\n",
        "    stohastic_model.load_state_dict(model.state_dict(), strict=False)\n",
        "    stohastic_model.embeddings.weight.requires_grad=False\n",
        "\n",
        "    stohastic_model.rnn.states.data = torch.from_numpy(clusters[:,:-2]).float().to(device)\n",
        "    stohastic_model.to(device)\n",
        "\n",
        "    stohastic_model = train_model(\n",
        "      stohastic_model,\n",
        "      params=params,\n",
        "      train_loader=train_loader, dev_loader=dev_loader\n",
        "    )\n",
        "    _, _, _, _, _, _, _, acc = _evaluate(train_loader, stohastic_model)\n",
        "    accuracy.append(acc)\n",
        "\n",
        "  results.append((n_clusters, accuracy))\n",
        "\n",
        "print (results)\n",
        "\n",
        "experiment_params = params.copy()\n",
        "experiment_params['n_unique_vectors'] = n_unique_vectors\n",
        "experiment_params['n_experiments'] = n_experiments\n",
        "experiment_params['results'] = results\n",
        "del experiment_params['number_of_states']\n",
        "\n",
        "with open(MODELS_DIR + \"/numeral/number-of-states-accuracy-ver2.rnn2.model.pkl\", 'wb') as f:\n",
        "  pickle.dump(experiment_params, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gif22AN84W42"
      },
      "source": [
        "Plot accuracy as function of the number of states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TtKJTj94WCT"
      },
      "source": [
        "with open(MODELS_DIR + \"/numeral/number-of-states-accuracy-ver2.rnn2.model.pkl\", 'rb') as f:\n",
        "  experiment_params = pickle.load(f)\n",
        "\n",
        "results = experiment_params['results']\n",
        "results.reverse()\n",
        "\n",
        "min_accuracy = [min(accuracy) for (n_states, accuracy) in results]\n",
        "max_accuracy = [max(accuracy) for (n_states, accuracy) in results]\n",
        "avg_accuracy = [sum(accuracy) / len(accuracy) for (n_states, accuracy) in results]\n",
        "x = [n_states for (n_states, accuracy) in results]\n",
        "\n",
        "figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')\n",
        "min_y, max_y = 0.5, 1.0\n",
        "plt.ylim(min_y, max_y)\n",
        "#plt.yticks(numpy.arange(min_y, max_y, 0.05))\n",
        "plt.xlim(0, 500)\n",
        "plt.xticks(numpy.arange(min(x), max(x) + 1, 50))\n",
        "plt.grid()\n",
        "\n",
        "plt.plot(x, avg_accuracy, '-o', label=\"average accuracy\")\n",
        "plt.plot(x, min_accuracy, '-ro', label=\"min accuracy\")\n",
        "plt.plot(x, max_accuracy, '-go', label=\"max accuracy\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel('number of states')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy as a function of the number of states')\n",
        "plt.savefig(MODELS_DIR + \"/numeral/number-of-states-accuracy-ver2.rnn2.model.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B6kjZM6rZdt"
      },
      "source": [
        "Silhouette scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Q01Tc2rU1w"
      },
      "source": [
        "for n_clusters in list(range(2, 4)):\n",
        "\n",
        "  km = KMeans(\n",
        "      n_clusters=n_clusters,\n",
        "      init=\"k-means++\",\n",
        "      n_init=1,\n",
        "      tol=1e-7,\n",
        "      random_state=0,\n",
        "      precompute_distances=False,\n",
        "      verbose=0,\n",
        "      algorithm=\"full\"\n",
        "  )\n",
        "  labels = km.fit_predict(vectors)\n",
        "  centroids = km.cluster_centers_\n",
        "\n",
        "  number_of_points_in_cluster = [0] * n_clusters\n",
        "  for label in labels:\n",
        "    number_of_points_in_cluster[label] += 1\n",
        "\n",
        "  from scipy.spatial import distance\n",
        "\n",
        "  a = [0] * vectors.shape[0]\n",
        "\n",
        "  for i, vector_i in enumerate(vectors):\n",
        "    centroid = labels[i]\n",
        "    if number_of_points_in_cluster[centroid] > 1:\n",
        "      sum_distances = 0\n",
        "      for j, vector_j in enumerate(vectors):\n",
        "        if i != j and labels[j] == centroid:\n",
        "          sum_distances += distance.euclidean(vector_i, vector_j)\n",
        "\n",
        "      a[i] = sum_distances / (number_of_points_in_cluster[centroid] - 1)\n",
        "\n",
        "  b = [0] * vectors.shape[0]\n",
        "\n",
        "  for i, vector_i in enumerate(vectors):\n",
        "    b_i = [0] * n_clusters\n",
        "    centroid = labels[i]\n",
        "    for j, vector_j in enumerate(vectors):\n",
        "      if labels[j] != centroid:\n",
        "        b_i[labels[j]] += distance.euclidean(vector_i, vector_j)\n",
        "    b_i = [bi / number_of_points_in_cluster[j] for j, bi in enumerate(b_i)]\n",
        "    b_i.pop(centroid)\n",
        "    b[i] = min(b_i)\n",
        "\n",
        "  s = [0] * vectors.shape[0]\n",
        "  for i in range(0, vectors.shape[0]):\n",
        "    if number_of_points_in_cluster[labels[i]] > 1:\n",
        "      s[i] = (b[i] - a[i]) / max(a[i], b[i])\n",
        "\n",
        "  print(s)\n",
        "  print (n_clusters, sum(s) / len(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zn8elRZ2Vd7"
      },
      "source": [
        "class Automaton():\n",
        "\n",
        "  def __init__(self, initial_state, transitions, is_final):\n",
        "    self.initial_state = initial_state\n",
        "    self.is_final = is_final\n",
        "\n",
        "    self.number_of_transitions = len(transitions)\n",
        "    self.number_of_states = len(is_final)\n",
        "\n",
        "    self.state_first_transition = [-1] * self.number_of_states\n",
        "    self.state_number_of_transitions = [0] * self.number_of_states\n",
        "    self.transitions_from = [-1] * self.number_of_transitions\n",
        "    self.transitions_to = [-1] * self.number_of_transitions\n",
        "    self.transitions_label = [-1] * self.number_of_transitions\n",
        "\n",
        "    def compare(transition1, transition2):\n",
        "      q_11, a1, q_21 = transition1\n",
        "      q_12, a2, q_22 = transition2\n",
        "      eq_ = q_11 - q_12\n",
        "      if eq_ == 0:\n",
        "        return ord(a1) - ord(a2)\n",
        "      return eq_\n",
        "\n",
        "    import functools\n",
        "    sorted_transtions = sorted(transitions, key=functools.cmp_to_key(compare))\n",
        "\n",
        "    state_index = -1\n",
        "    transition_index = 0\n",
        "\n",
        "    for transition in sorted_transtions:\n",
        "      q_1, a, q_2 = transition\n",
        "\n",
        "      if q_1 != state_index:\n",
        "        self.state_first_transition[q_1] = transition_index\n",
        "        state_index = q_1\n",
        "      self.transitions_from[transition_index] = q_1\n",
        "      self.transitions_to[transition_index] = q_2\n",
        "      self.transitions_label[transition_index] = a\n",
        "      transition_index += 1\n",
        "      self.state_number_of_transitions[q_1] += 1\n",
        "\n",
        "  def __print__(self):\n",
        "    print(\"===============\")\n",
        "    print(\"number_of_states\", self.number_of_states)\n",
        "    print(\"number_of_transitions\", self.number_of_transitions)\n",
        "    print(\"initial_state\", self.initial_state)\n",
        "    print(\"is_final\", list(enumerate(self.is_final)))\n",
        "    print(\"state_first_transition\", list(enumerate(self.state_first_transition)))\n",
        "    print(\"state_number_of_transitions\", list(enumerate(self.state_number_of_transitions)))\n",
        "    print(\"transitions_from\", list(enumerate(self.transitions_from)))\n",
        "    print(\"transitions_label\", list(enumerate(self.transitions_label)))\n",
        "    print(\"transitions_to\", list(enumerate(self.transitions_to)))\n",
        "    print(\"===============\")\n",
        "\n",
        "  def is_deterministic(self):\n",
        "    # dummy implementation\n",
        "    for transition_index in range(self.number_of_transitions):\n",
        "      transition_from = self.transitions_from[transition_index]\n",
        "      transition_label = self.transitions_label[transition_index]\n",
        "\n",
        "      for transition_index2 in range(self.number_of_transitions):\n",
        "        if transition_index != transition_index2:\n",
        "          transition_from2 = self.transitions_from[transition_index2]\n",
        "          transition_label2 = self.transitions_label[transition_index2]\n",
        "          if transition_from == transition_from2 and transition_label == transition_label2:\n",
        "            print (\"not deterministic \", transition_index, transition_index2, transition_from2, transition_label2, self.transitions_to[transition_index], self.transitions_to[transition_index2] )\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "  def contains_cycle(self):\n",
        "    def dfs(stack, visited):\n",
        "      while len(stack) > 0:\n",
        "        top = stack.pop()\n",
        "        visited[top] = True\n",
        "\n",
        "        for i in range(self.state_number_of_transitions[top]):\n",
        "          adjacent = self.transitions_to[self.state_first_transition[top] + i]\n",
        "          if not visited[adjacent]:\n",
        "            stack.append(adjacent)\n",
        "          else:\n",
        "            return True\n",
        "      return False\n",
        "\n",
        "    visited = [False] * self.number_of_states\n",
        "    stack = []\n",
        "    stack.append(self.initial_state)\n",
        "\n",
        "    return dfs(stack, visited)\n",
        "\n",
        "  def reachable_states(self):\n",
        "    def dfs(stack, visited):\n",
        "      while len(stack) > 0:\n",
        "        top = stack.pop()\n",
        "        visited[top] = True\n",
        "\n",
        "        for i in range(self.state_number_of_transitions[top]):\n",
        "          adjacent = self.transitions_to[self.state_first_transition[top] + i]\n",
        "          if not visited[adjacent]:\n",
        "            stack.append(adjacent)\n",
        "\n",
        "    visited = [False] * self.number_of_states\n",
        "    stack = []\n",
        "    stack.append(self.initial_state)\n",
        "\n",
        "    dfs(stack, visited)\n",
        "\n",
        "    reachable_states = set()\n",
        "    for state in range(self.number_of_states):\n",
        "      if visited[state]:\n",
        "        reachable_states.add(state)\n",
        "\n",
        "    return reachable_states\n",
        "\n",
        "  def coreachable_states(self):\n",
        "\n",
        "    def is_coreachable(state):\n",
        "      visited = [False] * self.number_of_states\n",
        "      stack = []\n",
        "\n",
        "      stack.append(state)\n",
        "\n",
        "      while len(stack) > 0:\n",
        "        top = stack.pop()\n",
        "        visited[top] = True\n",
        "\n",
        "        if self.is_final[top] == 1:\n",
        "            return True\n",
        "\n",
        "        for i in range(self.state_number_of_transitions[top]):\n",
        "          adjacent = self.transitions_to[self.state_first_transition[top] + i]\n",
        "          if not visited[adjacent]:\n",
        "            stack.append(adjacent)\n",
        "\n",
        "      return False\n",
        "\n",
        "    coreachable_states = set()\n",
        "    for state in range(self.number_of_states):\n",
        "      if is_coreachable(state):\n",
        "        coreachable_states.add(state)\n",
        "\n",
        "    return coreachable_states\n",
        "\n",
        "  def trim(self):\n",
        "\n",
        "    def delete_state(state):\n",
        "\n",
        "      first_transition = self.state_first_transition[state]\n",
        "      number_of_transitions = self.state_number_of_transitions[state]\n",
        "      last_transition = first_transition + number_of_transitions\n",
        "\n",
        "      # delete out-going transitions\n",
        "      self.transitions_from[first_transition:last_transition] = [-1] * number_of_transitions\n",
        "      self.transitions_to[first_transition:last_transition] = [-1] * number_of_transitions\n",
        "      self.transitions_label[first_transition:last_transition] = [-1] * number_of_transitions\n",
        "\n",
        "      # delete in-comming transitions\n",
        "      for transitions_index in range(self.number_of_transitions):\n",
        "        if self.transitions_to[transitions_index] == state:\n",
        "          transition_from = self.transitions_from[transitions_index]\n",
        "          self.state_number_of_transitions[transition_from] -= 1\n",
        "          self.transitions_label[transitions_index] = -1\n",
        "          self.transitions_from[transitions_index] = -1\n",
        "          self.transitions_to[transitions_index] = -1\n",
        "\n",
        "      self.is_final[state] = -1\n",
        "      self.state_first_transition[state] = -1\n",
        "      self.state_number_of_transitions[state] = -1\n",
        "\n",
        "    states_to_delete = set()\n",
        "    reachable_and_coreachable_states = self.reachable_states() & self.coreachable_states()\n",
        "    for state in range(self.number_of_states):\n",
        "      if not (state in reachable_and_coreachable_states):\n",
        "        states_to_delete.add(state)\n",
        "\n",
        "    for state in states_to_delete:\n",
        "      delete_state(state)\n",
        "\n",
        "    self.number_of_states = len([state for state in self.is_final if state != -1])\n",
        "    self.number_of_transitions = len([tr for tr in self.transitions_from if tr != -1])\n",
        "\n",
        "    state_index = 0\n",
        "    new_old_state, old_new_state = dict(), dict()\n",
        "    for state in range(len(self.is_final)):\n",
        "      if self.is_final[state] != -1:\n",
        "        new_old_state[state_index] = state\n",
        "        old_new_state[state] = state_index\n",
        "        state_index += 1\n",
        "\n",
        "    is_final = [-1] * self.number_of_states\n",
        "    for state in new_old_state.keys():\n",
        "      is_final[state] = self.is_final[new_old_state[state]]\n",
        "    self.is_final = is_final\n",
        "\n",
        "    transition_index = 0\n",
        "    new_old_transition, old_new_transition = dict(), dict()\n",
        "    for transition in range(len(self.transitions_from)):\n",
        "      if self.transitions_from[transition] != -1:\n",
        "        new_old_transition[transition_index] = transition\n",
        "        old_new_transition[transition] = transition_index\n",
        "        transition_index += 1\n",
        "\n",
        "    state_number_of_transitions = [-1] * self.number_of_states\n",
        "    for state in new_old_state.keys():\n",
        "      state_number_of_transitions[state] = self.state_number_of_transitions[new_old_state[state]]\n",
        "    self.state_number_of_transitions = state_number_of_transitions\n",
        "\n",
        "    state_first_transition = [-1] * self.number_of_states\n",
        "    for state in new_old_state.keys():\n",
        "      if self.state_number_of_transitions[state] > 0:\n",
        "        transition_index = self.state_first_transition[new_old_state[state]]\n",
        "        while self.transitions_from[transition_index] == -1:\n",
        "          transition_index += 1\n",
        "        state_first_transition[state] = old_new_transition[transition_index]\n",
        "    self.state_first_transition = state_first_transition\n",
        "\n",
        "    transitions_from = [-1] * self.number_of_transitions\n",
        "    transitions_to = [-1] * self.number_of_transitions\n",
        "    transitions_label = [-1] * self.number_of_transitions\n",
        "    for transition in new_old_transition.keys():\n",
        "      transitions_from[transition] = old_new_state[self.transitions_from[new_old_transition[transition]]]\n",
        "      transitions_to[transition] = old_new_state[self.transitions_to[new_old_transition[transition]]]\n",
        "      transitions_label[transition] = self.transitions_label[new_old_transition[transition]]\n",
        "    self.transitions_from = transitions_from\n",
        "    self.transitions_to = transitions_to\n",
        "    self.transitions_label = transitions_label\n",
        "\n",
        "    self.initial_state = old_new_state[self.initial_state]\n",
        "\n",
        "    return \n",
        "\n",
        "  def accepts(self, word):\n",
        "    state = self.initial_state\n",
        "\n",
        "    for character in word:\n",
        "      state = self.delta(state, character)\n",
        "\n",
        "      if state == -1:\n",
        "        return False\n",
        "\n",
        "    return (self.is_final[state] == 1)\n",
        "\n",
        "  def delta(self, state, character):\n",
        "\n",
        "    def binary_search(array, element, left, right):\n",
        "\n",
        "      import bisect\n",
        "\n",
        "      index = bisect.bisect_left(array, element, left, right)\n",
        "      if index != len(array) and array[index] == element: \n",
        "          return index\n",
        "      return -1\n",
        "\n",
        "\n",
        "    if self.state_first_transition[state] == -1:\n",
        "      return -1\n",
        "\n",
        "    index = binary_search(\n",
        "      self.transitions_label, character,\n",
        "      self.state_first_transition[state],\n",
        "      self.state_first_transition[state] + self.state_number_of_transitions[state]\n",
        "    )\n",
        "    if index == -1:\n",
        "      return -1\n",
        "    else:\n",
        "      return self.transitions_to[index]\n",
        "\n",
        "  def minimize(self):\n",
        "\n",
        "    def fnv_1a(state, state_class):\n",
        "      OFFSET_BASIS = 0x811C9DC5\n",
        "      PRIME = 0x1000193\n",
        "      MAX = 2 ** 32\n",
        "\n",
        "      hash_code = OFFSET_BASIS\n",
        "      hash_code ^= self.is_final[state]\n",
        "      hash_code = (hash_code * PRIME) % MAX\n",
        "\n",
        "      first_transition = self.state_first_transition[state]\n",
        "\n",
        "      if first_transition != -1:\n",
        "\n",
        "        number_of_transitions = self.state_number_of_transitions[state]\n",
        "        last_transition = first_transition + number_of_transitions\n",
        "\n",
        "        for transition_index in range(first_transition, last_transition):\n",
        "          hash_code ^= ord(self.transitions_label[transition_index])\n",
        "          hash_code = (hash_code * PRIME) % MAX\n",
        "          hash_code ^= state_class[self.transitions_to[transition_index]]\n",
        "          hash_code = (hash_code * PRIME) % MAX\n",
        "\n",
        "      return hash_code\n",
        "\n",
        "    def split_classes(states, state_class):\n",
        "      split = False\n",
        "      new_states = []\n",
        "      new_state_class = dict()\n",
        "      number_of_classes = 0\n",
        "\n",
        "      for state in states:\n",
        "\n",
        "        if len(state) == 1:\n",
        "          new_states.append(state)\n",
        "          (st, ) = state\n",
        "          new_state_class[st] = number_of_classes\n",
        "          number_of_classes += 1\n",
        "\n",
        "        else:\n",
        "          split_state = []\n",
        "\n",
        "          hash_code_states = dict()\n",
        "          for st in state:\n",
        "            hash_code_ = fnv_1a(st, state_class)\n",
        "            hash_code_states[hash_code_] = hash_code_states.get(hash_code_, set())\n",
        "            hash_code_states[hash_code_].add(st)\n",
        "\n",
        "          for hash_code, state in hash_code_states.items():\n",
        "            # todo check for colisions\n",
        "            split_state.append(state)\n",
        "            for st in state:\n",
        "              new_state_class[st] = number_of_classes\n",
        "            number_of_classes += 1\n",
        "\n",
        "          new_states.extend(split_state)\n",
        "\n",
        "          if len(split_state) > 1:\n",
        "              split = True\n",
        "\n",
        "      return split, new_states, new_state_class\n",
        "\n",
        "    state_class = dict()\n",
        "    final, not_final = set(), set()\n",
        "    for state in range(len(self.is_final)):\n",
        "      if self.is_final[state] == 1:\n",
        "        final.add(state)\n",
        "        state_class[state] = 1\n",
        "      else:\n",
        "        not_final.add(state)\n",
        "        state_class[state] = 0\n",
        "\n",
        "    split, states, state_class = split_classes([final, not_final], state_class)\n",
        "\n",
        "    while split:\n",
        "      split, states, state_class = split_classes(states, state_class)\n",
        "\n",
        "    #print (\"at end \", states, len(states))\n",
        "\n",
        "    number_of_states = len(states)\n",
        "    initial_state = state_class[self.initial_state]\n",
        "    is_final = [-1] * number_of_states\n",
        "    for state in states:\n",
        "      st = next(iter(state))\n",
        "      is_final[state_class[st]] = self.is_final[st]\n",
        "\n",
        "    transitions = set()\n",
        "    for state in states:\n",
        "      st = next(iter(state))\n",
        "\n",
        "      first_transition = self.state_first_transition[st]\n",
        "\n",
        "      if first_transition != -1:\n",
        "\n",
        "        number_of_transitions = self.state_number_of_transitions[st]\n",
        "        last_transition = first_transition + number_of_transitions\n",
        "\n",
        "        for transition_index in range(first_transition, last_transition):\n",
        "          transitions.add((\n",
        "            state_class[st],\n",
        "            self.transitions_label[transition_index],\n",
        "            state_class[self.transitions_to[transition_index]]\n",
        "          ))\n",
        "\n",
        "    # print (number_of_states)\n",
        "    # print (len(transitions))\n",
        "    return Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  def draw(self, file):\n",
        "\n",
        "    from graphviz import Digraph\n",
        "\n",
        "    nodes = set()\n",
        "    for q_1 in self.transitions_from:\n",
        "      nodes.add(q_1)\n",
        "    for q_2 in self.transitions_to:\n",
        "      nodes.add(q_2)\n",
        "\n",
        "    filename = DATA_DIR + \"/\" + file\n",
        "    fileformat = 'pdf'\n",
        "    g = Digraph('G', filename=filename, format=fileformat)\n",
        "    g.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "    for node in nodes:\n",
        "\n",
        "      if self.is_final[node] == 1:\n",
        "        if node == self.initial_state:\n",
        "          g.attr('node', shape='doublecircle', style='filled', color='grey80')\n",
        "        else:\n",
        "          g.attr('node', shape='doublecircle', style='filled', color='grey80')\n",
        "        g.node(str(node))\n",
        "      else:\n",
        "        if node == self.initial_state:\n",
        "          g.attr('node', shape='doubleoctagon', style='filled', color='grey80')\n",
        "        else:\n",
        "          g.attr('node', shape='circle', style='filled', color='grey80')\n",
        "        g.node(str(node))\n",
        "\n",
        "      g.attr('node', shape='point', color=\"black\")\n",
        "\n",
        "    g.edge('', str(self.initial_state), label='', color=\"black\")\n",
        "\n",
        "    for i in range(self.number_of_transitions):\n",
        "      q_1 = self.transitions_from[i]\n",
        "      a = self.transitions_label[i]\n",
        "      q_2 = self.transitions_to[i]\n",
        "\n",
        "      g.edge(str(q_1), str(q_2), label=str(a))\n",
        "\n",
        "    g.render() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sApV6MtPybMP"
      },
      "source": [
        "def test_trim():\n",
        "\n",
        "  initial_state = 2\n",
        "  transitions = set()\n",
        "  transitions.add((2, 'x', 1))\n",
        "  transitions.add((0, 'x', 1))\n",
        "  is_final = [0, 1, 0]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 2\n",
        "  assert reachable_states == {1, 2}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 3\n",
        "  assert coreachable_states == {0, 1, 2}\n",
        "  assert automaton.accepts('x')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 1\n",
        "  assert automaton.number_of_states == 2\n",
        "  assert len(automaton.reachable_states()) == 2\n",
        "  assert len(automaton.coreachable_states()) == 2\n",
        "  assert automaton.accepts('x')\n",
        "\n",
        "\n",
        "  initial_state = 2\n",
        "  transitions = set()\n",
        "  transitions.add((2, 'x', 1))\n",
        "  transitions.add((1, 'b', 3))\n",
        "  transitions.add((1, 'c', 5))\n",
        "  transitions.add((0, 'x', 1))\n",
        "  transitions.add((0, 'x', 4))\n",
        "  transitions.add((2, 'a', 4))\n",
        "  is_final = [0, 1, 0, 0, 0, 1]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 6\n",
        "  assert automaton.number_of_states == 6\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 5\n",
        "  assert reachable_states == {1, 2, 3, 4, 5}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 4\n",
        "  assert coreachable_states == {0, 1, 2, 5}\n",
        "  assert automaton.accepts('x')\n",
        "  assert automaton.accepts('xc')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  assert len(automaton.reachable_states()) == 3\n",
        "  assert len(automaton.coreachable_states()) == 3\n",
        "  assert automaton.accepts('x')\n",
        "  assert automaton.accepts('xc')\n",
        "\n",
        "\n",
        "  initial_state = 2\n",
        "  transitions = set()\n",
        "  transitions.add((2, 'a', 1))\n",
        "  transitions.add((1, 'b', 0))\n",
        "  is_final = [1, 1, 0]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 3\n",
        "  assert reachable_states == {0, 1, 2}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 3\n",
        "  assert coreachable_states == {0, 1, 2}\n",
        "  assert automaton.accepts('ab')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  assert len(automaton.reachable_states()) == 3\n",
        "  assert len(automaton.coreachable_states()) == 3\n",
        "  assert automaton.accepts('ab')\n",
        "\n",
        "\n",
        "  initial_state = 3\n",
        "  transitions = set()\n",
        "  transitions.add((0, 'a', 4))\n",
        "  transitions.add((3, 'a', 4))\n",
        "  transitions.add((3, 'b', 2))\n",
        "  transitions.add((2, 'c', 1))\n",
        "  is_final = [0, 1, 1, 0, 0]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 4\n",
        "  assert automaton.number_of_states == 5\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 4\n",
        "  assert reachable_states == {1, 2, 3, 4}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 3\n",
        "  assert coreachable_states == {1, 2, 3}\n",
        "  assert automaton.accepts('b')\n",
        "  assert automaton.accepts('bc')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  assert len(automaton.reachable_states()) == 3\n",
        "  assert len(automaton.coreachable_states()) == 3\n",
        "  assert automaton.accepts('b')\n",
        "  assert automaton.accepts('bc')\n",
        "\n",
        "\n",
        "  initial_state = 2\n",
        "  transitions = set()\n",
        "  transitions.add((2, 'a', 1))\n",
        "  transitions.add((1, 'b', 0))\n",
        "  is_final = [0, 1, 0]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 3\n",
        "  assert reachable_states == {0, 1, 2}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 2\n",
        "  assert coreachable_states == {1, 2}\n",
        "  assert automaton.accepts('a')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 1\n",
        "  assert automaton.number_of_states == 2\n",
        "  assert len(automaton.reachable_states()) == 2\n",
        "  assert len(automaton.coreachable_states()) == 2\n",
        "  assert automaton.accepts('a')\n",
        "\n",
        "\n",
        "  for iterations in range(1024):\n",
        "\n",
        "    labels = random.sample(string.ascii_lowercase, 6)\n",
        "    initial_state = 0\n",
        "    transitions = set()\n",
        "    transitions.add((0, labels[0], 1))\n",
        "    transitions.add((0, labels[1], 2))\n",
        "    transitions.add((0, labels[2], 3))\n",
        "    transitions.add((0, labels[3], 4))\n",
        "    transitions.add((0, labels[4], 5))\n",
        "    transitions.add((0, labels[5], 6))\n",
        "    is_final = [0, 1, 1, 1, 0, 0, 1]\n",
        "    automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "    assert automaton.contains_cycle() == False\n",
        "    assert automaton.number_of_transitions == 6\n",
        "    assert automaton.number_of_states == 7\n",
        "    reachable_states = automaton.reachable_states()\n",
        "    assert len(reachable_states) == 7\n",
        "    assert reachable_states == {0, 1, 2, 3, 4, 5, 6}\n",
        "    coreachable_states = automaton.coreachable_states()\n",
        "    assert len(coreachable_states) == 5\n",
        "    assert coreachable_states == {0, 1, 2, 3, 6}\n",
        "    assert automaton.accepts(labels[0])\n",
        "    assert automaton.accepts(labels[1])\n",
        "    assert automaton.accepts(labels[2])\n",
        "    assert automaton.accepts(labels[5])\n",
        "\n",
        "    automaton.trim()\n",
        "\n",
        "    assert automaton.contains_cycle() == False\n",
        "    assert automaton.number_of_transitions == 4\n",
        "    assert automaton.number_of_states == 5\n",
        "    assert len(automaton.reachable_states()) == 5\n",
        "    assert len(automaton.coreachable_states()) == 5\n",
        "    assert automaton.accepts(labels[0])\n",
        "    assert automaton.accepts(labels[1])\n",
        "    assert automaton.accepts(labels[2])\n",
        "    assert automaton.accepts(labels[5])\n",
        "\n",
        "test_trim()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME8j0HZ8yIH9"
      },
      "source": [
        "def test_minimize1():\n",
        "\n",
        "  initial_state = 0\n",
        "  transitions = set()\n",
        "  transitions.add((0, 'a', 1))\n",
        "  transitions.add((0, 'd', 3))\n",
        "  transitions.add((1, 'b', 2))\n",
        "  transitions.add((3, 'b', 2))\n",
        "  is_final = [0, 1, 1, 1]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  automaton.minimize()\n",
        "  print (automaton.number_of_states)\n",
        "\n",
        "def test_minimize2():\n",
        "\n",
        "  initial_state = 0\n",
        "  transitions = set()\n",
        "  transitions.add((0, 'a', 1))\n",
        "  transitions.add((0, 'd', 4))\n",
        "  transitions.add((1, 'b', 2))\n",
        "  transitions.add((2, 'c', 3))\n",
        "  transitions.add((4, 'b', 5))\n",
        "  transitions.add((5, 'c', 6))\n",
        "  is_final = [0, 1, 0, 1, 1, 0, 1]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  automaton = automaton.minimize()\n",
        "  print (automaton.number_of_states)\n",
        "  automaton.draw(\"min\")\n",
        "\n",
        "test_minimize2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96NiN7fMAnAA"
      },
      "source": [
        "def evaluate_model__(dataset, model, set_name):\n",
        "  data_loader = utils.data.DataLoader(\n",
        "    NLDataset(dataset), batch_size=1, shuffle = False\n",
        "  )\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "\n",
        "  for index, data in enumerate(data_loader):\n",
        "    label = data['y'][0].item()\n",
        "    result = model(data['x'], data['length'])\n",
        "    prediction = result.argmax(dim=1).cpu().numpy()[0]\n",
        "\n",
        "    x, length = data['x'], data['length']\n",
        "    x = x[0].tolist()\n",
        "    length = length.item()\n",
        "    x = x[:length]\n",
        "    x = [inv_alphabet[c] for c in x]\n",
        "\n",
        "    if label == 1:\n",
        "      if prediction == 1:\n",
        "        tp += 1\n",
        "        print (\"M tp \", \"\".join(x), index)\n",
        "      else:\n",
        "        fn += 1\n",
        "        print (\"M fn \", \"\".join(x), index)\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        fp += 1\n",
        "        print (\"M fp \", \"\".join(x), index)\n",
        "      else:\n",
        "        tn += 1\n",
        "        print (\"M tn \", \"\".join(x), index)\n",
        "\n",
        "  if tp == 0:\n",
        "    if fn == 0 and fp == 0:\n",
        "      pr, r, f1 = 1, 1, 1\n",
        "    else:\n",
        "      pr, r, f1 = 0, 0, 0\n",
        "  else:\n",
        "    pr = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    f1 = 2 * ((pr * r) / (pr + r))\n",
        "\n",
        "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "  print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "    set_name, tp, tn, fp, fn, pr, r, f1, accuracy\n",
        "  ))\n",
        "\n",
        "alphabet = {}\n",
        "with open(DATA_DIR + \"/words/alphabet.dict\", \"rb\") as f:\n",
        "  alphabet = pickle.load(f)\n",
        "\n",
        "inv_alphabet = { v : k for k, v in alphabet.items() }\n",
        "\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/words/train.data.npy\",\n",
        "  DATA_DIR + \"/words/train.length.npy\",\n",
        "  DATA_DIR + \"/words/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/words/dev.data.npy\",\n",
        "  DATA_DIR + \"/words/dev.length.npy\",\n",
        "  DATA_DIR + \"/words/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/words/test.data.npy\",\n",
        "  DATA_DIR + \"/words/test.length.npy\",\n",
        "  DATA_DIR + \"/words/test.labels.npy\"\n",
        ")\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path=\"/words/rnn.model.params\",\n",
        "    model_path=\"/words/rnn.model.pt\"\n",
        ")\n",
        "print (params)\n",
        "model_summary(model)\n",
        "\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "\n",
        "evaluate_model(train_loader, model, \"train\")\n",
        "evaluate_model(dev_loader, model, \"dev\")\n",
        "evaluate_model(test_loader, model, \"test\")\n",
        "\n",
        "# initial_state = 0\n",
        "# transitions = []\n",
        "\n",
        "# train_loader = utils.data.DataLoader(\n",
        "#   NLDataset(train_data), batch_size=1\n",
        "# )\n",
        "\n",
        "# for index, data in enumerate(train_loader):\n",
        "#     x, length = data['x'], data['length']\n",
        "#     softmax_output, rnn_output, transition_probabilities = model(x, length, return_probabilities=True, return_rnn_output=True)\n",
        "#     prediction = softmax_output.argmax(dim=1).cpu().numpy()[0]\n",
        "#     if prediction == 1:\n",
        "#       transition_probabilities, _ = nn.utils.rnn.pad_packed_sequence(transition_probabilities, batch_first=True)\n",
        "#       rnn_output, _ = nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n",
        "#       first = x[0]\n",
        "#       prev_state = initial_state\n",
        "#       for t in range(length[0]):\n",
        "#         l = inv_alphabet[first[t].item()]\n",
        "#         next_state = torch.argmax(transition_probabilities[0, t, ]).item() + 1\n",
        "#         transitions.append((prev_state, l, next_state))\n",
        "#         prev_state = next_state\n",
        "\n",
        "# start_state = torch.zeros((1, params['hidden_size']), device=device)\n",
        "# #print (model.softmax(model.linear(torch.cat((start_state, model.rnn.states)))))\n",
        "# is_final = model.softmax(model.linear(torch.cat((start_state, model.rnn.states)))).argmax(dim=1).cpu().tolist()\n",
        "\n",
        "# state_symbol_to_states = defaultdict(list)\n",
        "\n",
        "# for transition in transitions:\n",
        "#   q1, a, q2 = transition\n",
        "#   state_symbol_to_states[(q1, a)].append(q2)\n",
        "\n",
        "# count_non_deterministic, count_arbitrarily = 0, 0\n",
        "\n",
        "# transitions = set()\n",
        "# for state_symbol in state_symbol_to_states:\n",
        "#   states = state_symbol_to_states[state_symbol]\n",
        "#   q1, a = state_symbol\n",
        "#   states_set = set(states)\n",
        "\n",
        "#   if len(states_set) == 1:\n",
        "#     transitions.add((q1, a, states[0]))\n",
        "#   else:\n",
        "#     count_non_deterministic += 1\n",
        "#     counts = Counter(states)\n",
        "#     print (\"WARNING: Non deterministic transitions \", q1, a, counts)\n",
        "#     first_two_most_common = counts.most_common(2)\n",
        "\n",
        "#     if len(first_two_most_common) > 1:\n",
        "#       first_most_common, second_most_common = first_two_most_common\n",
        "#       first_most_common_state, first_most_common_count = first_most_common\n",
        "#       second_most_common_state, second_most_common_count = second_most_common\n",
        "#       if first_most_common_count == second_most_common_count:\n",
        "#         # should we determinize the automaton?\n",
        "#         count_arbitrarily += 1\n",
        "#         print (\"WARNING: Selected transition is arbitrarily\", q1, a, first_most_common_state)\n",
        "#       transitions.add((q1, a, first_most_common_state))\n",
        "\n",
        "#     else:\n",
        "#       first_most_common = first_two_most_common[0]\n",
        "#       first_most_common_state, first_most_common_count = first_most_common\n",
        "#       transitions.add((q1, a, first_most_common_state))\n",
        "\n",
        "# print (\"Number of not deterministic transitions\", count_non_deterministic)\n",
        "# print (\"Number of arbitrarily selected transitions\", count_arbitrarily)\n",
        "\n",
        "# automaton = Automaton(initial_state, transitions, is_final)\n",
        "# print (\"Number of states\", automaton.number_of_states)\n",
        "# print (\"Number of transitions\", automaton.number_of_transitions)\n",
        "# print (\"Reachable states \", len(automaton.reachable_states()))\n",
        "# print (\"Co-Reachable states \", len(automaton.coreachable_states()))\n",
        "# #print (\"DFA? \", automaton.is_deterministic())\n",
        "# automaton.trim()\n",
        "# print (\"Number of states\", automaton.number_of_states)\n",
        "# print (\"Number of transitions\", automaton.number_of_transitions)\n",
        "\n",
        "# automaton = automaton.minimize()\n",
        "# print (\"Number of states\", automaton.number_of_states)\n",
        "# print (\"Number of transitions\", automaton.number_of_transitions)\n",
        "# print (\"Has cycle?\", automaton.contains_cycle())\n",
        "# #automaton.draw(\"numeral-50.2.aut.min\")\n",
        "\n",
        "# def __evaluate_automaton(automaton, dataset):\n",
        "#   data_loader = utils.data.DataLoader(\n",
        "#     NLDataset(dataset), batch_size=1, shuffle = False\n",
        "#   )\n",
        "#   tp, tn, fp, fn = 0, 0, 0, 0\n",
        "\n",
        "#   for data in data_loader:\n",
        "#     label = data['y'][0].item()\n",
        "\n",
        "#     x, length = data['x'], data['length']\n",
        "#     x = x[0].tolist()\n",
        "#     length = length.item()\n",
        "#     x = x[:length]\n",
        "#     x = [inv_alphabet[c] for c in x]\n",
        "\n",
        "#     if automaton.accepts(x):\n",
        "#       prediction = 1\n",
        "#     else:\n",
        "#      prediction = 0\n",
        "\n",
        "#     if label == 1:\n",
        "#       if prediction == 1:\n",
        "#         tp += 1\n",
        "#         print (\"A tp \", \"\".join(x))\n",
        "#       else:\n",
        "#         fn += 1\n",
        "#         print (\"A fn \", \"\".join(x))\n",
        "#     else:\n",
        "#       if prediction == 1:\n",
        "#         fp += 1\n",
        "#         print (\"A fp \", \"\".join(x))\n",
        "#       else:\n",
        "#         tn += 1\n",
        "#         print (\"A tn \", \"\".join(x))\n",
        "\n",
        "#   if tp == 0:\n",
        "#     if fn == 0 and fp == 0:\n",
        "#       pr, r, f1 = 1, 1, 1\n",
        "#     else:\n",
        "#       pr, r, f1 = 0, 0, 0\n",
        "#   else:\n",
        "#     pr = tp / (tp + fp)\n",
        "#     r = tp / (tp + fn)\n",
        "#     f1 = 2 * ((pr * r) / (pr + r))\n",
        "\n",
        "#   accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "#   return tp, tn, fp, fn, pr, r, f1, accuracy\n",
        "\n",
        "# def _evaluate_automaton(automaton, dataset):\n",
        "#   data_loader = utils.data.DataLoader(\n",
        "#     NLDataset(dataset), batch_size=1, shuffle = False\n",
        "#   )\n",
        "\n",
        "#   labels, predictions = [], []\n",
        "\n",
        "#   for data in data_loader:\n",
        "#     labels.append(data['y'][0].item())\n",
        "\n",
        "#     x, length = data['x'], data['length']\n",
        "#     x = x[0].tolist()\n",
        "#     length = length.item()\n",
        "#     x = x[:length]\n",
        "#     x = [inv_alphabet[c] for c in x]\n",
        "\n",
        "#     if automaton.accepts(x):\n",
        "#       predictions.append(1)\n",
        "#     else:\n",
        "#       predictions.append(0)\n",
        "\n",
        "\n",
        "#   return __evaluate(predictions, labels)\n",
        "\n",
        "# def evaluate_automaton(automaton, dataset, set_name):\n",
        "#   tp, tn, fp, fn, pr, r, f1, acc = _evaluate_automaton(automaton, dataset)\n",
        "#   print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "#     set_name, tp, tn, fp, fn, pr, r, f1, acc\n",
        "#   ))\n",
        "\n",
        "# evaluate_automaton(automaton, train_data, \"train\")\n",
        "# evaluate_automaton(automaton, dev_data, \"dev\")\n",
        "# evaluate_automaton(automaton, test_data, \"test\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwhexEjUT31F"
      },
      "source": [
        "Automata build Second Variant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zece9UDT3Wl"
      },
      "source": [
        "def evaluate_model__(dataset, model, set_name):\n",
        "  data_loader = utils.data.DataLoader(\n",
        "    NLDataset(dataset), batch_size=1, shuffle = False\n",
        "  )\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "\n",
        "  for index, data in enumerate(data_loader):\n",
        "    label = data['y'][0].item()\n",
        "    result = model(data['x'], data['length'])\n",
        "    prediction = result.argmax(dim=1).cpu().numpy()[0]\n",
        "\n",
        "    x, length = data['x'], data['length']\n",
        "    x = x[0].tolist()\n",
        "    length = length.item()\n",
        "    x = x[:length]\n",
        "    x = [inv_alphabet[c] for c in x]\n",
        "\n",
        "    if label == 1:\n",
        "      if prediction == 1:\n",
        "        tp += 1\n",
        "        print (\"M tp \", \"\".join(x), index)\n",
        "      else:\n",
        "        fn += 1\n",
        "        print (\"M fn \", \"\".join(x), index)\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        fp += 1\n",
        "        print (\"M fp \", \"\".join(x), index)\n",
        "      else:\n",
        "        tn += 1\n",
        "        print (\"M tn \", \"\".join(x), index)\n",
        "\n",
        "  if tp == 0:\n",
        "    if fn == 0 and fp == 0:\n",
        "      pr, r, f1 = 1, 1, 1\n",
        "    else:\n",
        "      pr, r, f1 = 0, 0, 0\n",
        "  else:\n",
        "    pr = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    f1 = 2 * ((pr * r) / (pr + r))\n",
        "\n",
        "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "  print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "    set_name, tp, tn, fp, fn, pr, r, f1, accuracy\n",
        "  ))\n",
        "\n",
        "\n",
        "alphabet = {}\n",
        "with open(DATA_DIR + \"/numeral/alphabet.dict\", \"rb\") as f:\n",
        "  alphabet = pickle.load(f)\n",
        "\n",
        "inv_alphabet = { v : k for k, v in alphabet.items() }\n",
        "\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/numeral/train.data.npy\",\n",
        "  DATA_DIR + \"/numeral/train.length.npy\",\n",
        "  DATA_DIR + \"/numeral/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/numeral/dev.data.npy\",\n",
        "  DATA_DIR + \"/numeral/dev.length.npy\",\n",
        "  DATA_DIR + \"/numeral/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/numeral/test.data.npy\",\n",
        "  DATA_DIR + \"/numeral/test.length.npy\",\n",
        "  DATA_DIR + \"/numeral/test.labels.npy\"\n",
        ")\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path=\"/numeral/st-debug-rnn2.model.params\",\n",
        "    model_path=\"/numeral/st-debug-rnn2.model.pt\"\n",
        ")\n",
        "print (params)\n",
        "# model_summary(model)\n",
        "\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "# evaluate_model__(train_data, model, \"train\")\n",
        "evaluate_model(train_loader, model, \"train\")\n",
        "evaluate_model(dev_loader, model, \"dev\")\n",
        "evaluate_model(test_loader, model, \"test\")\n",
        "\n",
        "initial_state = 0\n",
        "transitions = set()\n",
        "states = set()\n",
        "queue = [initial_state]\n",
        "\n",
        "x = torch.tensor(list(inv_alphabet.keys()), device=device).unsqueeze(1)\n",
        "length = torch.ones(len(inv_alphabet), device=device)\n",
        "\n",
        "while len(queue) != 0:\n",
        "\n",
        "  current_state = queue.pop(0)\n",
        "  states.add(current_state)\n",
        "\n",
        "  if current_state == initial_state:\n",
        "    h_0 = torch.zeros((len(inv_alphabet), params['hidden_size']), device=device)\n",
        "  else:\n",
        "    h_0 = model.rnn.states[current_state - 1].expand(len(inv_alphabet), -1)\n",
        "\n",
        "  softmax_output, rnn_output, transition_probabilities = model(x, length, h_0=h_0, return_probabilities=True, return_rnn_output=True)\n",
        "  transition_probabilities, _ = nn.utils.rnn.pad_packed_sequence(transition_probabilities, batch_first=True)\n",
        "  probabilties, next_states = torch.max(transition_probabilities[:, 0, ], dim=1)\n",
        "  # if current_state == 0:\n",
        "  #   print (next_states)\n",
        "  #   print (probabilties)\n",
        "  #   print (torch.max(transition_probabilities[:, 0, ], dim=1))\n",
        "\n",
        "  next_states += 1\n",
        "\n",
        "  for i, char in enumerate(inv_alphabet):\n",
        "    l = inv_alphabet[char]\n",
        "    # if current_state == 0:\n",
        "    #   print ((current_state, l, next_states[i].item()))\n",
        "    #   print(probabilties.size(), probabilties[i].item())\n",
        "    transitions.add((current_state, l, next_states[i].item(), probabilties[i].item()))\n",
        "\n",
        "  for state in set(next_states.detach().cpu().tolist()):\n",
        "    if state not in states and state not in queue:\n",
        "      queue.append(state)\n",
        "\n",
        "print (\"transitiooons \", len(transitions))\n",
        "\n",
        "ps = [ps for (q1,l,q2,ps) in transitions]\n",
        "print (min(ps))\n",
        "print (max(ps))\n",
        "\n",
        "threshold = 0\n",
        "transitions = [(q1,l,q2) for (q1,l,q2,ps) in transitions if ps > threshold]\n",
        "print (\"transitiooons \", len(transitions))\n",
        "# print (states)\n",
        "# print (initial_state)\n",
        "\n",
        "mapping = { state : i for i, state in enumerate(states)}\n",
        "start_state = torch.zeros((1, params['hidden_size']), device=device)\n",
        "is_final = model.softmax(model.linear(torch.cat((start_state, model.rnn.states)))).argmax(dim=1).cpu().tolist()\n",
        "\n",
        "trs = set()\n",
        "for tr in transitions:\n",
        "  q1, l, q2 = tr\n",
        "  trs.add((mapping[q1], l, mapping[q2]))\n",
        "\n",
        "isf = []\n",
        "for k, v in mapping.items():\n",
        "  isf.append(is_final[k])\n",
        "\n",
        "print (\"mapping \", mapping)\n",
        "print (\"trrrrs \", trs)\n",
        "print (isf)\n",
        "\n",
        "automaton = Automaton(initial_state, trs, isf)\n",
        "print (\"Number of states\", automaton.number_of_states)\n",
        "print (\"Number of transitions\", automaton.number_of_transitions)\n",
        "print (\"Reachable states \", len(automaton.reachable_states()))\n",
        "print (\"Co-Reachable states \", len(automaton.coreachable_states()))\n",
        "print (\"DFA? \", automaton.is_deterministic())\n",
        "automaton.trim()\n",
        "print (\"Number of states\", automaton.number_of_states)\n",
        "print (\"Number of transitions\", automaton.number_of_transitions)\n",
        "\n",
        "automaton = automaton.minimize()\n",
        "print (\"Number of states\", automaton.number_of_states)\n",
        "print (\"Number of transitions\", automaton.number_of_transitions)\n",
        "print (\"Has cycle?\", automaton.contains_cycle())\n",
        "\n",
        "print (\"frrro \", automaton.transitions_from)\n",
        "print (\"frrro \", automaton.transitions_to)\n",
        "print (\"frrro \", automaton.transitions_label)\n",
        "\n",
        "def __evaluate_automaton(automaton, dataset):\n",
        "  data_loader = utils.data.DataLoader(\n",
        "    NLDataset(dataset), batch_size=1, shuffle = False\n",
        "  )\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "\n",
        "  for data in data_loader:\n",
        "    label = data['y'][0].item()\n",
        "\n",
        "    x, length = data['x'], data['length']\n",
        "    x = x[0].tolist()\n",
        "    length = length.item()\n",
        "    x = x[:length]\n",
        "    x = [inv_alphabet[c] for c in x]\n",
        "\n",
        "    if automaton.accepts(x):\n",
        "      prediction = 1\n",
        "    else:\n",
        "     prediction = 0\n",
        "\n",
        "    if label == 1:\n",
        "      if prediction == 1:\n",
        "        tp += 1\n",
        "        print (\"A tp \", \"\".join(x))\n",
        "      else:\n",
        "        fn += 1\n",
        "        print (\"A fn \", \"\".join(x))\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        fp += 1\n",
        "        print (\"A fp \", \"\".join(x))\n",
        "      else:\n",
        "        tn += 1\n",
        "        print (\"A tn \", \"\".join(x))\n",
        "\n",
        "  if tp == 0:\n",
        "    if fn == 0 and fp == 0:\n",
        "      pr, r, f1 = 1, 1, 1\n",
        "    else:\n",
        "      pr, r, f1 = 0, 0, 0\n",
        "  else:\n",
        "    pr = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    f1 = 2 * ((pr * r) / (pr + r))\n",
        "\n",
        "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "  return tp, tn, fp, fn, pr, r, f1, accuracy\n",
        "\n",
        "def _evaluate_automaton(automaton, dataset):\n",
        "  data_loader = utils.data.DataLoader(\n",
        "    NLDataset(dataset), batch_size=1, shuffle = False\n",
        "  )\n",
        "\n",
        "  labels, predictions = [], []\n",
        "\n",
        "  for data in data_loader:\n",
        "    labels.append(data['y'][0].item())\n",
        "\n",
        "    x, length = data['x'], data['length']\n",
        "    x = x[0].tolist()\n",
        "    length = length.item()\n",
        "    x = x[:length]\n",
        "    try:\n",
        "      x = [inv_alphabet[c] for c in x]\n",
        "    except:\n",
        "      print (inv_alphabet)\n",
        "      raise\n",
        "\n",
        "    if automaton.accepts(x):\n",
        "      predictions.append(1)\n",
        "    else:\n",
        "      predictions.append(0)\n",
        "\n",
        "\n",
        "  return __evaluate(predictions, labels)\n",
        "\n",
        "def evaluate_automaton(automaton, dataset, set_name):\n",
        "  tp, tn, fp, fn, pr, r, f1, acc = _evaluate_automaton(automaton, dataset)\n",
        "  print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "    set_name, tp, tn, fp, fn, pr, r, f1, acc\n",
        "  ))\n",
        "#__evaluate_automaton(automaton, train_data)\n",
        "evaluate_automaton(automaton, train_data, \"train\")\n",
        "evaluate_automaton(automaton, dev_data, \"dev\")\n",
        "evaluate_automaton(automaton, test_data, \"test\")\n",
        "# print(automaton.transitions_from)\n",
        "# print(automaton.transitions_to)\n",
        "# print(automaton.transitions_label)\n",
        "\n",
        "# ya\n",
        "# kur = torch.tensor([[13]]).to(device)\n",
        "# l = torch.tensor([1]).to(device)\n",
        "# res = model(kur, l)\n",
        "# print (\"model soft out \",res)\n",
        "# print(res.argmax(dim=1).detach().cpu().numpy())\n",
        "# print(\"state 3 \",model.rnn.states[16])\n",
        "\n",
        "# omodel, oparams = load_model(\n",
        "#     params_path=\"/numeral-50/rnn2.model.params\",\n",
        "#     model_path=\"/numeral-50/rnn2.model.pt\"\n",
        "# )\n",
        "# # import sys\n",
        "#numpy.set_printoptions(threshold=sys.maxsize)\n",
        "#centers = numpy.load(MODELS_DIR + \"/numeral-50/st-debug-rnn2.model.centers.npy\")\n",
        "#print(centers)\n",
        "# softmax_output, rnn_output = omodel(kur, l, return_rnn_output=True)\n",
        "# print (\"omodel soft output \",softmax_output)\n",
        "# print (\"omodel rnn output \", rnn_output)\n",
        "# print(softmax_output.argmax(dim=1).detach().cpu().numpy())\n",
        "print (alphabet)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}