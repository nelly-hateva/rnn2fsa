{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "State Regularized RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNVwvZpTE9b/6VB+AQ/FnXv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelly-hateva/rnn2fsa/blob/master/notebooks/State_Regularized_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y0Pyq0G0A4W"
      },
      "source": [
        "# Prerequisities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B18oUqOvxmd8"
      },
      "source": [
        "## Mount Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvCTwouDz-zs"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "MOUNT_POINT = \"/content/drive/\"\n",
        "drive.mount(MOUNT_POINT, force_remount=True)\n",
        "\n",
        "DATA_DIR = MOUNT_POINT + \"My Drive/Thesis/data\"\n",
        "MODELS_DIR = MOUNT_POINT + \"My Drive/Thesis/models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDQZ5n1R0aWe"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u56lsr5U0W7h"
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append(MOUNT_POINT + \"My Drive/Thesis/src\") \n",
        "\n",
        "from automata import Evaluation\n",
        "from dataset import NLDataset, Preprocessing\n",
        "from measures import Measures\n",
        "from model import NLNN\n",
        "from rnn2fsa import Algorithm1, Algorithm2\n",
        "from training import Trainer, ModelSerializer\n",
        "from utils import Reproducibility"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbBftCsLWw_p"
      },
      "source": [
        "## Set device and check runtime resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYo9XQBnWYp0"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "import torch\n",
        "\n",
        "ram_gb = virtual_memory().total / 1e+9\n",
        "print('{:.2f} GB RAM available\\n'.format(ram_gb))\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select \"Runtime\" -> \"Change runtime type\", ')\n",
        "  print('and then select \"High-RAM\" in the \"Runtime shape\" dropdown. ')\n",
        "  print('Then re-execute this cell.\\n')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device: {}\\n'.format(device))\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select \"Runtime\" -> \"Change runtime type\" to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.\\n')\n",
        "else:\n",
        "  print(gpu_info)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuhvQxl2gPBE"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImCiBGbxyNnN"
      },
      "source": [
        "## Train numeral model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8hg1ewO6Q5h"
      },
      "source": [
        "train, dev, test, alphabet = NLDataset.load(DATA_DIR + \"/numeral/\")\n",
        "\n",
        "# numeral/modelgradviz.params\n",
        "# train : TP : 471 TN : 448 FP : 23 FN : 8 Pr : 0.95 R : 0.98 F1: 0.97 ACC : 0.97 \n",
        "# dev : TP : 52 TN : 57 FP : 8 FN : 2 Pr : 0.87 R : 0.96 F1: 0.91 ACC : 0.92 \n",
        "# test : TP : 58 TN : 46 FP : 12 FN : 3 Pr : 0.83 R : 0.95 F1: 0.89 ACC : 0.87 \n",
        "# {'num_embeddings': 23, 'mode': 'rnn', 'nonlinearity': 'relu', 'hidden_size': 20, 'bias': False, 'number_of_states': 3000, 'temperature': 0.1, 'batch_size': 10, 'lr': 0.001, 'weight_decay': 0, 'num_epochs': 200, 'patience': 500, 'save_after_each_epoch': False}\n",
        "\n",
        "# numeral/stmodel.params\n",
        "# clip gra 1.0\n",
        "# params = {\n",
        "#   'num_embeddings': len(alphabet),\n",
        "#   #'embedding_dim': 3,\n",
        "#   'mode': 'rnn',\n",
        "#   'nonlinearity': 'relu',\n",
        "#   'hidden_size': 20,\n",
        "#   'bias': False,\n",
        "#   'number_of_states': 2000,\n",
        "#   'temperature': 0.1,#1e-7,\n",
        "\n",
        "#   'batch_size': 10, # 100 1e-2 / 10 1e-3\n",
        "#   'lr': 1e-3,\n",
        "#   'weight_decay': 1e-4,\n",
        "#   'num_epochs': 200,\n",
        "#   'patience': 500,\n",
        "#   'save_after_each_epoch': False,\n",
        "# }\n",
        "# Best epoch: 50\tBest dev accuracy: 0.87\tTime: 534.71s\n",
        "# train : TP : 447 TN : 419 FP : 52 FN : 32 Pr : 0.90 R : 0.93 F1: 0.91 ACC : 0.91 \n",
        "# dev : TP : 49 TN : 55 FP : 10 FN : 5 Pr : 0.83 R : 0.91 F1: 0.87 ACC : 0.87 \n",
        "# test : TP : 54 TN : 42 FP : 16 FN : 7 Pr : 0.77 R : 0.89 F1: 0.82 ACC : 0.81 \n",
        "\n",
        "model_params = {\n",
        "  'num_embeddings': len(alphabet),\n",
        "  #'embedding_dim': 3,\n",
        "  'mode': 'rnn',\n",
        "  'nonlinearity': 'relu',\n",
        "  'hidden_size': 20,\n",
        "  'bias': False,\n",
        "  'number_of_states': 3000,\n",
        "  'temperature': 0.1#1e-7\n",
        "}\n",
        "\n",
        "optimizer_params = {\n",
        "  'lr': 1e-3,\n",
        "  'weight_decay': 0,\n",
        "}\n",
        "\n",
        "params = {\n",
        "  'batch_size': 10, # 100 1e-2 / 10 1e-3\n",
        "  'num_epochs': 200,\n",
        "  'max_norm': 1.0,\n",
        "  'patience': 10\n",
        "}\n",
        "\n",
        "train_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(train), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(dev), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(test), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  checkpoint_dir=MODELS_DIR + \"/numeral/check/\",\n",
        "  best_model_dir=MODELS_DIR + \"/numeral/check/\",\n",
        "  model_class=NLNN,\n",
        "  model_params=model_params,\n",
        "  loss=nn.CrossEntropyLoss(),\n",
        "  optimizer_class=optim.Adam,\n",
        "  optimizer_params=optimizer_params,\n",
        "  device=device,\n",
        "  params=params\n",
        ")\n",
        "\n",
        "model = trainer.fit(\n",
        "  train_dataloader=train_dataloader, dev_dataloader=dev_dataloader\n",
        ")\n",
        "\n",
        "trainer.plot_training_losses(labels={\n",
        "  'avg_train_losses': 'Average Train Losses',\n",
        "  'avg_dev_losses': 'Average Dev Losses',\n",
        "  'dev_accuracies': 'Dev Accuracy',\n",
        "  'xlabel': 'epochs',\n",
        "  'min_avg_dev_loss': 'Minimum Average Dev Loss',\n",
        "  'max_dev_accuracy': 'Maximum Dev Accuracy'\n",
        "  }, path=MODELS_DIR + \"/numeral/trainer/losses.en.jpg\"\n",
        ")\n",
        "trainer.plot_training_losses(labels={\n",
        "  'avg_train_losses': 'Средни стойности на целевата функция върху TRAIN',\n",
        "  'avg_dev_losses': 'Средни стойности на целевата функция върху DEV',\n",
        "  'dev_accuracies': 'Точност върху DEV',\n",
        "  'xlabel': 'епохи',\n",
        "  'min_avg_dev_loss': 'Минимална средна стойност на целевата функция върху DEV',\n",
        "  'max_dev_accuracy': 'Максимална точност върху DEV'\n",
        "  }, path=MODELS_DIR + \"/numeral/trainer/losses.bg.jpg\"\n",
        ")\n",
        "\n",
        "print_accuracy(train_dataloader, model, \"train\")\n",
        "print_accuracy(dev_dataloader, model, \"dev\")\n",
        "print_accuracy(test_dataloader, model, \"test\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6-wKoZ7oBoA"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVCnhBZeaYmO"
      },
      "source": [
        "train, dev, test, alphabet = NLDataset.load(DATA_DIR + \"/words/\")\n",
        "\n",
        "model_params = {\n",
        "  'num_embeddings': len(alphabet),\n",
        "  #'embedding_dim': 3,\n",
        "  'mode': 'rnn',\n",
        "  'nonlinearity': 'relu',\n",
        "  'hidden_size': 100,\n",
        "  'bias': False,\n",
        "  'number_of_states': 60000,\n",
        "  'temperature': 0.1,#1e-7,\n",
        "}\n",
        "\n",
        "optimizer_params = {\n",
        "  'lr': 1e-4,\n",
        "  'weight_decay': 1e-6\n",
        "}\n",
        "\n",
        "params = {\n",
        "  'batch_size': 20,\n",
        "  'num_epochs': 50,\n",
        "  'max_norm': 1.0,\n",
        "  'patience': 10\n",
        "}\n",
        "\n",
        "train_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(train), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_dataloader = utils.data.DataLoader(\n",
        "  NLDatasetcollections(dev), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(test), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  checkpoint_dir=MODELS_DIR + \"/words/model/\",\n",
        "  best_model_dir=MODELS_DIR + \"/words/model/\",\n",
        "  model_class=NLNN,\n",
        "  model_params=model_params,\n",
        "  loss=nn.CrossEntropyLoss(),\n",
        "  optimizer_class=optim.Adam,\n",
        "  optimizer_params=optimizer_params,\n",
        "  device=device,\n",
        "  params=params\n",
        ")\n",
        "\n",
        "model = trainer.fit(\n",
        "  train_dataloader=train_dataloader, dev_dataloader=dev_dataloader\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJy8vsDRiLTg"
      },
      "source": [
        "## Resume training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY1qZV9SiNr-"
      },
      "source": [
        "train, dev, test, alphabet = NLDataset.load(DATA_DIR + \"/words/\")\n",
        "params = {\n",
        "  'batch_size': 20\n",
        "}\n",
        "train_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(train), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(dev), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(test), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  checkpoint=MODELS_DIR + \"/words/model/checkpoint.pt\",\n",
        "  checkpoint_dir=MODELS_DIR + \"/words/model/\",\n",
        "  best_model_dir=MODELS_DIR + \"/words/model/\",\n",
        "  device=device,\n",
        "  model_class=NLNN,\n",
        "  loss=nn.CrossEntropyLoss(),\n",
        "  optimizer_class=optim.Adam\n",
        ")\n",
        "\n",
        "model = trainer.fit(\n",
        "  train_dataloader=train_dataloader, dev_dataloader=dev_dataloader,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gif22AN84W42"
      },
      "source": [
        "# Accuracy as function of the number of states\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8Ds6I7bjm6"
      },
      "source": [
        "train, dev, test, alphabet = NLDataset.load(DATA_DIR + \"/numeral/\")\n",
        "\n",
        "params = {\n",
        "  'num_embeddings': len(alphabet),\n",
        "  #'embedding_dim': 3,\n",
        "  'mode': 'rnn',\n",
        "  'nonlinearity': 'relu',\n",
        "  'hidden_size': 20,\n",
        "  'bias': False,\n",
        "  'number_of_states': 3000,\n",
        "  'temperature': 0.1,\n",
        "\n",
        "  'batch_size': 10, # 100 1e-2 / 10 1e-3\n",
        "  'lr': 1e-3,\n",
        "  'weight_decay': 1e-5,\n",
        "  'num_epochs': 200,\n",
        "  'max_norm': 1.0,\n",
        "  'patience': 10,\n",
        "  'save_after_each_epoch': False,\n",
        "}\n",
        "\n",
        "train_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(train), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(dev), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_dataloader = utils.data.DataLoader(\n",
        "  NLDataset(test), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "train_results, dev_results = [], []\n",
        "n_experiments = 5\n",
        "\n",
        "for n_clusters in [3000, 2000, 1000, 300, 100]:\n",
        "  print(\"n_clusters \", n_clusters)\n",
        "\n",
        "  params['number_of_states'] = n_clusters\n",
        "\n",
        "  train_accuracy_values, dev_accuracy_values = [], []\n",
        "\n",
        "  for _ in range (0, n_experiments):\n",
        "    model = NLNN(params)\n",
        "\n",
        "    model = Trainer().fit(\n",
        "      model, train_dataloader=train_dataloader, dev_dataloader=dev_dataloader,\n",
        "      params=params\n",
        "    )\n",
        "\n",
        "    _, _, _, _, _, _, _, acc = compute_accuracy(train_dataloader, model)\n",
        "    train_accuracy_values.append(acc)\n",
        "\n",
        "    _, _, _, _, _, _, _, acc = compute_accuracy(dev_dataloader, model)\n",
        "    dev_accuracy_values.append(acc)\n",
        "\n",
        "  train_results.append((n_clusters, train_accuracy_values))\n",
        "  dev_results.append((n_clusters, dev_accuracy_values))\n",
        "\n",
        "print (train_results)\n",
        "print (dev_results)\n",
        "# with open(MODELS_DIR + \"/numeral/number-of-states-accuracy-tain.pkl\", 'wb') as f:\n",
        "#   pickle.dump(train_results, f)\n",
        "\n",
        "# with open(MODELS_DIR + \"/numeral/number-of-states-accuracy-dev.pkl\", 'wb') as f:\n",
        "#   pickle.dump(dev_results, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_wFBh8q3WJN"
      },
      "source": [
        "## Plot saved results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TtKJTj94WCT"
      },
      "source": [
        "def plot(input_file, output_file, labels):\n",
        "  with open(input_file, 'rb') as f:\n",
        "    results = pickle.load(f)\n",
        "\n",
        "    results.reverse()\n",
        "\n",
        "    min_accuracy = [min(accuracy) for (n_states, accuracy) in results]\n",
        "    max_accuracy = [max(accuracy) for (n_states, accuracy) in results]\n",
        "    avg_accuracy = [sum(accuracy) / len(accuracy) for (n_states, accuracy) in results]\n",
        "    x = [n_states for (n_states, accuracy) in results]\n",
        "\n",
        "    figure(num=None, figsize=(10, 10), dpi=100, facecolor='w', edgecolor='w')\n",
        "    plt.ylim(0.5, 1.0)\n",
        "    plt.xlim(min(x), max(x))\n",
        "    plt.xticks(x)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.plot(x, avg_accuracy, '-o', label=labels['avg'])\n",
        "    plt.plot(x, min_accuracy, '-ro', label=labels['min'])\n",
        "    plt.plot(x, max_accuracy, '-go', label=labels['max'])\n",
        "\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.xlabel(labels['xlabel'])\n",
        "    plt.ylabel(labels['ylabel'])\n",
        "    plt.savefig(output_file)\n",
        "    plt.show()\n",
        "\n",
        "plot(\n",
        "  MODELS_DIR + \"/numeral/number-of-states-accuracy-tain.pkl\",\n",
        "  MODELS_DIR + '/numeral/number-of-states-accuracy-tain-en.jpg',\n",
        "  {\n",
        "    'avg': 'average accuracy',\n",
        "    'min': 'min accuracy',\n",
        "    'max': 'max accuracy',\n",
        "    'xlabel': 'number of states',\n",
        "    'ylabel': 'accuracy of the model'\n",
        "  }\n",
        ")\n",
        "\n",
        "plot(\n",
        "  MODELS_DIR + \"/numeral/number-of-states-accuracy-dev.pkl\",\n",
        "  MODELS_DIR + '/numeral/number-of-states-accuracy-dev-en.jpg',\n",
        "  {\n",
        "    'avg': 'average accuracy',\n",
        "    'min': 'min accuracy',\n",
        "    'max': 'max accuracy',\n",
        "    'xlabel': 'number of states',\n",
        "    'ylabel': 'accuracy of the model'\n",
        "  }\n",
        ")\n",
        "\n",
        "# plot(\n",
        "#   MODELS_DIR + \"/numeral/number-of-states-accuracy.model.pkl\",\n",
        "#   MODELS_DIR + '/numeral/number-of-states-accuracy-bg.model.jpg',\n",
        "#   {\n",
        "#     'avg': 'средна точност',\n",
        "#     'min': 'минимална точност',\n",
        "#     'max': 'максимална точност',\n",
        "#     'xlabel': 'брой състояния',\n",
        "#     'ylabel': 'точност на модела'\n",
        "#   }\n",
        "# )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}