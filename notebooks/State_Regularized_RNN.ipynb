{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "State Regularized RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP9smu/9SLDVTIJ21powZzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelly-hateva/tardis/blob/master/notebooks/State_Regularized_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y0Pyq0G0A4W"
      },
      "source": [
        "Prerequisities: Access data in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvCTwouDz-zs",
        "outputId": "8115a21a-0a5b-4945-f678-8fef91c2cd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "MOUNT_POINT = \"/content/drive/\"\n",
        "DATA_DIR = MOUNT_POINT + \"My Drive/Colab Notebooks/Thesis-Data\"\n",
        "MODELS_DIR = MOUNT_POINT + \"My Drive/Colab Notebooks/Thesis-Models\"\n",
        "drive.mount(MOUNT_POINT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqVIc7pe0Xxw"
      },
      "source": [
        "DATA_DIR = \"Thesis-Data\"\n",
        "MODELS_DIR = \"Thesis-Models\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDQZ5n1R0aWe"
      },
      "source": [
        "Import libraries, set random seed and device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u56lsr5U0W7h",
        "outputId": "6a1563da-fc99-4a91-9451-8a93d1ab0fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import pickle\n",
        "import copy\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "from torch import nn, optim, utils\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def seed_torch(seed=666):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  numpy.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_torch()\n",
        "\n",
        "# setting device on GPU if available, else CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIdpnf0yerS5"
      },
      "source": [
        "State Regularized RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ed6ar_Jc8dM"
      },
      "source": [
        "class SRRNN(nn.Module):\n",
        "\n",
        "  r\"\"\" https://arxiv.org/pdf/1901.08817.pdf\n",
        "       https://github.com/deepsemantic/sr-rnns\n",
        "  Applies a single-layer State Regularized RNN to an input sequence.\n",
        "\n",
        "  If :attr:`mode` is ``'rnn'``, then for each element in the input sequence computes the function\n",
        "\n",
        "    .. math::\n",
        "        h_t' = \\text{tanh}(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})\n",
        "\n",
        "    where :math:`x_t` is the input at time `t`, :math:`h_{(t-1)}` is the hidden state\n",
        "    at time `t-1` or the initial hidden state at time `0`.\n",
        "    If :attr:`nonlinearity` is ``'relu'``, then `ReLU` is used instead of `tanh`.\n",
        "\n",
        "  If :attr:`mode` is ``'gru'``, then for each element in the input sequence computes the function\n",
        "\n",
        "    .. math::\n",
        "      \\begin{array}{ll} \\\\\n",
        "        r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\\n",
        "        z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\\n",
        "        n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\\n",
        "        h_t' = (1 - z_t) * n_t + z_t * h_{(t-1)}\n",
        "      \\end{array}\n",
        "\n",
        "    where :math:`x_t` is the input at time `t`, :math:`h_{(t-1)}` is the hidden state\n",
        "    at time `t-1` or the initial hidden state at time `0`.\n",
        "    :math:`r_t`, :math:`z_t`, :math:`n_t` are the reset, update, and new gates, respectively.\n",
        "    :math:`\\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.\n",
        "\n",
        "  In both modes, if :attr:`number_of_states` is ``None``, then the hidden state\n",
        "  at time `t` :math:`h_t` equals :math:`h_t'`. \n",
        "\n",
        "  Otherwise,\n",
        "\n",
        "    .. math::\n",
        "      \\begin{array}{ll} \\\\\n",
        "        \\alpha_{i} = \\frac{\\exp(- \\Vert{h_t' - s_i}\\Vert)}{\\sum_{i=1}^{k} \\exp(- \\Vert{h_t' - s_i}\\Vert)}\n",
        "        h_t = {\\sum_{i=1}^{k} \\alpha_{i} s_i}\n",
        "      \\end{array}\n",
        "\n",
        "    where :math:`\\{s_1, s_2, ..., s_k\\}` are the k learnable states. \n",
        "    :math:`\\alpha_{i}` is the probability of the RNN to transition to state i\n",
        "    given the vector :math:`h_t'` for which we write :math:`p_{h_t'}(i) = \\alpha_{i}`\n",
        "\n",
        "    Args:\n",
        "        input_size: The number of expected features in the input `x`\n",
        "        hidden_size: The number of features in the hidden state `h`\n",
        "        mode: The RNN mode to use.\n",
        "          Can be either ``'rnn'`` or ``'gru'``. Default: ``'rnn'``\n",
        "        nonlinearity: The non-linearity to use if :attr:`mode` is ``'rnn'``.\n",
        "          Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``\n",
        "        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
        "            Default: ``True``\n",
        "        number_of_states: The number of learnable finite states.\n",
        "          If ``None`` then the stohastic component is not used. Default: ``None``\n",
        "\n",
        "    Inputs: input, h_0\n",
        "        - **input** of shape `(batch, seq_len, input_size)`:\n",
        "          tensor containing the features of the input sequence.\n",
        "          The input can also be a packed variable length sequence.\n",
        "          See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
        "          :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
        "        - **h_0** of shape `(batch, hidden_size)`: tensor\n",
        "          containing the initial hidden state for each element in the batch.\n",
        "\n",
        "          If `h_0` is not provided, it defaults to zero.\n",
        "\n",
        "    Outputs: output, h_n\n",
        "        - **output** of shape `(batch, seq_len, hidden_size)`: tensor\n",
        "          containing the output features `(h_t)` from the RNN for each `t`.\n",
        "          If a :class:`torch.nn.utils.rnn.PackedSequence` has been\n",
        "          given as the input, the output will also be a packed sequence.\n",
        "        - **h_n** of shape `(batch, hidden_size)`: tensor\n",
        "          containing the hidden state for `t = seq_len`.\n",
        "\n",
        "    Attributes:\n",
        "        If :attr:`mode` is ``'rnn'``\n",
        "          weight_ih: the learnable input-hidden weights,\n",
        "              of shape `(hidden_size, input_size)`\n",
        "          weight_hh: the learnable hidden-hidden weights,\n",
        "              of shape `(hidden_size, hidden_size)`\n",
        "          bias_ih: the learnable input-hidden bias,\n",
        "              of shape `(hidden_size)`\n",
        "          bias_hh: the learnable hidden-hidden bias,\n",
        "              of shape `(hidden_size)`\n",
        "        If :attr:`mode` is ``'gru'``\n",
        "          weight_ih : the learnable input-hidden weights\n",
        "              (W_ir|W_iz|W_in), of shape `(3*hidden_size, input_size)`\n",
        "          weight_hh : the learnable hidden-hidden weights\n",
        "              (W_hr|W_hz|W_hn), of shape `(3*hidden_size, hidden_size)`\n",
        "          bias_ih : the learnable input-hidden bias\n",
        "              (b_ir|b_iz|b_in), of shape `(3*hidden_size)`\n",
        "          bias_hh : the learnable hidden-hidden bias\n",
        "              (b_hr|b_hz|b_hn), of shape `(3*hidden_size)`\n",
        "        states: the learnable finite number of states, of shape\n",
        "            `(number_of_states, hidden_size)\n",
        "\n",
        "    .. note::\n",
        "        All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n",
        "        where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n",
        "\n",
        "    Examples::\n",
        "        >>> rnn = SRRNN(10, 20)\n",
        "        >>> input = torch.randn(6, 3, 10)\n",
        "        >>> h_0 = torch.randn(6, 20)\n",
        "        >>> output, h_n = rnn(input, h_0)\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "    self, input_size, hidden_size, bias=True, mode='rnn', nonlinearity='tanh',\n",
        "    number_of_states=None, temperature=1.00\n",
        "  ):\n",
        "    super(SRRNN, self).__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.bias = bias\n",
        "\n",
        "    if mode == 'rnn':\n",
        "      self.rnn_cell = nn.RNNCell(\n",
        "        input_size, hidden_size, bias=bias, nonlinearity=nonlinearity\n",
        "      )\n",
        "    elif mode == 'gru':\n",
        "      self.rnn_cell = nn.GRUCell(\n",
        "        input_size, hidden_size, bias=bias\n",
        "      )\n",
        "    else:\n",
        "      raise ValueError(\"Unknown mode '{}'\".format(mode))\n",
        "\n",
        "    self.stohastic_component = False\n",
        "\n",
        "    if number_of_states:\n",
        "      self.stohastic_component = True\n",
        "      self.number_of_states = number_of_states\n",
        "\n",
        "      self.softmax = nn.Softmax(dim=1)\n",
        "      self.states = nn.Parameter(\n",
        "        torch.Tensor(\n",
        "            self.number_of_states, hidden_size\n",
        "        )\n",
        "      )\n",
        "      self.temperature = temperature\n",
        "\n",
        "  def extra_repr(self):\n",
        "    s = ''\n",
        "    if 'number_of_states' in self.__dict__:\n",
        "      s = 'number_of_states={number_of_states}'\n",
        "      if 'temperature' in self.__dict__ and self.temperature != 1.00:\n",
        "        s += ', temperature={temperature}'\n",
        "\n",
        "    return s.format(**self.__dict__)\n",
        "\n",
        "  def permute_hidden(self, hidden, permutation, dim=0):\n",
        "    if permutation is None:\n",
        "      return hidden\n",
        "    return hidden.index_select(dim, permutation)\n",
        "\n",
        "  def forward(self, input, h_0=None):\n",
        "    orig_input = input\n",
        "\n",
        "    if isinstance(orig_input, nn.utils.rnn.PackedSequence):\n",
        "      input, batch_sizes, sorted_indices, unsorted_indices = input\n",
        "      max_batch_size = int(batch_sizes[0])\n",
        "    else:\n",
        "      max_batch_size, sorted_indices = input.size(0), None\n",
        "\n",
        "    if h_0 is None:\n",
        "      h_0 = torch.zeros(\n",
        "        max_batch_size, self.hidden_size, dtype=input.dtype, device=input.device\n",
        "      )\n",
        "      #  h_0 = self.states[0].expand(max_batch_size, -1)\n",
        "    else:\n",
        "      # Each batch of the hidden state should match the input sequence that\n",
        "      # the user believes he/she is passing in.\n",
        "      h_0 = self.permute_hidden(h_0, sorted_indices)\n",
        "\n",
        "    if isinstance(orig_input, nn.utils.rnn.PackedSequence):\n",
        "      output, hidden, transition_probabilities = self.forward_packed(input, batch_sizes, h_0)\n",
        "\n",
        "      if self.stohastic_component:\n",
        "        transition_probabilities = nn.utils.rnn.PackedSequence(\n",
        "          transition_probabilities, batch_sizes, sorted_indices, unsorted_indices\n",
        "        )\n",
        "\n",
        "      hidden = self.permute_hidden(hidden, unsorted_indices)\n",
        "      output = nn.utils.rnn.PackedSequence(\n",
        "        output, batch_sizes, sorted_indices, unsorted_indices\n",
        "      )\n",
        "\n",
        "      return output, hidden, transition_probabilities\n",
        "\n",
        "    return self.forward_tensor(input, h_0)\n",
        "\n",
        "  def forward_tensor(self, input, h_0):\n",
        "    output, h_t = [], h_0\n",
        "\n",
        "    if self.stohastic_component:\n",
        "      transition_probabilities = []\n",
        "\n",
        "    for t in range(input.size(1)):\n",
        "      result = self.forward_impl(input[:,t,:], h_t)\n",
        "      if self.stohastic_component:\n",
        "        transition_probs, h_t = result\n",
        "        transition_probabilities.append(transition_probs)\n",
        "      else:\n",
        "        h_t = result\n",
        "      output.append(h_t)\n",
        "\n",
        "    output = torch.stack(output).permute(1, 0, 2)\n",
        "\n",
        "    if self.stohastic_component:\n",
        "      return output, h_t, torch.stack(transition_probabilities).permute(1, 0, 2)\n",
        "    return output, h_t, None\n",
        "\n",
        "  def forward_packed(self, input, batch_sizes, h_0):\n",
        "    output, h_n, t, h_t = [], [], 0, h_0\n",
        "\n",
        "    if self.stohastic_component:\n",
        "      transition_probabilities = []\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "      batch_size = int(batch_size)\n",
        "\n",
        "      h_t, h_n_ = h_t[:batch_size], h_t[batch_size:]\n",
        "      h_n.append(h_n_)\n",
        "      result = self.forward_impl(input[t : t + batch_size], h_t)\n",
        "\n",
        "      if self.stohastic_component:\n",
        "        transition_probs, h_t = result\n",
        "        transition_probabilities.append(transition_probs)\n",
        "      else:\n",
        "        h_t = result\n",
        "\n",
        "      output.append(h_t)\n",
        "      t += batch_size\n",
        "\n",
        "    h_n.append(h_t)\n",
        "    h_n.reverse()\n",
        "\n",
        "    output, h_n = torch.cat(output), torch.cat(h_n)\n",
        "\n",
        "    if self.stohastic_component:\n",
        "      return output, h_n, torch.cat(transition_probabilities)\n",
        "    return output, h_n, None\n",
        "\n",
        "  def forward_impl(self, input, h_t):\n",
        "    h_t_ = self.rnn_cell(input, h_t)\n",
        "    if self.stohastic_component:\n",
        "      transition_probs = self.softmax(\n",
        "        (- torch.pow(self.states - h_t_.unsqueeze(1), 2).sum(2)) / self.temperature\n",
        "      )\n",
        "      return transition_probs, torch.matmul(transition_probs, self.states)\n",
        "    return h_t_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnLM-uoyjSDJ"
      },
      "source": [
        "Test for pack padded sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kndaEv8LjTB7"
      },
      "source": [
        "with torch.no_grad():\n",
        "  for _ in range(1024):\n",
        "\n",
        "    batch_size = random.randint(1, 15)\n",
        "    seq_length = random.randint(1, 15)\n",
        "    input_size = random.randint(1, 5)\n",
        "    hidden_size = random.randint(1, 5)\n",
        "    bias = bool(random.getrandbits(1))\n",
        "    nonlinearity = random.choice(['tanh', 'relu'])\n",
        "    mode = random.choice(['rnn', 'gru'])\n",
        "    number_of_states = random.randint(1, 15)\n",
        "    temperature = random.choice([1.00, 0.5, 0.1])\n",
        "\n",
        "    input = torch.rand(batch_size, seq_length, input_size).to(device)\n",
        "    h_0 = torch.rand(batch_size, hidden_size).to(device)\n",
        "\n",
        "    lengths = torch.randint(1, seq_length + 1, (batch_size,)).to(device)\n",
        "    packed_sequence = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "      input, lengths, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "\n",
        "    # without stohastic component\n",
        "    model = SRRNN(\n",
        "      input_size, hidden_size, bias=bias, mode=mode, nonlinearity=nonlinearity\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    output, h_n, _ = model(input, h_0)\n",
        "\n",
        "    assert h_n.size() == (batch_size, hidden_size)\n",
        "    assert output.size() == (batch_size, seq_length, hidden_size)\n",
        "\n",
        "    output_packed, h_n_packed, _ = model(packed_sequence, h_0)\n",
        "\n",
        "    assert h_n_packed.size() == (batch_size, hidden_size)\n",
        "    assert output_packed.data.size() == (lengths.sum().item(), hidden_size)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      assert torch.allclose(h_n_packed[i,:], output[i, lengths[i].item() - 1, :], atol=1e-07)\n",
        "\n",
        "    # with stohastic component\n",
        "    model = SRRNN(\n",
        "      input_size, hidden_size, bias=bias, mode=mode, nonlinearity=nonlinearity,\n",
        "      number_of_states=number_of_states, temperature=temperature\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    output, h_n, transition_probabilities = model(input, h_0)\n",
        "\n",
        "    assert h_n.size() == (batch_size, hidden_size)\n",
        "    assert output.size() == (batch_size, seq_length, hidden_size)\n",
        "    assert transition_probabilities.size() == (batch_size, seq_length, number_of_states)\n",
        "\n",
        "    output_packed, h_n_packed, transition_probabilities_packed = model(packed_sequence, h_0)\n",
        "\n",
        "    assert h_n_packed.size() == (batch_size, hidden_size)\n",
        "    assert output_packed.data.size() == (lengths.sum().item(), hidden_size)\n",
        "    assert transition_probabilities_packed.data.size() == (lengths.sum().item(), number_of_states)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      assert torch.allclose(h_n_packed[i,:], output[i, lengths[i].item() - 1, :], atol=1e-07)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qAaRQ6-0LOD"
      },
      "source": [
        "class NLNN(nn.Module):\n",
        "\n",
        "  def __init__(self, params):\n",
        "    super(NLNN, self).__init__()\n",
        "\n",
        "    # + 1 because of the padding with 0s\n",
        "    num_embeddings = params['num_embeddings'] + 1\n",
        "\n",
        "    if 'embedding_dim' in params:\n",
        "      embedding_dim = params['embedding_dim']\n",
        "      self.embeddings = nn.Embedding(\n",
        "        num_embeddings,\n",
        "        embedding_dim,\n",
        "        padding_idx = 0\n",
        "      )\n",
        "      input_size = embedding_dim\n",
        "    else:\n",
        "      # one hot encoding\n",
        "      self.embeddings = nn.Embedding(\n",
        "          num_embeddings,\n",
        "          num_embeddings,\n",
        "          padding_idx = 0\n",
        "      )\n",
        "      nn.init.eye_(self.embeddings.weight.data)\n",
        "      self.embeddings.weight.requires_grad = False\n",
        "      input_size = num_embeddings\n",
        "\n",
        "    if 'number_of_states' in params:\n",
        "      self.rnn = SRRNN(\n",
        "        input_size, params['hidden_size'], bias=params['bias'],\n",
        "        mode=params['mode'], nonlinearity=params['nonlinearity'],\n",
        "        number_of_states=params['number_of_states'],\n",
        "        temperature=params['temperature']\n",
        "      )\n",
        "    else:\n",
        "      self.rnn = SRRNN(\n",
        "        input_size, params['hidden_size'], bias=params['bias'],\n",
        "        mode=params['mode'], nonlinearity=params['nonlinearity']\n",
        "      )\n",
        "\n",
        "    self.linear = nn.Linear(\n",
        "      in_features=params['hidden_size'], out_features=2, bias=True\n",
        "    )\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(\n",
        "    self, x, length, return_rnn_output=False, return_probabilities=False\n",
        "  ):\n",
        "    embedding_output = self.embeddings(x)\n",
        "    packed_sequence = nn.utils.rnn.pack_padded_sequence(\n",
        "      embedding_output, length, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "    rnn_output, h_n, transition_probabilities = self.rnn(packed_sequence)\n",
        "    linear_output = self.linear(h_n)\n",
        "    softmax_output = self.softmax(linear_output)\n",
        "\n",
        "    if return_rnn_output and return_probabilities:\n",
        "      return softmax_output, rnn_output, transition_probabilities\n",
        "    elif return_rnn_output:\n",
        "      return softmax_output, rnn_output\n",
        "    elif return_probabilities:\n",
        "      return softmax_output, transition_probabilities\n",
        "    else:\n",
        "      return softmax_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOiy1Kys02Wf"
      },
      "source": [
        "class NLDataset(utils.data.Dataset):\n",
        "\n",
        "  def __init__(self, dataset):\n",
        "    data, length, labels = dataset\n",
        "    self.data = torch.tensor(data).long().to(device)\n",
        "    self.length = torch.tensor(length).long().to(device)\n",
        "    self.labels = torch.tensor(labels).long().to(device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "      'x': self.data[idx],\n",
        "      'length': self.length[idx],\n",
        "      'y': self.labels[idx]\n",
        "    }\n",
        "\n",
        "def load_data(filename_data, filename_length, filename_labels):\n",
        "  return numpy.load(filename_data, allow_pickle=True), \\\n",
        "    numpy.load(filename_length, allow_pickle=True), \\\n",
        "    numpy.load(filename_labels, allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUX8FKwB1SyD"
      },
      "source": [
        "def __evaluate(predictions, labels):\n",
        "  assert(len(predictions) == len(labels))\n",
        "\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "  for prediction, label in zip(predictions, labels):\n",
        "    if label == 1:\n",
        "      if prediction == 1:\n",
        "        tp += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        fp += 1\n",
        "      else:\n",
        "        tn += 1\n",
        "\n",
        "  if tp == 0:\n",
        "    if fn == 0 and fp == 0:\n",
        "      pr, r, f1 = 1, 1, 1\n",
        "    else:\n",
        "      pr, r, f1 = 0, 0, 0\n",
        "  else:\n",
        "    pr = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    f1 = 2 * ((pr * r) / (pr + r))\n",
        "\n",
        "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "  return tp, tn, fp, fn, pr, r, f1, accuracy\n",
        "\n",
        "def _evaluate(data_loader, model):\n",
        "  model.eval()\n",
        "\n",
        "  predictions, labels = [], []\n",
        "\n",
        "  for data in data_loader:\n",
        "    t0 = time.time()\n",
        "    result = model(data['x'], data['length'])\n",
        "    argmax = result.argmax(dim=1).cpu().numpy()\n",
        "    predictions.extend(list(argmax))\n",
        "    labels.extend(list(data['y'].cpu().numpy()))\n",
        "\n",
        "  return __evaluate(predictions, labels)\n",
        "\n",
        "def evaluate_model(data_loader, model, set_name):\n",
        "  tp, tn, fp, fn, pr, r, f1, acc = _evaluate(data_loader, model)\n",
        "  print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "    set_name, tp, tn, fp, fn, pr, r, f1, acc\n",
        "  ))\n",
        "\n",
        "def model_summary(model):\n",
        "  print (model)\n",
        "  print()\n",
        "\n",
        "  print(\"Trainable parameters:\")\n",
        "  for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "      print (\" \", name)\n",
        "  print()\n",
        "\n",
        "  number_of_trainable_parameters = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad\n",
        "  )\n",
        "  print(\"Number of trainable parameters {0:,}\".format(\n",
        "    number_of_trainable_parameters\n",
        "  ))\n",
        "  print()\n",
        "\n",
        "def train_model(\n",
        "    model, params=None, params_path=None, model_path=None,\n",
        "    train_loader=None, dev_loader=None\n",
        "):\n",
        "  model.to(device)\n",
        "  model_summary(model)\n",
        "\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "  best_accuracy, best_state_dict, best_epoch = 0, dict(), -1\n",
        "\n",
        "  # first_batch = next(iter(train_loader))\n",
        "  # print (first_batch)\n",
        "\n",
        "  for epoch in range(params['num_epochs']):\n",
        "    print('Epoch {}/{} : '.format(epoch, params['num_epochs']))\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.train() # set the model to training mode\n",
        "    #for data in [first_batch] * 1:\n",
        "    for data in train_loader:\n",
        "      model.zero_grad()\n",
        "      predictions = model(data['x'], data['length'])\n",
        "      batch_loss = loss_function(predictions, data['y'])\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    _, _, _, _, _, _, _, dev_accuracy = _evaluate(dev_loader, model)\n",
        "\n",
        "    if dev_accuracy > best_accuracy:\n",
        "      best_accuracy = dev_accuracy\n",
        "      best_state_dict = copy.deepcopy(model.state_dict())\n",
        "      best_epoch = epoch\n",
        "\n",
        "    print(\n",
        "      \"dev accuracy:{}\\ttime:{:.2f}s\"\n",
        "      .format(dev_accuracy, time.time() - t0)\n",
        "    )\n",
        "\n",
        "    if model_path:\n",
        "      torch.save(\n",
        "        {\n",
        "          \"epoch\": epoch,\n",
        "          \"model_state_dict\": model.state_dict(),\n",
        "          \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "          \"loss\": loss_function,\n",
        "          \"best_accuracy\": best_accuracy,\n",
        "          \"best_epoch\": best_epoch,\n",
        "          \"best_state_dict\": best_state_dict\n",
        "        },\n",
        "        MODELS_DIR + model_path + str(epoch)\n",
        "      )\n",
        "      if params_path:\n",
        "        with open(MODELS_DIR + params_path, 'wb') as f:\n",
        "          pickle.dump(params, f)\n",
        "\n",
        "    # _, _, _, _, _, _, _, train_accuracy = _evaluate(train_loader, model)\n",
        "\n",
        "    # if train_accuracy > best_accuracy:\n",
        "    #   best_accuracy = train_accuracy\n",
        "    #   best_state_dict = copy.deepcopy(model.state_dict())\n",
        "    #   best_epoch = epoch\n",
        "\n",
        "    # print(\n",
        "    #   \"train accuracy:{}\\ttime:{:.2f}s\"\n",
        "    #   .format(train_accuracy, time.time() - t0)\n",
        "    # )\n",
        "\n",
        "  model.load_state_dict(best_state_dict)\n",
        "  if model_path:\n",
        "    print(\"Best epoch {} and accuracy {}\".format(best_epoch, best_accuracy))\n",
        "    torch.save(model.state_dict(), MODELS_DIR + model_path)\n",
        "    if params_path:\n",
        "      with open(MODELS_DIR + params_path, 'wb') as f:\n",
        "        pickle.dump(params, f)\n",
        "\n",
        "  #evaluate_model([first_batch] * 1, model, \"train\")\n",
        "  evaluate_model(train_loader, model, \"train\")\n",
        "  evaluate_model(dev_loader, model, \"dev\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-anSMTog1LSb",
        "outputId": "30cff2d4-afcb-4643-e5bb-872461812443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "train_data = load_data(\n",
        "  DATA_DIR + \"/cmu/train.data.npy\",\n",
        "  DATA_DIR + \"/cmu/train.length.npy\",\n",
        "  DATA_DIR + \"/cmu/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/cmu/dev.data.npy\",\n",
        "  DATA_DIR + \"/cmu/dev.length.npy\",\n",
        "  DATA_DIR + \"/cmu/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/cmu/test.data.npy\",\n",
        "  DATA_DIR + \"/cmu/test.length.npy\",\n",
        "  DATA_DIR + \"/cmu/test.labels.npy\"\n",
        ")\n",
        "\n",
        "alphabet = {}\n",
        "with open(DATA_DIR + \"/cmu/alphabet.dict\", \"rb\") as f:\n",
        "  alphabet = pickle.load(f)\n",
        "\n",
        "params = {\n",
        "  'batch_size': 100,\n",
        "  'num_epochs': 60,\n",
        "  'num_embeddings': len(alphabet),\n",
        "  'embedding_dim': 10,\n",
        "  'mode': 'rnn',\n",
        "  'nonlinearity': 'relu',\n",
        "  'hidden_size': 300,\n",
        "  'bias': False\n",
        "}\n",
        "\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "model = NLNN(params)\n",
        "train_model(\n",
        "  model,\n",
        "  params=params,\n",
        "  params_path=\"/cmu/rnn.model.params\",\n",
        "  model_path=\"/cmu/rnn.model.pt\",\n",
        "  train_loader=train_loader, dev_loader=dev_loader\n",
        ")\n",
        "#evaluate_model(dev_loader, model, \"dev\")\n",
        "evaluate_model(test_loader, model, \"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-304c176e6774>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_data = load_data(\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/cmu/train.data.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/cmu/train.length.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/cmu/train.labels.npy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuAHR09WBXtD"
      },
      "source": [
        "def load_model(params_path=None, model_path=None):\n",
        "  with open(MODELS_DIR + params_path, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "  model = NLNN(params)\n",
        "  checkpoint = torch.load(MODELS_DIR + model_path, map_location=device)\n",
        "\n",
        "  if 'model_state_dict' in checkpoint:\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  else:\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "  model.to(device)\n",
        "  return model, params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9-hy2t-P6oI",
        "outputId": "9a109e87-39c3-4a83-ad68-3e98b1ad63b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model, params = load_model(\n",
        "    params_path=\"/words/stohastic.rnn.model.params\",\n",
        "    model_path=\"/words/stohastic.rnn.model.pt9\"\n",
        ")\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/words/train.data.npy\",\n",
        "  DATA_DIR + \"/words/train.length.npy\",\n",
        "  DATA_DIR + \"/words/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/words/dev.data.npy\",\n",
        "  DATA_DIR + \"/words/dev.length.npy\",\n",
        "  DATA_DIR + \"/words/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/words/test.data.npy\",\n",
        "  DATA_DIR + \"/words/test.length.npy\",\n",
        "  DATA_DIR + \"/words/test.labels.npy\"\n",
        ")\n",
        "params['batch_size'] = 20\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params['batch_size'], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "\n",
        "evaluate_model(train_loader, model, \"train\")\n",
        "evaluate_model(test_loader, model, \"test\")\n",
        "evaluate_model(test_loader, model, \"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.loss.CrossEntropyLoss' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
            "  warnings.warn(msg, SourceChangeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'best_state_dict': OrderedDict([('embeddings.weight', tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], device='cuda:0')), ('rnn.states', tensor([[ 7.9762e-05, -1.1814e-04,  8.5403e-05,  ..., -4.9699e-05,\n",
            "          2.4633e-04,  6.4858e-05],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.7397e+05,  ...,  3.3397e+05,\n",
            "          1.8389e+05,  1.6554e+04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  7.5808e+04,  ...,  1.4593e+05,\n",
            "          8.0580e+04,  6.6665e+03],\n",
            "        ...,\n",
            "        [ 7.9762e-05, -1.1814e-04,  8.5403e-05,  ..., -4.9699e-05,\n",
            "          2.4633e-04,  6.4858e-05],\n",
            "        [ 7.9762e-05, -1.1814e-04,  8.5403e-05,  ..., -4.9699e-05,\n",
            "          2.4633e-04,  6.4858e-05],\n",
            "        [ 7.9762e-05, -1.1814e-04,  8.5403e-05,  ..., -4.9699e-05,\n",
            "          2.4633e-04,  6.4858e-05]], device='cuda:0')), ('rnn.rnn_cell.weight_ih', tensor([[-1.3461e-39, -2.7153e-03,  1.0736e-09,  ...,  2.0428e-05,\n",
            "         -4.4540e-10, -4.5959e-02],\n",
            "        [ 1.6369e-39,  9.1154e-02, -7.9743e-10,  ...,  2.2491e-05,\n",
            "         -2.3212e-09, -7.6507e-03],\n",
            "        [ 3.2086e-40,  5.1405e-02,  3.3502e-06,  ...,  3.3744e-06,\n",
            "          2.9607e-08,  6.9598e-03],\n",
            "        ...,\n",
            "        [-1.2507e-40,  5.9398e-02, -2.0035e-05,  ..., -9.4086e-05,\n",
            "          3.4069e-08,  2.0846e-02],\n",
            "        [-1.3126e-39,  8.4431e-02, -1.0834e-09,  ..., -3.6884e-06,\n",
            "         -5.4070e-07, -1.0171e-02],\n",
            "        [-1.6910e-39,  1.2267e-01,  7.1414e-06,  ..., -3.9045e-06,\n",
            "         -1.8513e-08, -2.8229e-03]], device='cuda:0')), ('rnn.rnn_cell.weight_hh', tensor([[-9.1834e-02, -3.7876e-04,  1.1282e-02,  ...,  3.9777e-02,\n",
            "          3.0684e-02,  1.0552e-02],\n",
            "        [-3.2193e-02, -6.7151e-02, -7.7104e-02,  ..., -1.1114e-02,\n",
            "          4.5777e-03, -8.8272e-02],\n",
            "        [-4.3330e-02, -2.1629e-02, -2.2749e-02,  ..., -6.5303e-03,\n",
            "          4.7280e-02, -2.5885e-02],\n",
            "        ...,\n",
            "        [-6.1381e-02, -3.3079e-03, -2.3809e-03,  ..., -1.6436e-02,\n",
            "         -1.7667e-02,  1.2483e-03],\n",
            "        [ 8.3804e-02,  1.4118e-03,  4.3417e-02,  ...,  2.3472e-02,\n",
            "         -7.1375e-05,  3.1400e-02],\n",
            "        [-6.6037e-02, -3.4611e-02, -1.5944e-02,  ..., -5.9621e-03,\n",
            "          3.7816e-02, -2.5718e-02]], device='cuda:0')), ('linear.weight', tensor([[-1.2184e-02,  1.2146e-02, -3.6208e-03, -2.1979e-02, -5.1807e-03,\n",
            "         -4.9715e-02, -6.1506e-02,  2.5817e-03,  1.1861e-02,  3.9681e-03,\n",
            "         -1.7523e-02,  2.8840e-03, -3.9636e-03, -6.2211e-04, -7.4769e-03,\n",
            "         -4.5330e-03,  4.2247e-03,  1.5487e-03, -3.3355e-02,  3.3372e-03,\n",
            "         -6.8682e-03, -7.2037e-03, -4.4990e-03, -4.3993e-02, -7.2511e-02,\n",
            "         -2.7370e-02, -1.0294e-01,  1.0236e-02, -7.3365e-03, -7.6525e-03,\n",
            "         -6.7303e-03, -4.3974e-03, -1.5106e-01, -8.5784e-02, -9.1900e-02,\n",
            "         -7.9555e-02,  1.7713e-03, -1.1785e-01, -2.3777e-02, -9.0655e-03,\n",
            "         -2.4129e-02, -1.0536e-01, -3.4464e-02,  8.5544e-03, -1.7670e-02,\n",
            "         -2.1562e-02, -3.9435e-02, -5.3322e-03, -7.6647e-02, -7.2671e-02,\n",
            "         -1.0948e-01, -2.5609e-03, -4.6781e-02, -1.4384e-01, -3.8732e-02,\n",
            "         -5.6250e-02, -1.5218e-02, -1.3318e-03,  2.1255e-03, -1.3181e-02,\n",
            "         -1.4142e-02,  4.9927e-03,  8.9743e-03, -2.0337e-02,  1.2925e-02,\n",
            "         -7.3162e-03, -1.1414e-01, -8.8562e-03, -4.1002e-02, -1.2418e-02,\n",
            "         -1.0534e-01, -1.7186e-03, -7.4805e-02, -3.3098e-02, -4.7833e-03,\n",
            "         -2.1303e-02, -1.9189e-02, -2.0192e-03, -4.0269e-02, -9.6421e-03,\n",
            "         -4.2855e-03, -7.7923e-03, -1.0807e-02, -7.4150e-02, -1.3355e-01,\n",
            "          4.8657e-03, -9.5704e-03, -4.4040e-02, -1.8928e-02, -1.8428e-01,\n",
            "         -9.9428e-03,  4.5673e-03, -1.1321e-02,  1.0803e-02, -3.2801e-02,\n",
            "         -6.1606e-02, -6.1179e-03, -1.0477e-01, -2.4272e-02, -8.7065e-03],\n",
            "        [ 1.1459e-02, -1.2138e-02,  3.6762e-03,  2.1944e-02,  6.0935e-03,\n",
            "          5.0646e-02,  6.1498e-02, -2.1196e-03, -1.1861e-02, -6.3279e-03,\n",
            "          1.7876e-02, -3.1932e-03,  4.9640e-03,  6.2048e-04,  8.1563e-03,\n",
            "          3.7237e-03, -4.1497e-03, -1.3359e-03,  3.3367e-02, -3.3290e-03,\n",
            "          7.8202e-03,  1.0525e-02,  4.6280e-03,  4.3993e-02,  7.1522e-02,\n",
            "          2.7389e-02,  1.0462e-01, -1.0229e-02,  7.3899e-03,  7.1460e-03,\n",
            "          6.4318e-03,  3.3907e-03,  1.5336e-01,  8.3132e-02,  8.7288e-02,\n",
            "          7.7905e-02, -1.4916e-03,  1.1840e-01,  2.3777e-02,  9.0988e-03,\n",
            "          2.4064e-02,  1.0536e-01,  3.4636e-02, -7.6397e-03,  1.7671e-02,\n",
            "          2.1574e-02,  3.9481e-02,  7.0291e-03,  7.6837e-02,  7.2555e-02,\n",
            "          1.0923e-01,  2.7203e-03,  4.6742e-02,  1.4443e-01,  3.9433e-02,\n",
            "          5.6318e-02,  1.5211e-02,  3.9292e-04, -2.9644e-03,  6.9554e-03,\n",
            "          1.4136e-02, -2.5343e-03, -8.9335e-03,  2.0275e-02, -1.2920e-02,\n",
            "          6.9052e-03,  1.1405e-01,  8.6770e-03,  4.1474e-02,  1.2371e-02,\n",
            "          1.0510e-01,  6.7009e-05,  7.4960e-02,  3.3244e-02,  6.7779e-03,\n",
            "          2.1317e-02,  1.9189e-02,  1.7239e-03,  3.9506e-02,  8.8103e-03,\n",
            "          4.8190e-03,  7.7965e-03,  1.0623e-02,  7.4288e-02,  1.3297e-01,\n",
            "         -3.1583e-03,  8.3793e-03,  4.4039e-02,  1.8962e-02,  1.8428e-01,\n",
            "          9.5158e-03, -4.5299e-03,  1.1017e-02, -1.0803e-02,  3.2822e-02,\n",
            "          6.3603e-02,  6.2995e-03,  1.0503e-01,  2.2737e-02,  8.6533e-03]],\n",
            "       device='cuda:0')), ('linear.bias', tensor([ 0.1222, -0.1273], device='cuda:0'))]), 'epoch': 9, 'optimizer_state_dict': {'state': {140587183809520: {'exp_avg': tensor([[-1.3931e-11, -5.3492e-12, -3.1719e-12,  ..., -3.5102e-11,\n",
            "          2.5793e-11, -3.2645e-10],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.7397e+00,  ...,  3.3397e+00,\n",
            "          1.8389e+00,  1.6187e-01],\n",
            "        [ 0.0000e+00,  0.0000e+00,  7.5808e-01,  ...,  1.4593e+00,\n",
            "          8.0580e-01,  6.3845e-02],\n",
            "        ...,\n",
            "        [-1.3931e-11, -5.3492e-12, -3.1719e-12,  ..., -3.5102e-11,\n",
            "          2.5793e-11, -3.2645e-10],\n",
            "        [-1.3931e-11, -5.3492e-12, -3.1719e-12,  ..., -3.5102e-11,\n",
            "          2.5793e-11, -3.2645e-10],\n",
            "        [-1.3931e-11, -5.3492e-12, -3.1719e-12,  ..., -3.5102e-11,\n",
            "          2.5793e-11, -3.2645e-10]], device='cuda:0'), 'exp_avg_sq': tensor([[9.6367e-17, 4.9094e-17, 1.8491e-17,  ..., 9.0186e-17, 9.3291e-17,\n",
            "         8.2863e-17],\n",
            "        [0.0000e+00, 0.0000e+00, 3.0266e+00,  ..., 1.1154e+01, 3.3814e+00,\n",
            "         2.6206e-02],\n",
            "        [0.0000e+00, 0.0000e+00, 5.7469e-01,  ..., 2.1297e+00, 6.4931e-01,\n",
            "         4.0775e-03],\n",
            "        ...,\n",
            "        [9.6367e-17, 4.9094e-17, 1.8491e-17,  ..., 9.0186e-17, 9.3291e-17,\n",
            "         8.2863e-17],\n",
            "        [9.6367e-17, 4.9094e-17, 1.8491e-17,  ..., 9.0186e-17, 9.3291e-17,\n",
            "         8.2863e-17],\n",
            "        [9.6367e-17, 4.9094e-17, 1.8491e-17,  ..., 9.0186e-17, 9.3291e-17,\n",
            "         8.2863e-17]], device='cuda:0'), 'step': 577460}, 140587183811032: {'exp_avg': tensor([[-7.0065e-45,  8.9117e-14, -1.7909e-14,  ...,  2.8071e-18,\n",
            "          3.9355e-17, -1.2141e-13],\n",
            "        [ 7.0065e-45, -4.4833e-13, -2.9468e-14,  ...,  1.0820e-18,\n",
            "         -4.3020e-19, -2.5981e-13],\n",
            "        [ 0.0000e+00, -2.8572e-14,  1.1205e-14,  ..., -5.8424e-17,\n",
            "          1.3109e-17, -3.8194e-13],\n",
            "        ...,\n",
            "        [-0.0000e+00,  9.5195e-13, -4.3238e-14,  ...,  2.1462e-16,\n",
            "          8.6367e-17, -2.8759e-13],\n",
            "        [-7.0065e-45,  3.7465e-12,  3.2948e-14,  ..., -4.3112e-17,\n",
            "          5.7123e-14,  9.6977e-13],\n",
            "        [-7.0065e-45,  6.5300e-11, -6.4805e-13,  ..., -5.8888e-15,\n",
            "         -3.1874e-12, -2.4999e-11]], device='cuda:0'), 'exp_avg_sq': tensor([[0.0000e+00, 8.5907e-19, 1.1542e-20,  ..., 2.6209e-21, 4.1000e-21,\n",
            "         2.2618e-19],\n",
            "        [0.0000e+00, 1.0064e-19, 2.3823e-22,  ..., 5.0760e-22, 2.2379e-22,\n",
            "         5.3877e-20],\n",
            "        [0.0000e+00, 2.9510e-19, 1.5354e-21,  ..., 8.4133e-22, 1.2703e-21,\n",
            "         2.1315e-19],\n",
            "        ...,\n",
            "        [0.0000e+00, 8.1406e-19, 9.6120e-21,  ..., 2.5096e-21, 4.1506e-21,\n",
            "         2.2375e-19],\n",
            "        [0.0000e+00, 7.7300e-19, 4.5886e-21,  ..., 2.4148e-21, 2.7144e-21,\n",
            "         2.7031e-19],\n",
            "        [0.0000e+00, 1.7895e-19, 1.8803e-22,  ..., 1.7979e-22, 1.8648e-21,\n",
            "         1.0246e-19]], device='cuda:0'), 'step': 577460}, 140587183809952: {'exp_avg': tensor([[ 1.6702e-17,  1.2480e-17,  2.1914e-17,  ...,  4.0871e-17,\n",
            "         -3.0462e-17,  1.3131e-15],\n",
            "        [-1.7751e-18, -2.0167e-18, -5.9487e-18,  ..., -4.3613e-18,\n",
            "          3.2438e-18, -2.2295e-16],\n",
            "        [ 2.0867e-17,  1.3311e-17,  1.9880e-17,  ...,  5.0223e-17,\n",
            "         -3.7751e-17,  1.6663e-15],\n",
            "        ...,\n",
            "        [ 3.5063e-17,  2.7057e-17,  4.3579e-17,  ...,  8.5636e-17,\n",
            "         -6.3888e-17,  2.8599e-15],\n",
            "        [-2.4409e-18, -1.4866e-18,  9.3757e-18,  ..., -5.5864e-18,\n",
            "          4.3086e-18, -1.5422e-16],\n",
            "        [ 9.3478e-16,  6.4704e-16,  1.2756e-15,  ...,  2.2530e-15,\n",
            "         -1.6921e-15,  7.6516e-14]], device='cuda:0'), 'exp_avg_sq': tensor([[2.4418e-22, 1.2744e-23, 1.4561e-22,  ..., 2.4248e-22, 2.4356e-22,\n",
            "         1.3766e-23],\n",
            "        [1.5425e-23, 2.3355e-23, 1.4902e-23,  ..., 1.5338e-23, 1.5407e-23,\n",
            "         1.3730e-23],\n",
            "        [9.4927e-23, 1.6102e-23, 7.5215e-23,  ..., 9.5468e-23, 9.5276e-23,\n",
            "         1.4885e-23],\n",
            "        ...,\n",
            "        [1.9215e-22, 1.1011e-23, 1.1242e-22,  ..., 1.9091e-22, 1.9170e-22,\n",
            "         1.2953e-23],\n",
            "        [2.1215e-22, 1.1334e-23, 6.9496e-23,  ..., 2.0640e-22, 2.0943e-22,\n",
            "         1.5897e-23],\n",
            "        [1.2031e-23, 1.0101e-23, 1.0703e-23,  ..., 1.1895e-23, 1.1993e-23,\n",
            "         1.0374e-23]], device='cuda:0'), 'step': 577460}, 140587183810312: {'exp_avg': tensor([[-5.4256e-09, -5.3606e-09, -3.9343e-09,  8.5259e-07,  8.1755e-09,\n",
            "         -7.2396e-10,  3.4067e-08,  1.5274e-05,  2.5552e-10, -1.1115e-05,\n",
            "         -6.1306e-06, -1.5654e-08, -4.4221e-06, -6.6013e-10, -1.9265e-05,\n",
            "          1.3958e-07,  1.7897e-05, -1.6645e-05, -1.0129e-06,  2.2281e-05,\n",
            "         -1.5364e-09, -1.1101e-05,  5.4830e-06, -4.8445e-10, -1.9905e-06,\n",
            "         -2.5798e-08, -1.9068e-05,  8.1869e-11,  9.2378e-06, -4.8166e-07,\n",
            "         -1.1931e-10, -7.7814e-06,  3.4686e-06, -1.8271e-05,  9.9780e-09,\n",
            "          1.8189e-08,  1.9344e-05, -3.5410e-14, -1.8457e-11,  7.7756e-12,\n",
            "          1.8985e-05,  1.7999e-05, -1.7753e-05,  1.6092e-05, -2.0152e-08,\n",
            "          9.0859e-07,  1.9121e-12, -1.5552e-05, -1.6208e-08,  8.4451e-12,\n",
            "         -2.5645e-13,  1.0755e-10,  1.1961e-05,  9.1401e-10, -1.4525e-08,\n",
            "          1.4340e-11, -4.7489e-10,  1.3741e-05, -3.1070e-08,  3.0302e-11,\n",
            "          9.0935e-09, -1.0563e-06,  7.6599e-07,  1.2747e-09, -2.2105e-05,\n",
            "         -6.5902e-11, -3.0868e-06, -2.8345e-09,  4.3231e-08, -6.8227e-11,\n",
            "         -1.7701e-08, -2.0062e-05,  7.0367e-12,  4.2703e-11, -6.0419e-06,\n",
            "         -1.1351e-05,  5.5983e-11, -1.6517e-08,  1.5459e-05,  1.8260e-05,\n",
            "          5.1052e-12, -1.1658e-10, -4.3502e-08, -5.0835e-06,  1.5444e-05,\n",
            "         -1.6659e-09, -1.8792e-05, -1.9089e-05,  4.5389e-10,  1.1212e-09,\n",
            "          1.2440e-05, -1.5852e-08, -5.9527e-07, -2.0985e-05, -1.2978e-09,\n",
            "          6.3761e-08,  1.8935e-05, -1.5352e-08,  1.0650e-08, -2.7560e-08],\n",
            "        [ 5.4256e-09,  5.3605e-09,  3.9343e-09, -8.5259e-07, -8.1755e-09,\n",
            "          7.2404e-10, -3.4067e-08, -1.5274e-05, -2.5547e-10,  1.1115e-05,\n",
            "          6.1306e-06,  1.5654e-08,  4.4221e-06,  6.6012e-10,  1.9265e-05,\n",
            "         -1.3958e-07, -1.7897e-05,  1.6645e-05,  1.0129e-06, -2.2281e-05,\n",
            "          1.5364e-09,  1.1101e-05, -5.4830e-06,  4.8416e-10,  1.9905e-06,\n",
            "          2.5798e-08,  1.9068e-05, -8.1962e-11, -9.2378e-06,  4.8166e-07,\n",
            "          1.1940e-10,  7.7814e-06, -3.4686e-06,  1.8271e-05, -9.9780e-09,\n",
            "         -1.8189e-08, -1.9344e-05,  5.2744e-14,  1.8404e-11, -7.8158e-12,\n",
            "         -1.8985e-05, -1.7999e-05,  1.7753e-05, -1.6092e-05,  2.0152e-08,\n",
            "         -9.0859e-07, -1.9031e-12,  1.5552e-05,  1.6208e-08, -8.4100e-12,\n",
            "          3.2986e-13, -1.0736e-10, -1.1961e-05, -9.1400e-10,  1.4525e-08,\n",
            "         -1.4387e-11,  4.7489e-10, -1.3741e-05,  3.1070e-08, -3.0291e-11,\n",
            "         -9.0935e-09,  1.0563e-06, -7.6599e-07, -1.2747e-09,  2.2105e-05,\n",
            "          6.5971e-11,  3.0868e-06,  2.8345e-09, -4.3231e-08,  6.8163e-11,\n",
            "          1.7701e-08,  2.0062e-05, -7.0888e-12, -4.2721e-11,  6.0419e-06,\n",
            "          1.1351e-05, -5.5982e-11,  1.6517e-08, -1.5459e-05, -1.8260e-05,\n",
            "         -5.1231e-12,  1.1658e-10,  4.3502e-08,  5.0835e-06, -1.5444e-05,\n",
            "          1.6660e-09,  1.8792e-05,  1.9089e-05, -4.5383e-10, -1.1212e-09,\n",
            "         -1.2440e-05,  1.5852e-08,  5.9527e-07,  2.0985e-05,  1.2978e-09,\n",
            "         -6.3761e-08, -1.8935e-05,  1.5352e-08, -1.0650e-08,  2.7560e-08]],\n",
            "       device='cuda:0'), 'exp_avg_sq': tensor([[1.6055e-10, 2.3225e-10, 4.0681e-11, 1.3286e-10, 1.1311e-10, 2.7560e-10,\n",
            "         1.1210e-10, 9.7446e-09, 2.7335e-10, 8.1896e-09, 6.6878e-09, 3.5437e-10,\n",
            "         4.5166e-09, 4.3406e-11, 2.3081e-08, 2.6886e-10, 1.9036e-08, 1.6200e-08,\n",
            "         8.2644e-11, 2.0417e-08, 1.6435e-10, 9.6629e-09, 6.4168e-09, 2.6725e-10,\n",
            "         4.5438e-09, 4.7097e-11, 2.1339e-08, 1.8145e-10, 7.1856e-09, 6.2774e-11,\n",
            "         8.7802e-11, 7.4952e-09, 1.9752e-09, 1.2669e-08, 1.5729e-10, 7.4269e-10,\n",
            "         2.3357e-08, 3.0866e-10, 2.3659e-10, 2.0427e-10, 1.3437e-08, 1.5249e-08,\n",
            "         1.5802e-08, 1.9100e-08, 4.1747e-11, 5.7655e-10, 1.8583e-10, 1.6704e-08,\n",
            "         1.5170e-10, 1.8390e-10, 2.2801e-10, 1.8069e-10, 8.5633e-09, 5.5343e-11,\n",
            "         1.4772e-10, 2.3862e-10, 9.8180e-11, 8.9111e-09, 4.5169e-11, 7.2938e-11,\n",
            "         1.1332e-10, 2.5934e-09, 3.8852e-09, 1.6471e-10, 1.9974e-08, 2.4027e-10,\n",
            "         5.0673e-09, 1.6286e-10, 1.2431e-10, 2.4046e-10, 2.4964e-09, 1.4895e-08,\n",
            "         2.0267e-10, 1.8926e-10, 5.5235e-09, 1.0191e-08, 3.2232e-11, 1.5054e-10,\n",
            "         1.6314e-08, 1.8896e-08, 2.3125e-10, 8.7611e-11, 1.1605e-10, 5.8242e-09,\n",
            "         1.6206e-08, 1.0448e-10, 1.8772e-08, 2.0885e-08, 1.6849e-10, 4.6107e-11,\n",
            "         8.3694e-09, 1.5232e-10, 7.3084e-10, 1.6723e-08, 8.3388e-11, 4.0452e-10,\n",
            "         2.0311e-08, 1.5298e-10, 1.5682e-10, 3.4900e-10],\n",
            "        [1.6055e-10, 2.3225e-10, 4.0681e-11, 1.3286e-10, 1.1311e-10, 2.7560e-10,\n",
            "         1.1210e-10, 9.7446e-09, 2.7335e-10, 8.1896e-09, 6.6878e-09, 3.5437e-10,\n",
            "         4.5166e-09, 4.3406e-11, 2.3081e-08, 2.6886e-10, 1.9036e-08, 1.6200e-08,\n",
            "         8.2644e-11, 2.0417e-08, 1.6435e-10, 9.6629e-09, 6.4168e-09, 2.6725e-10,\n",
            "         4.5438e-09, 4.7097e-11, 2.1339e-08, 1.8145e-10, 7.1856e-09, 6.2774e-11,\n",
            "         8.7802e-11, 7.4952e-09, 1.9752e-09, 1.2669e-08, 1.5729e-10, 7.4269e-10,\n",
            "         2.3357e-08, 3.0866e-10, 2.3659e-10, 2.0427e-10, 1.3437e-08, 1.5249e-08,\n",
            "         1.5802e-08, 1.9100e-08, 4.1747e-11, 5.7655e-10, 1.8583e-10, 1.6704e-08,\n",
            "         1.5170e-10, 1.8390e-10, 2.2801e-10, 1.8069e-10, 8.5633e-09, 5.5343e-11,\n",
            "         1.4772e-10, 2.3862e-10, 9.8180e-11, 8.9111e-09, 4.5169e-11, 7.2938e-11,\n",
            "         1.1332e-10, 2.5934e-09, 3.8852e-09, 1.6471e-10, 1.9974e-08, 2.4027e-10,\n",
            "         5.0673e-09, 1.6286e-10, 1.2431e-10, 2.4046e-10, 2.4964e-09, 1.4895e-08,\n",
            "         2.0267e-10, 1.8926e-10, 5.5235e-09, 1.0191e-08, 3.2232e-11, 1.5054e-10,\n",
            "         1.6314e-08, 1.8896e-08, 2.3125e-10, 8.7611e-11, 1.1605e-10, 5.8242e-09,\n",
            "         1.6206e-08, 1.0448e-10, 1.8772e-08, 2.0885e-08, 1.6849e-10, 4.6107e-11,\n",
            "         8.3694e-09, 1.5232e-10, 7.3084e-10, 1.6723e-08, 8.3388e-11, 4.0452e-10,\n",
            "         2.0311e-08, 1.5298e-10, 1.5682e-10, 3.4900e-10]], device='cuda:0'), 'step': 577460}, 140587183811104: {'exp_avg': tensor([-0.0012,  0.0012], device='cuda:0'), 'exp_avg_sq': tensor([0.0032, 0.0032], device='cuda:0'), 'step': 577460}}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'amsgrad': False, 'weight_decay': 1e-05, 'params': [140587183812400, 140587183809520, 140587183811032, 140587183809952, 140587183810312, 140587183811104], 'eps': 1e-08}]}, 'best_accuracy': 0.5286498018896678, 'best_epoch': 4, 'loss': CrossEntropyLoss(), 'model_state_dict': OrderedDict([('embeddings.weight', tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], device='cuda:0')), ('rnn.states', tensor([[ 1.6347e-06,  6.8983e-07,  1.9289e-07,  ...,  4.3210e-06,\n",
            "         -3.0988e-06,  5.0796e-05],\n",
            "        [ 0.0000e+00,  0.0000e+00,  1.7397e+05,  ...,  3.3397e+05,\n",
            "          1.8389e+05,  1.6187e+04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  7.5808e+04,  ...,  1.4593e+05,\n",
            "          8.0580e+04,  6.3845e+03],\n",
            "        ...,\n",
            "        [ 1.6347e-06,  6.8983e-07,  1.9289e-07,  ...,  4.3210e-06,\n",
            "         -3.0988e-06,  5.0796e-05],\n",
            "        [ 1.6347e-06,  6.8983e-07,  1.9289e-07,  ...,  4.3210e-06,\n",
            "         -3.0988e-06,  5.0796e-05],\n",
            "        [ 1.6347e-06,  6.8983e-07,  1.9289e-07,  ...,  4.3210e-06,\n",
            "         -3.0988e-06,  5.0796e-05]], device='cuda:0')), ('rnn.rnn_cell.weight_ih', tensor([[-1.3461e-39,  4.7463e-07, -5.3478e-09,  ..., -4.6339e-13,\n",
            "          5.5486e-12, -1.3848e-07],\n",
            "        [ 1.6369e-39, -4.8020e-09, -1.9285e-08,  ...,  2.0340e-13,\n",
            "          4.2178e-15, -1.2301e-07],\n",
            "        [ 3.2086e-40,  9.0380e-08,  3.1539e-09,  ...,  1.2201e-11,\n",
            "          8.8618e-13, -2.0992e-07],\n",
            "        ...,\n",
            "        [-1.2507e-40,  2.9937e-07, -1.2813e-08,  ..., -5.8810e-11,\n",
            "          1.2209e-11, -3.3833e-07],\n",
            "        [-1.3126e-39,  4.5171e-07,  9.5184e-09,  ...,  3.8914e-11,\n",
            "          2.9616e-09, -1.7603e-07],\n",
            "        [-1.6910e-39,  1.1179e-05, -1.7869e-07,  ..., -1.0847e-08,\n",
            "         -1.5462e-07, -1.1734e-05]], device='cuda:0')), ('rnn.rnn_cell.weight_hh', tensor([[ 3.6069e-12,  2.2806e-12,  3.3908e-12,  ...,  9.0482e-12,\n",
            "         -6.6603e-12,  2.0811e-10],\n",
            "        [-5.0402e-13, -1.0671e-12, -2.0502e-12,  ..., -1.1789e-12,\n",
            "          8.9949e-13, -1.3357e-10],\n",
            "        [ 3.5248e-12,  1.9320e-12,  1.5065e-12,  ...,  8.3581e-12,\n",
            "         -6.3307e-12,  2.8049e-10],\n",
            "        ...,\n",
            "        [ 6.8927e-12,  3.6832e-12,  4.9863e-12,  ...,  1.7585e-11,\n",
            "         -1.2836e-11,  2.6652e-10],\n",
            "        [-7.9714e-12, -5.6202e-12, -1.0386e-11,  ..., -1.9735e-11,\n",
            "          1.4622e-11, -5.9478e-10],\n",
            "        [ 6.5233e-11,  5.8753e-11,  6.1973e-11,  ...,  1.6697e-10,\n",
            "         -1.2167e-10,  5.1982e-09]], device='cuda:0')), ('linear.weight', tensor([[-5.4135e-05, -2.4996e-05,  7.2201e-06,  3.4654e-03,  4.6776e-05,\n",
            "          1.2894e-05,  8.0696e-05,  2.3895e-02, -1.8176e-05, -1.6271e-02,\n",
            "         -8.8289e-03,  8.1478e-05, -6.2470e-03,  1.5405e-06, -7.3486e-02,\n",
            "          6.4791e-04,  4.9789e-02, -3.9091e-02, -2.4655e-03,  7.0703e-02,\n",
            "         -1.6213e-05, -1.9687e-02,  2.0643e-02,  2.3845e-05, -6.8812e-03,\n",
            "         -4.1415e-05, -6.8155e-02,  1.1651e-06,  3.2294e-02, -9.6694e-04,\n",
            "         -4.6301e-07, -2.3581e-02,  1.3577e-02, -3.8774e-02,  9.5056e-05,\n",
            "          5.8744e-05,  7.6153e-02,  2.9771e-09,  5.7266e-06,  2.1810e-07,\n",
            "          4.2535e-02,  5.0854e-02, -4.6409e-02,  5.3157e-02,  1.2403e-05,\n",
            "         -6.8572e-04,  3.0885e-08, -4.4622e-02, -1.4322e-04,  1.2898e-07,\n",
            "         -1.8208e-08,  1.5241e-06,  1.9525e-02,  2.4363e-06, -1.2138e-04,\n",
            "         -2.4637e-06, -2.3345e-06,  2.1602e-02,  8.0018e-05,  1.1281e-07,\n",
            "          5.2133e-05, -1.1086e-02,  3.5174e-03,  1.3532e-05, -6.8106e-02,\n",
            "          9.7720e-06, -6.9335e-03, -2.9247e-05,  2.7771e-04,  9.6508e-06,\n",
            "         -2.1375e-04, -4.9237e-02,  1.8547e-07,  7.5408e-07, -1.0323e-02,\n",
            "         -2.0402e-02,  2.7435e-07, -1.4379e-04,  4.3615e-02,  5.2582e-02,\n",
            "          2.0318e-06, -4.5547e-07, -1.0742e-04, -8.6199e-03,  4.3597e-02,\n",
            "         -8.7305e-06, -6.3183e-02, -6.9747e-02,  5.1142e-06, -1.0833e-05,\n",
            "          2.0190e-02, -1.4122e-04,  4.5233e-04, -5.6166e-02, -5.4558e-06,\n",
            "         -2.7796e-04,  6.4982e-02, -1.3797e-04,  1.0081e-04, -2.1938e-03],\n",
            "        [ 5.4134e-05,  2.4994e-05, -7.2204e-06, -3.4654e-03, -4.6774e-05,\n",
            "         -1.2886e-05, -8.0692e-05, -2.3895e-02,  1.8181e-05,  1.6271e-02,\n",
            "          8.8290e-03, -8.1492e-05,  6.2470e-03, -1.5413e-06,  7.3486e-02,\n",
            "         -6.4792e-04, -4.9789e-02,  3.9091e-02,  2.4655e-03, -7.0703e-02,\n",
            "          1.6219e-05,  1.9687e-02, -2.0643e-02, -2.3873e-05,  6.8812e-03,\n",
            "          4.1415e-05,  6.8155e-02, -1.1743e-06, -3.2294e-02,  9.6695e-04,\n",
            "          4.7180e-07,  2.3581e-02, -1.3577e-02,  3.8774e-02, -9.5058e-05,\n",
            "         -5.8753e-05, -7.6153e-02, -1.2536e-09, -5.7320e-06, -2.2209e-07,\n",
            "         -4.2535e-02, -5.0854e-02,  4.6409e-02, -5.3157e-02, -1.2403e-05,\n",
            "          6.8570e-04, -2.9999e-08,  4.4622e-02,  1.4322e-04, -1.2550e-07,\n",
            "          2.5500e-08, -1.5061e-06, -1.9525e-02, -2.4346e-06,  1.2138e-04,\n",
            "          2.4591e-06,  2.3347e-06, -2.1602e-02, -8.0016e-05, -1.1170e-07,\n",
            "         -5.2135e-05,  1.1086e-02, -3.5174e-03, -1.3524e-05,  6.8106e-02,\n",
            "         -9.7653e-06,  6.9335e-03,  2.9250e-05, -2.7771e-04, -9.6570e-06,\n",
            "          2.1374e-04,  4.9237e-02, -1.9064e-07, -7.5587e-07,  1.0323e-02,\n",
            "          2.0402e-02, -2.7428e-07,  1.4379e-04, -4.3615e-02, -5.2582e-02,\n",
            "         -2.0336e-06,  4.5603e-07,  1.0742e-04,  8.6199e-03, -4.3597e-02,\n",
            "          8.7325e-06,  6.3183e-02,  6.9748e-02, -5.1091e-06,  1.0833e-05,\n",
            "         -2.0190e-02,  1.4123e-04, -4.5232e-04,  5.6166e-02,  5.4563e-06,\n",
            "          2.7796e-04, -6.4982e-02,  1.3798e-04, -1.0081e-04,  2.1938e-03]],\n",
            "       device='cuda:0')), ('linear.bias', tensor([ 0.0095, -0.0143], device='cuda:0'))])}\n",
            "train : TP : 0 TN : 577343 FP : 0 FN : 577574 Pr : 0 R : 0 F1: 0 ACC : 0.49989999281333636 \n",
            "test : TP : 0 TN : 72176 FP : 0 FN : 72189 Pr : 0 R : 0 F1: 0 ACC : 0.49995497523638 \n",
            "test : TP : 0 TN : 72176 FP : 0 FN : 72189 Pr : 0 R : 0 F1: 0 ACC : 0.49995497523638 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HVfo4wbNPWD"
      },
      "source": [
        "Get vectors from pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgFdeeanNONL"
      },
      "source": [
        "def get_model_vectors(model, params, dataset_loader):\n",
        "\n",
        "  with torch.no_grad():\n",
        "    vectors, dataset_size = [], 0\n",
        "\n",
        "    for data in train_loader:\n",
        "      x, length = data['x'], data['length']\n",
        "      dataset_size += x.size(0)\n",
        "      _, rnn_output = model(x, length, return_rnn_output=True)\n",
        "      softmax_output =  model.softmax(model.linear(rnn_output.data))\n",
        "      vectors.extend(\n",
        "        torch.cat((rnn_output.data, softmax_output), dim=1).cpu().detach().numpy()\n",
        "      )\n",
        "\n",
        "    h_0 = torch.zeros((1, params['hidden_size']), device=device)\n",
        "    h_0 = torch.cat((h_0, model.softmax(model.linear(h_0))), dim=1)\n",
        "    vectors.extend(\n",
        "      h_0.expand(dataset_size, params['hidden_size'] + 2).cpu().detach().numpy()\n",
        "    )\n",
        "\n",
        "    vectors, counts = numpy.unique(numpy.array(vectors), axis=0, return_counts=True)\n",
        "\n",
        "    print(\"Number of unique vectors {0:,}\".format(vectors.shape[0]))\n",
        "    print(\"Number of total vectors {0:,}\".format(numpy.sum(counts)))\n",
        "\n",
        "  return vectors, counts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT6dazLMrjxe"
      },
      "source": [
        "model, params = load_model(\n",
        "    params_path=\"/numeral-50/rnn.model.params\",\n",
        "    model_path=\"/numeral-50/rnn.model.pt\"\n",
        ")\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/numeral-50/train.data.npy\",\n",
        "  DATA_DIR + \"/numeral-50/train.length.npy\",\n",
        "  DATA_DIR + \"/numeral-50/train.labels.npy\"\n",
        ")\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=500, shuffle = False\n",
        ")\n",
        "\n",
        "vectors, counts = get_model_vectors(model, params, train_loader)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "fig = plt.figure(figsize=(50, 50))\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(vectors)\n",
        "\n",
        "t = reduced.transpose()\n",
        "\n",
        "plt.scatter(t[0], t[1])\n",
        "plt.show()\n",
        "\n",
        "# numpy.savetxt(MODELS_DIR + \"/words/rnn.model.vectors.npy.gz\", vectors)\n",
        "# numpy.savetxt(MODELS_DIR + \"/words/rnn.model.vectors.counts.npy.gz\", counts, fmt='%d')\n",
        "# numpy.save(MODELS_DIR + \"/words/rnn.model.vectors.npy\", vectors)\n",
        "# numpy.save(MODELS_DIR + \"/words/rnn.model.vectors.weights.npy\", counts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fprBS8yOteJS"
      },
      "source": [
        "def read_centroids(file):\n",
        "  centroids = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        centroids.append([float(p) for p in line.split()])\n",
        "\n",
        "  print(\"Number of centroids: {0:,}\".format(len(centroids)))\n",
        "\n",
        "  return torch.tensor(centroids).cuda()\n",
        "\n",
        "centroids = read_centroids(MODELS_DIR + \"/numeral-50/rnn.model.vectors.txt.centroids.txt\")\n",
        "print (centroids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B6kjZM6rZdt"
      },
      "source": [
        "Silhouette scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_Q01Tc2rU1w"
      },
      "source": [
        "for n_clusters in list(range(2, 4)):\n",
        "\n",
        "  km = KMeans(\n",
        "      n_clusters=n_clusters,\n",
        "      init=\"k-means++\",\n",
        "      n_init=1,\n",
        "      tol=1e-7,\n",
        "      random_state=0,\n",
        "      precompute_distances=False,\n",
        "      verbose=0,\n",
        "      algorithm=\"full\"\n",
        "  )\n",
        "  labels = km.fit_predict(vectors)\n",
        "  centroids = km.cluster_centers_\n",
        "\n",
        "  number_of_points_in_cluster = [0] * n_clusters\n",
        "  for label in labels:\n",
        "    number_of_points_in_cluster[label] += 1\n",
        "\n",
        "  from scipy.spatial import distance\n",
        "\n",
        "  a = [0] * vectors.shape[0]\n",
        "\n",
        "  for i, vector_i in enumerate(vectors):\n",
        "    centroid = labels[i]\n",
        "    if number_of_points_in_cluster[centroid] > 1:\n",
        "      sum_distances = 0\n",
        "      for j, vector_j in enumerate(vectors):\n",
        "        if i != j and labels[j] == centroid:\n",
        "          sum_distances += distance.euclidean(vector_i, vector_j)\n",
        "\n",
        "      a[i] = sum_distances / (number_of_points_in_cluster[centroid] - 1)\n",
        "\n",
        "  b = [0] * vectors.shape[0]\n",
        "\n",
        "  for i, vector_i in enumerate(vectors):\n",
        "    b_i = [0] * n_clusters\n",
        "    centroid = labels[i]\n",
        "    for j, vector_j in enumerate(vectors):\n",
        "      if labels[j] != centroid:\n",
        "        b_i[labels[j]] += distance.euclidean(vector_i, vector_j)\n",
        "    b_i = [bi / number_of_points_in_cluster[j] for j, bi in enumerate(b_i)]\n",
        "    b_i.pop(centroid)\n",
        "    b[i] = min(b_i)\n",
        "\n",
        "  s = [0] * vectors.shape[0]\n",
        "  for i in range(0, vectors.shape[0]):\n",
        "    if number_of_points_in_cluster[labels[i]] > 1:\n",
        "      s[i] = (b[i] - a[i]) / max(a[i], b[i])\n",
        "\n",
        "  print(s)\n",
        "  print (n_clusters, sum(s) / len(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZOWoW4tbo-e"
      },
      "source": [
        "Train stohastic model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8Ds6I7bjm6",
        "outputId": "b9d9e734-9e3a-4225-c5e6-94e3328bad46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model, params = load_model(\n",
        "    params_path=\"/words/rnn.model.params\",\n",
        "    model_path=\"/words/rnn.model.pt\"\n",
        ")\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/words/train.data.npy\",\n",
        "  DATA_DIR + \"/words/train.length.npy\",\n",
        "  DATA_DIR + \"/words/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/words/dev.data.npy\",\n",
        "  DATA_DIR + \"/words/dev.length.npy\",\n",
        "  DATA_DIR + \"/words/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/words/test.data.npy\",\n",
        "  DATA_DIR + \"/words/test.length.npy\",\n",
        "  DATA_DIR + \"/words/test.labels.npy\"\n",
        ")\n",
        "params['batch_size'] = 20\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params['batch_size'], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params['batch_size'], shuffle = False\n",
        ")\n",
        "\n",
        "params['num_epochs'] = 20\n",
        "params['number_of_states'] = 60000\n",
        "params['temperature'] = 0.01\n",
        "\n",
        "stohastic_model = NLNN(params)\n",
        "stohastic_model.load_state_dict(model.state_dict(), strict=False)\n",
        "stohastic_model.embeddings.weight.requires_grad=False\n",
        "print(params)\n",
        "model_summary(stohastic_model)\n",
        "\n",
        "centroids = numpy.load(MODELS_DIR + \"/words/mini-clusters.npy\")\n",
        "stohastic_model.rnn.states.data = torch.from_numpy(centroids[:,:-2]).float().to(device)\n",
        "stohastic_model.to(device)\n",
        "\n",
        "train_model(\n",
        "  stohastic_model,\n",
        "  params=params,\n",
        "  params_path=\"/words/stohastic.rnn.model.params\",\n",
        "  model_path=\"/words/stohastic.rnn.model.pt\",\n",
        "  train_loader=train_loader, dev_loader=dev_loader\n",
        ")\n",
        "evaluate_model(test_loader, stohastic_model, \"test\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'temperature': 0.01, 'num_embeddings': 30, 'number_of_states': 60000, 'num_epochs': 20, 'mode': 'rnn', 'hidden_size': 100, 'nonlinearity': 'relu', 'bias': False, 'batch_size': 20}\n",
            "NLNN(\n",
            "  (embeddings): Embedding(31, 31, padding_idx=0)\n",
            "  (rnn): SRRNN(\n",
            "    number_of_states=60000, temperature=0.01\n",
            "    (rnn_cell): RNNCell(31, 100, bias=False, nonlinearity=relu)\n",
            "    (softmax): Softmax(dim=1)\n",
            "  )\n",
            "  (linear): Linear(in_features=100, out_features=2, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "\n",
            "Trainable parameters:\n",
            "  rnn.states\n",
            "  rnn.rnn_cell.weight_ih\n",
            "  rnn.rnn_cell.weight_hh\n",
            "  linear.weight\n",
            "  linear.bias\n",
            "\n",
            "Number of trainable parameters 6,013,302\n",
            "\n",
            "NLNN(\n",
            "  (embeddings): Embedding(31, 31, padding_idx=0)\n",
            "  (rnn): SRRNN(\n",
            "    number_of_states=60000, temperature=0.01\n",
            "    (rnn_cell): RNNCell(31, 100, bias=False, nonlinearity=relu)\n",
            "    (softmax): Softmax(dim=1)\n",
            "  )\n",
            "  (linear): Linear(in_features=100, out_features=2, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "\n",
            "Trainable parameters:\n",
            "  rnn.states\n",
            "  rnn.rnn_cell.weight_ih\n",
            "  rnn.rnn_cell.weight_hh\n",
            "  linear.weight\n",
            "  linear.bias\n",
            "\n",
            "Number of trainable parameters 6,013,302\n",
            "\n",
            "Epoch 0/20 : \n",
            "dev accuracy:0.5180654456789782\ttime:278718.75s\n",
            "Epoch 1/20 : \n",
            "dev accuracy:0.5183148153279211\ttime:279073.85s\n",
            "Epoch 2/20 : \n",
            "dev accuracy:0.5008450860325289\ttime:322765.26s\n",
            "Epoch 3/20 : \n",
            "dev accuracy:0.5181000803524425\ttime:328087.02s\n",
            "Epoch 4/20 : \n",
            "dev accuracy:0.5286498018896678\ttime:388266.24s\n",
            "Epoch 5/20 : \n",
            "dev accuracy:0.5164099082873846\ttime:427350.51s\n",
            "Epoch 6/20 : \n",
            "dev accuracy:0.49915491396747114\ttime:396053.64s\n",
            "Epoch 7/20 : \n",
            "dev accuracy:0.5008450860325289\ttime:370406.99s\n",
            "Epoch 8/20 : \n",
            "dev accuracy:0.5008450860325289\ttime:329509.27s\n",
            "Epoch 9/20 : \n",
            "dev accuracy:0.5008450860325289\ttime:348679.79s\n",
            "Epoch 10/20 : \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zn8elRZ2Vd7"
      },
      "source": [
        "class Automaton():\n",
        "\n",
        "  def __init__(self, initial_state, transitions, is_final):\n",
        "    self.initial_state = initial_state\n",
        "    self.is_final = is_final\n",
        "\n",
        "    self.number_of_transitions = len(transitions)\n",
        "    self.number_of_states = len(is_final)\n",
        "\n",
        "    self.state_first_transition = [-1] * self.number_of_states\n",
        "    self.state_number_of_transitions = [0] * self.number_of_states\n",
        "    self.transitions_from = [-1] * self.number_of_transitions\n",
        "    self.transitions_to = [-1] * self.number_of_transitions\n",
        "    self.transitions_label = [-1] * self.number_of_transitions\n",
        "\n",
        "    def compare(transition1, transition2):\n",
        "      q_11, a1, q_21 = transition1\n",
        "      q_12, a2, q_22 = transition2\n",
        "      eq_ = q_11 - q_12\n",
        "      if eq_ == 0:\n",
        "        return ord(a1) - ord(a2)\n",
        "      return eq_\n",
        "\n",
        "    import functools\n",
        "    sorted_transtions = sorted(transitions, key=functools.cmp_to_key(compare))\n",
        "\n",
        "    state_index = -1\n",
        "    transition_index = 0\n",
        "\n",
        "    for transition in sorted_transtions:\n",
        "      q_1, a, q_2 = transition\n",
        "\n",
        "      if q_1 != state_index:\n",
        "        self.state_first_transition[q_1] = transition_index\n",
        "        state_index = q_1\n",
        "      self.transitions_from[transition_index] = q_1\n",
        "      self.transitions_to[transition_index] = q_2\n",
        "      self.transitions_label[transition_index] = a\n",
        "      transition_index += 1\n",
        "      self.state_number_of_transitions[q_1] += 1\n",
        "\n",
        "  def __print__(self):\n",
        "    print(\"===============\")\n",
        "    print(\"number_of_states\", self.number_of_states)\n",
        "    print(\"number_of_transitions\", self.number_of_transitions)\n",
        "    print(\"initial_state\", self.initial_state)\n",
        "    print(\"is_final\", list(enumerate(self.is_final)))\n",
        "    print(\"state_first_transition\", list(enumerate(self.state_first_transition)))\n",
        "    print(\"state_number_of_transitions\", list(enumerate(self.state_number_of_transitions)))\n",
        "    print(\"transitions_from\", list(enumerate(self.transitions_from)))\n",
        "    print(\"transitions_label\", list(enumerate(self.transitions_label)))\n",
        "    print(\"transitions_to\", list(enumerate(self.transitions_to)))\n",
        "    print(\"===============\")\n",
        "\n",
        "  def is_deterministic(self):\n",
        "    # dummy implementation\n",
        "    for transition_index in range(self.number_of_transitions):\n",
        "      transition_from = self.transitions_from[transition_index]\n",
        "      transition_label = self.transitions_label[transition_index]\n",
        "\n",
        "      for transition_index2 in range(self.number_of_transitions):\n",
        "        if transition_index != transition_index2:\n",
        "          transition_from2 = self.transitions_from[transition_index2]\n",
        "          transition_label2 = self.transitions_label[transition_index2]\n",
        "          if transition_from == transition_from2 and transition_label == transition_label2:\n",
        "            print (\"not deterministic \", transition_index, transition_index2, transition_from2, transition_label2, self.transitions_to[transition_index], self.transitions_to[transition_index2] )\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "  def contains_cycle(self):\n",
        "    def dfs(stack, visited):\n",
        "      while len(stack) > 0:\n",
        "        top = stack.pop()\n",
        "        visited[top] = True\n",
        "\n",
        "        for i in range(self.state_number_of_transitions[top]):\n",
        "          adjacent = self.transitions_to[self.state_first_transition[top] + i]\n",
        "          if not visited[adjacent]:\n",
        "            stack.append(adjacent)\n",
        "          else:\n",
        "            return True\n",
        "      return False\n",
        "\n",
        "    visited = [False] * self.number_of_states\n",
        "    stack = []\n",
        "    stack.append(self.initial_state)\n",
        "\n",
        "    return dfs(stack, visited)\n",
        "\n",
        "  def reachable_states(self):\n",
        "    def dfs(stack, visited):\n",
        "      while len(stack) > 0:\n",
        "        top = stack.pop()\n",
        "        visited[top] = True\n",
        "\n",
        "        for i in range(self.state_number_of_transitions[top]):\n",
        "          adjacent = self.transitions_to[self.state_first_transition[top] + i]\n",
        "          if not visited[adjacent]:\n",
        "            stack.append(adjacent)\n",
        "\n",
        "    visited = [False] * self.number_of_states\n",
        "    stack = []\n",
        "    stack.append(self.initial_state)\n",
        "\n",
        "    dfs(stack, visited)\n",
        "\n",
        "    reachable_states = set()\n",
        "    for state in range(self.number_of_states):\n",
        "      if visited[state]:\n",
        "        reachable_states.add(state)\n",
        "\n",
        "    return reachable_states\n",
        "\n",
        "  def coreachable_states(self):\n",
        "\n",
        "    def is_coreachable(state):\n",
        "      visited = [False] * self.number_of_states\n",
        "      stack = []\n",
        "\n",
        "      stack.append(state)\n",
        "\n",
        "      while len(stack) > 0:\n",
        "        top = stack.pop()\n",
        "        visited[top] = True\n",
        "\n",
        "        if self.is_final[top] == 1:\n",
        "            return True\n",
        "\n",
        "        for i in range(self.state_number_of_transitions[top]):\n",
        "          adjacent = self.transitions_to[self.state_first_transition[top] + i]\n",
        "          if not visited[adjacent]:\n",
        "            stack.append(adjacent)\n",
        "\n",
        "      return False\n",
        "\n",
        "    coreachable_states = set()\n",
        "    for state in range(self.number_of_states):\n",
        "      if is_coreachable(state):\n",
        "        coreachable_states.add(state)\n",
        "\n",
        "    return coreachable_states\n",
        "\n",
        "  def trim(self):\n",
        "\n",
        "    def delete_state(state):\n",
        "\n",
        "      first_transition = self.state_first_transition[state]\n",
        "      number_of_transitions = self.state_number_of_transitions[state]\n",
        "      last_transition = first_transition + number_of_transitions\n",
        "\n",
        "      # delete out-going transitions\n",
        "      self.transitions_from[first_transition:last_transition] = [-1] * number_of_transitions\n",
        "      self.transitions_to[first_transition:last_transition] = [-1] * number_of_transitions\n",
        "      self.transitions_label[first_transition:last_transition] = [-1] * number_of_transitions\n",
        "\n",
        "      # delete in-comming transitions\n",
        "      for transitions_index in range(self.number_of_transitions):\n",
        "        if self.transitions_to[transitions_index] == state:\n",
        "          transition_from = self.transitions_from[transitions_index]\n",
        "          self.state_number_of_transitions[transition_from] -= 1\n",
        "          self.transitions_label[transitions_index] = -1\n",
        "          self.transitions_from[transitions_index] = -1\n",
        "          self.transitions_to[transitions_index] = -1\n",
        "\n",
        "      self.is_final[state] = -1\n",
        "      self.state_first_transition[state] = -1\n",
        "      self.state_number_of_transitions[state] = -1\n",
        "\n",
        "    states_to_delete = set()\n",
        "    reachable_and_coreachable_states = self.reachable_states() & self.coreachable_states()\n",
        "    for state in range(self.number_of_states):\n",
        "      if not (state in reachable_and_coreachable_states):\n",
        "        states_to_delete.add(state)\n",
        "\n",
        "    for state in states_to_delete:\n",
        "      delete_state(state)\n",
        "\n",
        "    self.number_of_states = len([state for state in self.is_final if state != -1])\n",
        "    self.number_of_transitions = len([tr for tr in self.transitions_from if tr != -1])\n",
        "\n",
        "    state_index = 0\n",
        "    new_old_state, old_new_state = dict(), dict()\n",
        "    for state in range(len(self.is_final)):\n",
        "      if self.is_final[state] != -1:\n",
        "        new_old_state[state_index] = state\n",
        "        old_new_state[state] = state_index\n",
        "        state_index += 1\n",
        "\n",
        "    is_final = [-1] * self.number_of_states\n",
        "    for state in new_old_state.keys():\n",
        "      is_final[state] = self.is_final[new_old_state[state]]\n",
        "    self.is_final = is_final\n",
        "\n",
        "    transition_index = 0\n",
        "    new_old_transition, old_new_transition = dict(), dict()\n",
        "    for transition in range(len(self.transitions_from)):\n",
        "      if self.transitions_from[transition] != -1:\n",
        "        new_old_transition[transition_index] = transition\n",
        "        old_new_transition[transition] = transition_index\n",
        "        transition_index += 1\n",
        "\n",
        "    state_number_of_transitions = [-1] * self.number_of_states\n",
        "    for state in new_old_state.keys():\n",
        "      state_number_of_transitions[state] = self.state_number_of_transitions[new_old_state[state]]\n",
        "    self.state_number_of_transitions = state_number_of_transitions\n",
        "\n",
        "    state_first_transition = [-1] * self.number_of_states\n",
        "    for state in new_old_state.keys():\n",
        "      if self.state_number_of_transitions[state] > 0:\n",
        "        transition_index = self.state_first_transition[new_old_state[state]]\n",
        "        while self.transitions_from[transition_index] == -1:\n",
        "          transition_index += 1\n",
        "        state_first_transition[state] = old_new_transition[transition_index]\n",
        "    self.state_first_transition = state_first_transition\n",
        "\n",
        "    transitions_from = [-1] * self.number_of_transitions\n",
        "    transitions_to = [-1] * self.number_of_transitions\n",
        "    transitions_label = [-1] * self.number_of_transitions\n",
        "    for transition in new_old_transition.keys():\n",
        "      transitions_from[transition] = old_new_state[self.transitions_from[new_old_transition[transition]]]\n",
        "      transitions_to[transition] = old_new_state[self.transitions_to[new_old_transition[transition]]]\n",
        "      transitions_label[transition] = self.transitions_label[new_old_transition[transition]]\n",
        "    self.transitions_from = transitions_from\n",
        "    self.transitions_to = transitions_to\n",
        "    self.transitions_label = transitions_label\n",
        "\n",
        "    self.initial_state = old_new_state[self.initial_state]\n",
        "\n",
        "    return \n",
        "\n",
        "  def accepts(self, word):\n",
        "    state = self.initial_state\n",
        "\n",
        "    for character in word:\n",
        "      state = self.delta(state, character)\n",
        "\n",
        "      if state == -1:\n",
        "        return False\n",
        "\n",
        "    return (self.is_final[state] == 1)\n",
        "\n",
        "  def delta(self, state, character):\n",
        "\n",
        "    def binary_search(array, element, left, right):\n",
        "\n",
        "      import bisect\n",
        "\n",
        "      index = bisect.bisect_left(array, element, left, right)\n",
        "      if index != len(array) and array[index] == element: \n",
        "          return index\n",
        "      return -1\n",
        "\n",
        "\n",
        "    if self.state_first_transition[state] == -1:\n",
        "      return -1\n",
        "\n",
        "    index = binary_search(\n",
        "      self.transitions_label, character,\n",
        "      self.state_first_transition[state],\n",
        "      self.state_first_transition[state] + self.state_number_of_transitions[state]\n",
        "    )\n",
        "    if index == -1:\n",
        "      return -1\n",
        "    else:\n",
        "      return self.transitions_to[index]\n",
        "\n",
        "  def minimize(self):\n",
        "\n",
        "    def fnv_1a(state, state_class):\n",
        "      OFFSET_BASIS = 0x811C9DC5\n",
        "      PRIME = 0x1000193\n",
        "      MAX = 2 ** 32\n",
        "\n",
        "      hash_code = OFFSET_BASIS\n",
        "      hash_code ^= self.is_final[state]\n",
        "      hash_code = (hash_code * PRIME) % MAX\n",
        "\n",
        "      first_transition = self.state_first_transition[state]\n",
        "\n",
        "      if first_transition != -1:\n",
        "\n",
        "        number_of_transitions = self.state_number_of_transitions[state]\n",
        "        last_transition = first_transition + number_of_transitions\n",
        "\n",
        "        for transition_index in range(first_transition, last_transition):\n",
        "          hash_code ^= ord(self.transitions_label[transition_index])\n",
        "          hash_code = (hash_code * PRIME) % MAX\n",
        "          hash_code ^= state_class[self.transitions_to[transition_index]]\n",
        "          hash_code = (hash_code * PRIME) % MAX\n",
        "\n",
        "      return hash_code\n",
        "\n",
        "    def split_classes(states, state_class):\n",
        "      split = False\n",
        "      new_states = []\n",
        "      new_state_class = dict()\n",
        "      number_of_classes = 0\n",
        "\n",
        "      for state in states:\n",
        "\n",
        "        if len(state) == 1:\n",
        "          new_states.append(state)\n",
        "          (st, ) = state\n",
        "          new_state_class[st] = number_of_classes\n",
        "          number_of_classes += 1\n",
        "\n",
        "        else:\n",
        "          split_state = []\n",
        "\n",
        "          hash_code_states = dict()\n",
        "          for st in state:\n",
        "            hash_code_ = fnv_1a(st, state_class)\n",
        "            hash_code_states[hash_code_] = hash_code_states.get(hash_code_, set())\n",
        "            hash_code_states[hash_code_].add(st)\n",
        "\n",
        "          for hash_code, state in hash_code_states.items():\n",
        "            # todo check for colisions\n",
        "            split_state.append(state)\n",
        "            for st in state:\n",
        "              new_state_class[st] = number_of_classes\n",
        "            number_of_classes += 1\n",
        "\n",
        "          new_states.extend(split_state)\n",
        "\n",
        "          if len(split_state) > 1:\n",
        "              split = True\n",
        "\n",
        "      return split, new_states, new_state_class\n",
        "\n",
        "    state_class = dict()\n",
        "    final, not_final = set(), set()\n",
        "    for state in range(len(self.is_final)):\n",
        "      if self.is_final[state] == 1:\n",
        "        final.add(state)\n",
        "        state_class[state] = 1\n",
        "      else:\n",
        "        not_final.add(state)\n",
        "        state_class[state] = 0\n",
        "\n",
        "    split, states, state_class = split_classes([final, not_final], state_class)\n",
        "\n",
        "    while split:\n",
        "      split, states, state_class = split_classes(states, state_class)\n",
        "\n",
        "    #print (\"at end \", states, len(states))\n",
        "\n",
        "    number_of_states = len(states)\n",
        "    initial_state = state_class[self.initial_state]\n",
        "    is_final = [-1] * number_of_states\n",
        "    for state in states:\n",
        "      st = next(iter(state))\n",
        "      is_final[state_class[st]] = self.is_final[st]\n",
        "\n",
        "    transitions = set()\n",
        "    for state in states:\n",
        "      st = next(iter(state))\n",
        "\n",
        "      first_transition = self.state_first_transition[st]\n",
        "\n",
        "      if first_transition != -1:\n",
        "\n",
        "        number_of_transitions = self.state_number_of_transitions[st]\n",
        "        last_transition = first_transition + number_of_transitions\n",
        "\n",
        "        for transition_index in range(first_transition, last_transition):\n",
        "          transitions.add((\n",
        "            state_class[st],\n",
        "            self.transitions_label[transition_index],\n",
        "            state_class[self.transitions_to[transition_index]]\n",
        "          ))\n",
        "\n",
        "    # print (number_of_states)\n",
        "    # print (len(transitions))\n",
        "    return Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  def draw(self, file):\n",
        "\n",
        "    from graphviz import Digraph\n",
        "\n",
        "    nodes = set()\n",
        "    for q_1 in self.transitions_from:\n",
        "      nodes.add(q_1)\n",
        "    for q_2 in self.transitions_to:\n",
        "      nodes.add(q_2)\n",
        "\n",
        "    filename = DATA_DIR + \"/\" + file\n",
        "    fileformat = 'pdf'\n",
        "    g = Digraph('G', filename=filename, format=fileformat)\n",
        "    g.attr(rankdir='LR', size='8,5')\n",
        "\n",
        "    for node in nodes:\n",
        "\n",
        "      if self.is_final[node] == 1:\n",
        "        if node == self.initial_state:\n",
        "          g.attr('node', shape='doublecircle', style='filled', color='grey80')\n",
        "        else:\n",
        "          g.attr('node', shape='doublecircle', style='filled', color='grey80')\n",
        "        g.node(str(node))\n",
        "      else:\n",
        "        if node == self.initial_state:\n",
        "          g.attr('node', shape='doubleoctagon', style='filled', color='grey80')\n",
        "        else:\n",
        "          g.attr('node', shape='circle', style='filled', color='grey80')\n",
        "        g.node(str(node))\n",
        "\n",
        "      g.attr('node', shape='point', color=\"black\")\n",
        "\n",
        "    g.edge('', str(self.initial_state), label='', color=\"black\")\n",
        "\n",
        "    for i in range(self.number_of_transitions):\n",
        "      q_1 = self.transitions_from[i]\n",
        "      a = self.transitions_label[i]\n",
        "      q_2 = self.transitions_to[i]\n",
        "\n",
        "      g.edge(str(q_1), str(q_2), label=str(a))\n",
        "\n",
        "    g.render() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sApV6MtPybMP"
      },
      "source": [
        "def test_trim():\n",
        "\n",
        "  initial_state = 2\n",
        "  transitions = set()\n",
        "  transitions.add((2, 'x', 1))\n",
        "  transitions.add((0, 'x', 1))\n",
        "  is_final = [0, 1, 0]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 2\n",
        "  assert reachable_states == {1, 2}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 3\n",
        "  assert coreachable_states == {0, 1, 2}\n",
        "  assert automaton.accepts('x')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 1\n",
        "  assert automaton.number_of_states == 2\n",
        "  assert len(automaton.reachable_states()) == 2\n",
        "  assert len(automaton.coreachable_states()) == 2\n",
        "  assert automaton.accepts('x')\n",
        "\n",
        "\n",
        "  initial_state = 2\n",
        "  transitions = set()\n",
        "  transitions.add((2, 'x', 1))\n",
        "  transitions.add((1, 'b', 3))\n",
        "  transitions.add((1, 'c', 5))\n",
        "  transitions.add((0, 'x', 1))\n",
        "  transitions.add((0, 'x', 4))\n",
        "  transitions.add((2, 'a', 4))\n",
        "  is_final = [0, 1, 0, 0, 0, 1]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 6\n",
        "  assert automaton.number_of_states == 6\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 5\n",
        "  assert reachable_states == {1, 2, 3, 4, 5}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 4\n",
        "  assert coreachable_states == {0, 1, 2, 5}\n",
        "  assert automaton.accepts('x')\n",
        "  assert automaton.accepts('xc')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  assert len(automaton.reachable_states()) == 3\n",
        "  assert len(automaton.coreachable_states()) == 3\n",
        "  assert automaton.accepts('x')\n",
        "  assert automaton.accepts('xc')\n",
        "\n",
        "\n",
        "  initial_state = 2\n",
        "  transitions = set()\n",
        "  transitions.add((2, 'a', 1))\n",
        "  transitions.add((1, 'b', 0))\n",
        "  is_final = [1, 1, 0]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 3\n",
        "  assert reachable_states == {0, 1, 2}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 3\n",
        "  assert coreachable_states == {0, 1, 2}\n",
        "  assert automaton.accepts('ab')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  assert len(automaton.reachable_states()) == 3\n",
        "  assert len(automaton.coreachable_states()) == 3\n",
        "  assert automaton.accepts('ab')\n",
        "\n",
        "\n",
        "  initial_state = 3\n",
        "  transitions = set()\n",
        "  transitions.add((0, 'a', 4))\n",
        "  transitions.add((3, 'a', 4))\n",
        "  transitions.add((3, 'b', 2))\n",
        "  transitions.add((2, 'c', 1))\n",
        "  is_final = [0, 1, 1, 0, 0]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 4\n",
        "  assert automaton.number_of_states == 5\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 4\n",
        "  assert reachable_states == {1, 2, 3, 4}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 3\n",
        "  assert coreachable_states == {1, 2, 3}\n",
        "  assert automaton.accepts('b')\n",
        "  assert automaton.accepts('bc')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  assert len(automaton.reachable_states()) == 3\n",
        "  assert len(automaton.coreachable_states()) == 3\n",
        "  assert automaton.accepts('b')\n",
        "  assert automaton.accepts('bc')\n",
        "\n",
        "\n",
        "  initial_state = 2\n",
        "  transitions = set()\n",
        "  transitions.add((2, 'a', 1))\n",
        "  transitions.add((1, 'b', 0))\n",
        "  is_final = [0, 1, 0]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 2\n",
        "  assert automaton.number_of_states == 3\n",
        "  reachable_states = automaton.reachable_states()\n",
        "  assert len(reachable_states) == 3\n",
        "  assert reachable_states == {0, 1, 2}\n",
        "  coreachable_states = automaton.coreachable_states()\n",
        "  assert len(coreachable_states) == 2\n",
        "  assert coreachable_states == {1, 2}\n",
        "  assert automaton.accepts('a')\n",
        "  automaton.trim()\n",
        "  assert automaton.contains_cycle() == False\n",
        "  assert automaton.number_of_transitions == 1\n",
        "  assert automaton.number_of_states == 2\n",
        "  assert len(automaton.reachable_states()) == 2\n",
        "  assert len(automaton.coreachable_states()) == 2\n",
        "  assert automaton.accepts('a')\n",
        "\n",
        "\n",
        "  for iterations in range(1024):\n",
        "\n",
        "    labels = random.sample(string.ascii_lowercase, 6)\n",
        "    initial_state = 0\n",
        "    transitions = set()\n",
        "    transitions.add((0, labels[0], 1))\n",
        "    transitions.add((0, labels[1], 2))\n",
        "    transitions.add((0, labels[2], 3))\n",
        "    transitions.add((0, labels[3], 4))\n",
        "    transitions.add((0, labels[4], 5))\n",
        "    transitions.add((0, labels[5], 6))\n",
        "    is_final = [0, 1, 1, 1, 0, 0, 1]\n",
        "    automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "    assert automaton.contains_cycle() == False\n",
        "    assert automaton.number_of_transitions == 6\n",
        "    assert automaton.number_of_states == 7\n",
        "    reachable_states = automaton.reachable_states()\n",
        "    assert len(reachable_states) == 7\n",
        "    assert reachable_states == {0, 1, 2, 3, 4, 5, 6}\n",
        "    coreachable_states = automaton.coreachable_states()\n",
        "    assert len(coreachable_states) == 5\n",
        "    assert coreachable_states == {0, 1, 2, 3, 6}\n",
        "    assert automaton.accepts(labels[0])\n",
        "    assert automaton.accepts(labels[1])\n",
        "    assert automaton.accepts(labels[2])\n",
        "    assert automaton.accepts(labels[5])\n",
        "\n",
        "    automaton.trim()\n",
        "\n",
        "    assert automaton.contains_cycle() == False\n",
        "    assert automaton.number_of_transitions == 4\n",
        "    assert automaton.number_of_states == 5\n",
        "    assert len(automaton.reachable_states()) == 5\n",
        "    assert len(automaton.coreachable_states()) == 5\n",
        "    assert automaton.accepts(labels[0])\n",
        "    assert automaton.accepts(labels[1])\n",
        "    assert automaton.accepts(labels[2])\n",
        "    assert automaton.accepts(labels[5])\n",
        "\n",
        "test_trim()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME8j0HZ8yIH9"
      },
      "source": [
        "def test_minimize1():\n",
        "\n",
        "  initial_state = 0\n",
        "  transitions = set()\n",
        "  transitions.add((0, 'a', 1))\n",
        "  transitions.add((0, 'd', 3))\n",
        "  transitions.add((1, 'b', 2))\n",
        "  transitions.add((3, 'b', 2))\n",
        "  is_final = [0, 1, 1, 1]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  automaton.minimize()\n",
        "  print (automaton.number_of_states)\n",
        "\n",
        "def test_minimize2():\n",
        "\n",
        "  initial_state = 0\n",
        "  transitions = set()\n",
        "  transitions.add((0, 'a', 1))\n",
        "  transitions.add((0, 'd', 4))\n",
        "  transitions.add((1, 'b', 2))\n",
        "  transitions.add((2, 'c', 3))\n",
        "  transitions.add((4, 'b', 5))\n",
        "  transitions.add((5, 'c', 6))\n",
        "  is_final = [0, 1, 0, 1, 1, 0, 1]\n",
        "  automaton = Automaton(initial_state, transitions, is_final)\n",
        "\n",
        "  automaton = automaton.minimize()\n",
        "  print (automaton.number_of_states)\n",
        "  automaton.draw(\"min\")\n",
        "\n",
        "test_minimize2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96NiN7fMAnAA",
        "outputId": "1da025c8-270d-4155-952a-ab2bb748e4f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "def evaluate_model__(dataset, model, set_name):\n",
        "  data_loader = utils.data.DataLoader(\n",
        "    NLDataset(dataset), batch_size=1, shuffle = False\n",
        "  )\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "\n",
        "  for index, data in enumerate(data_loader):\n",
        "    label = data['y'][0].item()\n",
        "    result = model(data['x'], data['length'])\n",
        "    prediction = result.argmax(dim=1).cpu().numpy()[0]\n",
        "\n",
        "    x, length = data['x'], data['length']\n",
        "    x = x[0].tolist()\n",
        "    length = length.item()\n",
        "    x = x[:length]\n",
        "    x = [inv_alphabet[c] for c in x]\n",
        "\n",
        "    if label == 1:\n",
        "      if prediction == 1:\n",
        "        tp += 1\n",
        "        print (\"M tp \", \"\".join(x), index)\n",
        "      else:\n",
        "        fn += 1\n",
        "        print (\"M fn \", \"\".join(x), index)\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        fp += 1\n",
        "        print (\"M fp \", \"\".join(x), index)\n",
        "      else:\n",
        "        tn += 1\n",
        "        print (\"M tn \", \"\".join(x), index)\n",
        "\n",
        "  if tp == 0:\n",
        "    if fn == 0 and fp == 0:\n",
        "      pr, r, f1 = 1, 1, 1\n",
        "    else:\n",
        "      pr, r, f1 = 0, 0, 0\n",
        "  else:\n",
        "    pr = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    f1 = 2 * ((pr * r) / (pr + r))\n",
        "\n",
        "  accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "  print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "    set_name, tp, tn, fp, fn, pr, r, f1, accuracy\n",
        "  ))\n",
        "\n",
        "alphabet = {}\n",
        "with open(DATA_DIR + \"/words/alphabet.dict\", \"rb\") as f:\n",
        "  alphabet = pickle.load(f)\n",
        "\n",
        "inv_alphabet = { v : k for k, v in alphabet.items() }\n",
        "\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/words/train.data.npy\",\n",
        "  DATA_DIR + \"/words/train.length.npy\",\n",
        "  DATA_DIR + \"/words/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/words/dev.data.npy\",\n",
        "  DATA_DIR + \"/words/dev.length.npy\",\n",
        "  DATA_DIR + \"/words/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/words/test.data.npy\",\n",
        "  DATA_DIR + \"/words/test.length.npy\",\n",
        "  DATA_DIR + \"/words/test.labels.npy\"\n",
        ")\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path=\"/words/rnn.model.params\",\n",
        "    model_path=\"/words/rnn.model.pt\"\n",
        ")\n",
        "print (params)\n",
        "model_summary(model)\n",
        "\n",
        "train_loader = utils.data.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_loader = utils.data.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_loader = utils.data.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "\n",
        "evaluate_model(train_loader, model, \"train\")\n",
        "evaluate_model(dev_loader, model, \"dev\")\n",
        "evaluate_model(test_loader, model, \"test\")\n",
        "\n",
        "# initial_state = 0\n",
        "# transitions = []\n",
        "\n",
        "# train_loader = utils.data.DataLoader(\n",
        "#   NLDataset(train_data), batch_size=1\n",
        "# )\n",
        "\n",
        "# for index, data in enumerate(train_loader):\n",
        "#     x, length = data['x'], data['length']\n",
        "#     softmax_output, rnn_output, transition_probabilities = model(x, length, return_probabilities=True, return_rnn_output=True)\n",
        "#     prediction = softmax_output.argmax(dim=1).cpu().numpy()[0]\n",
        "#     if prediction == 1:\n",
        "#       transition_probabilities, _ = nn.utils.rnn.pad_packed_sequence(transition_probabilities, batch_first=True)\n",
        "#       rnn_output, _ = nn.utils.rnn.pad_packed_sequence(rnn_output, batch_first=True)\n",
        "#       first = x[0]\n",
        "#       prev_state = initial_state\n",
        "#       for t in range(length[0]):\n",
        "#         l = inv_alphabet[first[t].item()]\n",
        "#         next_state = torch.argmax(transition_probabilities[0, t, ]).item() + 1\n",
        "#         transitions.append((prev_state, l, next_state))\n",
        "#         prev_state = next_state\n",
        "\n",
        "# start_state = torch.zeros((1, params['hidden_size']), device=device)\n",
        "# #print (model.softmax(model.linear(torch.cat((start_state, model.rnn.states)))))\n",
        "# is_final = model.softmax(model.linear(torch.cat((start_state, model.rnn.states)))).argmax(dim=1).cpu().tolist()\n",
        "\n",
        "# state_symbol_to_states = defaultdict(list)\n",
        "\n",
        "# for transition in transitions:\n",
        "#   q1, a, q2 = transition\n",
        "#   state_symbol_to_states[(q1, a)].append(q2)\n",
        "\n",
        "# count_non_deterministic, count_arbitrarily = 0, 0\n",
        "\n",
        "# transitions = set()\n",
        "# for state_symbol in state_symbol_to_states:\n",
        "#   states = state_symbol_to_states[state_symbol]\n",
        "#   q1, a = state_symbol\n",
        "#   states_set = set(states)\n",
        "\n",
        "#   if len(states_set) == 1:\n",
        "#     transitions.add((q1, a, states[0]))\n",
        "#   else:\n",
        "#     count_non_deterministic += 1\n",
        "#     counts = Counter(states)\n",
        "#     print (\"WARNING: Non deterministic transitions \", q1, a, counts)\n",
        "#     first_two_most_common = counts.most_common(2)\n",
        "\n",
        "#     if len(first_two_most_common) > 1:\n",
        "#       first_most_common, second_most_common = first_two_most_common\n",
        "#       first_most_common_state, first_most_common_count = first_most_common\n",
        "#       second_most_common_state, second_most_common_count = second_most_common\n",
        "#       if first_most_common_count == second_most_common_count:\n",
        "#         # should we determinize the automaton?\n",
        "#         count_arbitrarily += 1\n",
        "#         print (\"WARNING: Selected transition is arbitrarily\", q1, a, first_most_common_state)\n",
        "#       transitions.add((q1, a, first_most_common_state))\n",
        "\n",
        "#     else:\n",
        "#       first_most_common = first_two_most_common[0]\n",
        "#       first_most_common_state, first_most_common_count = first_most_common\n",
        "#       transitions.add((q1, a, first_most_common_state))\n",
        "\n",
        "# print (\"Number of not deterministic transitions\", count_non_deterministic)\n",
        "# print (\"Number of arbitrarily selected transitions\", count_arbitrarily)\n",
        "\n",
        "# automaton = Automaton(initial_state, transitions, is_final)\n",
        "# print (\"Number of states\", automaton.number_of_states)\n",
        "# print (\"Number of transitions\", automaton.number_of_transitions)\n",
        "# print (\"Reachable states \", len(automaton.reachable_states()))\n",
        "# print (\"Co-Reachable states \", len(automaton.coreachable_states()))\n",
        "# #print (\"DFA? \", automaton.is_deterministic())\n",
        "# automaton.trim()\n",
        "# print (\"Number of states\", automaton.number_of_states)\n",
        "# print (\"Number of transitions\", automaton.number_of_transitions)\n",
        "\n",
        "# automaton = automaton.minimize()\n",
        "# print (\"Number of states\", automaton.number_of_states)\n",
        "# print (\"Number of transitions\", automaton.number_of_transitions)\n",
        "# print (\"Has cycle?\", automaton.contains_cycle())\n",
        "# #automaton.draw(\"numeral-50.2.aut.min\")\n",
        "\n",
        "# def __evaluate_automaton(automaton, dataset):\n",
        "#   data_loader = utils.data.DataLoader(\n",
        "#     NLDataset(dataset), batch_size=1, shuffle = False\n",
        "#   )\n",
        "#   tp, tn, fp, fn = 0, 0, 0, 0\n",
        "\n",
        "#   for data in data_loader:\n",
        "#     label = data['y'][0].item()\n",
        "\n",
        "#     x, length = data['x'], data['length']\n",
        "#     x = x[0].tolist()\n",
        "#     length = length.item()\n",
        "#     x = x[:length]\n",
        "#     x = [inv_alphabet[c] for c in x]\n",
        "\n",
        "#     if automaton.accepts(x):\n",
        "#       prediction = 1\n",
        "#     else:\n",
        "#      prediction = 0\n",
        "\n",
        "#     if label == 1:\n",
        "#       if prediction == 1:\n",
        "#         tp += 1\n",
        "#         print (\"A tp \", \"\".join(x))\n",
        "#       else:\n",
        "#         fn += 1\n",
        "#         print (\"A fn \", \"\".join(x))\n",
        "#     else:\n",
        "#       if prediction == 1:\n",
        "#         fp += 1\n",
        "#         print (\"A fp \", \"\".join(x))\n",
        "#       else:\n",
        "#         tn += 1\n",
        "#         print (\"A tn \", \"\".join(x))\n",
        "\n",
        "#   if tp == 0:\n",
        "#     if fn == 0 and fp == 0:\n",
        "#       pr, r, f1 = 1, 1, 1\n",
        "#     else:\n",
        "#       pr, r, f1 = 0, 0, 0\n",
        "#   else:\n",
        "#     pr = tp / (tp + fp)\n",
        "#     r = tp / (tp + fn)\n",
        "#     f1 = 2 * ((pr * r) / (pr + r))\n",
        "\n",
        "#   accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "#   return tp, tn, fp, fn, pr, r, f1, accuracy\n",
        "\n",
        "# def _evaluate_automaton(automaton, dataset):\n",
        "#   data_loader = utils.data.DataLoader(\n",
        "#     NLDataset(dataset), batch_size=1, shuffle = False\n",
        "#   )\n",
        "\n",
        "#   labels, predictions = [], []\n",
        "\n",
        "#   for data in data_loader:\n",
        "#     labels.append(data['y'][0].item())\n",
        "\n",
        "#     x, length = data['x'], data['length']\n",
        "#     x = x[0].tolist()\n",
        "#     length = length.item()\n",
        "#     x = x[:length]\n",
        "#     x = [inv_alphabet[c] for c in x]\n",
        "\n",
        "#     if automaton.accepts(x):\n",
        "#       predictions.append(1)\n",
        "#     else:\n",
        "#       predictions.append(0)\n",
        "\n",
        "\n",
        "#   return __evaluate(predictions, labels)\n",
        "\n",
        "# def evaluate_automaton(automaton, dataset, set_name):\n",
        "#   tp, tn, fp, fn, pr, r, f1, acc = _evaluate_automaton(automaton, dataset)\n",
        "#   print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "#     set_name, tp, tn, fp, fn, pr, r, f1, acc\n",
        "#   ))\n",
        "\n",
        "# evaluate_automaton(automaton, train_data, \"train\")\n",
        "# evaluate_automaton(automaton, dev_data, \"dev\")\n",
        "# evaluate_automaton(automaton, test_data, \"test\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 200, 'num_epochs': 40, 'num_embeddings': 30, 'mode': 'rnn', 'nonlinearity': 'relu', 'hidden_size': 100, 'bias': False}\n",
            "NLNN(\n",
            "  (embeddings): Embedding(31, 31, padding_idx=0)\n",
            "  (rnn): SRRNN(\n",
            "    (rnn_cell): RNNCell(31, 100, bias=False, nonlinearity=relu)\n",
            "  )\n",
            "  (linear): Linear(in_features=100, out_features=2, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "\n",
            "Trainable parameters:\n",
            "  rnn.rnn_cell.weight_ih\n",
            "  rnn.rnn_cell.weight_hh\n",
            "  linear.weight\n",
            "  linear.bias\n",
            "\n",
            "Number of trainable parameters 13,302\n",
            "\n",
            "train : TP : 548500 TN : 514724 FP : 62619 FN : 29074 Pr : 0.8975338681991559 R : 0.9496618615103866 F1: 0.9228623370374015 ACC : 0.9206064158723094 \n",
            "dev : TP : 68218 TN : 64073 FP : 8231 FN : 3842 Pr : 0.8923334510588758 R : 0.9466833194560089 F1: 0.9187052636540546 ACC : 0.9163711174531046 \n",
            "test : TP : 68296 TN : 63951 FP : 8225 FN : 3893 Pr : 0.8925131663203565 R : 0.9460721162503982 F1: 0.9185125411875462 ACC : 0.9160599868389152 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}