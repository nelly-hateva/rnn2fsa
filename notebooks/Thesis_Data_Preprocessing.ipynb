{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis - Data Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9uJ2J17ja1x",
        "colab_type": "text"
      },
      "source": [
        "Prerequisities: Access data in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGwWWTicjL8T",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "ec53e630-30cf-4aaf-ae3d-f95525efe27d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "MOUNT_POINT = \"/content/drive/\"\n",
        "drive.mount(MOUNT_POINT)\n",
        "DATA_DIR = MOUNT_POINT + \"My Drive/Colab Notebooks/Thesis-Data/bg\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsoIv2Dujh0X",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbdRm8T8jjmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import os\n",
        "import numpy\n",
        "import pickle\n",
        "from collections import defaultdict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpQZJbAajn3-",
        "colab_type": "text"
      },
      "source": [
        "Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U26LFrnMjprP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed(seed=666):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  numpy.random.seed(seed)\n",
        "\n",
        "seed()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hepLIdPjuoi",
        "colab_type": "text"
      },
      "source": [
        "Read positive examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLw-CiVvjxGT",
        "colab_type": "code",
        "outputId": "f24d86b9-be9d-41de-a079-9044b71de372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "def read_data(file):\n",
        "  with open(file) as f:\n",
        "    return { w.strip().lower() for w in f.readlines() }\n",
        "\n",
        "positive_examples = read_data(DATA_DIR + \"/words.dat\")\n",
        "\n",
        "print(\"Positive examples size {0:,}\".format(len(positive_examples)))\n",
        "\n",
        "MAX_WORD_LENGTH = max([len(w) for w in positive_examples])\n",
        "print (f'Maximum word length {MAX_WORD_LENGTH}')\n",
        "\n",
        "alphabet = { c for w in positive_examples for c in w }\n",
        "alphabet = list(alphabet)\n",
        "alphabet.sort()\n",
        "\n",
        "print (f'Alphabet size {len(alphabet)}')\n",
        "\n",
        "alphabet = { c : i + 1 for (i, c) in enumerate(alphabet) }\n",
        "\n",
        "with open(DATA_DIR + \"/model/large/alphabet.dict\", 'wb') as f:\n",
        "  pickle.dump(alphabet, f)\n",
        "\n",
        "with open(DATA_DIR + '/model/large/alphabet.tsv', \"w\") as f:\n",
        "  for c, i in alphabet.items():\n",
        "    f.write(c)\n",
        "    f.write(\"\\t\")\n",
        "    f.write(str(i))\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive examples size 721,823\n",
            "Maximum word length 25\n",
            "Alphabet size 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "997MfFF02D2X",
        "colab_type": "text"
      },
      "source": [
        "Generate negative words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syqMtdomgd5q",
        "colab_type": "code",
        "outputId": "d868ac8b-7d34-4730-ad15-c74138a6e1c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def delete_random_char(s):\n",
        "  if len(s) >= 2:\n",
        "    pos = random.randint(0, len(s) - 1)\n",
        "    return s[:pos] + s[pos + 1:]\n",
        "  return s\n",
        "\n",
        "def insert_random_char(s, alphabet):\n",
        "  pos = random.randint(0, len(s))\n",
        "  return s[:pos] + random.choice(alphabet) + s[pos:]\n",
        "\n",
        "def replace_random_char(s, alphabet):\n",
        "  if len(s) >= 1:\n",
        "    pos = random.randint(0, len(s) - 1)\n",
        "    choices = alphabet.copy()\n",
        "    choices = choices.remove(s[pos])\n",
        "    return s[:pos] + random.choice(alphabet) + s[pos + 1:]\n",
        "  return s\n",
        "\n",
        "def random_transposition(s):\n",
        "  if len(s) >= 2:\n",
        "    pos = random.randint(0, len(s) - 2)\n",
        "    return s[:pos] + s[pos + 1] + s[pos] + s[pos + 2:]\n",
        "  return s\n",
        "\n",
        "def generate_negative_examples(positive_examples, alphabet):\n",
        "  negative_examples = set()\n",
        "\n",
        "  while len(negative_examples) < len(positive_examples):\n",
        "\n",
        "    for w in positive_examples:\n",
        "\n",
        "      if len(negative_examples) == len(positive_examples):\n",
        "        break\n",
        "\n",
        "      if len(w) == 1: \n",
        "        if bool(random.getrandbits(1)):\n",
        "          w1 = replace_random_char(w, alphabet)\n",
        "        else:\n",
        "          w1 = insert_random_char(w, alphabet)\n",
        "\n",
        "      if len(w) >= 2:\n",
        "        op = random.randint(1, 4)\n",
        "        if op == 1:\n",
        "          w1 = replace_random_char(w, alphabet)\n",
        "        elif op == 2:\n",
        "          w1 = insert_random_char(w, alphabet)\n",
        "        elif op == 3:\n",
        "          w1 = delete_random_char(w)\n",
        "        else:\n",
        "          w1 = random_transposition(w)\n",
        "\n",
        "      if w1 not in positive_examples and w1 not in negative_examples:\n",
        "        negative_examples.add(w1)\n",
        "\n",
        "  return negative_examples\n",
        "\n",
        "# negative_examples = generate_negative_examples(positive_examples, list(alphabet.keys()))\n",
        "# print (f'Generated negative examples {len(negative_examples)}')\n",
        "\n",
        "# with open(DATA_DIR + \"/numeral-50-NEG.dat\", \"w\") as f:\n",
        "#   for w in negative_examples:\n",
        "#     f.write(w)\n",
        "#     f.write(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated negative examples 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugXmLoIdr0FK",
        "colab_type": "text"
      },
      "source": [
        "Read negative examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-PL9RexT6jn",
        "colab_type": "code",
        "outputId": "8215a5ab-a114-4086-fb73-814727b0697d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "negative_examples = read_data(DATA_DIR + \"/negative-2.0.dat\")\n",
        "print(\"Negative examples size {0:,}\".format(len(negative_examples)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Negative examples size 721,823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypqlr6vpm8nX",
        "colab_type": "code",
        "outputId": "d042b955-6893-427a-b97a-18ce677bae9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def shuffle_and_split_data(data, train_size=0.8, dev_size=0.1, test_size=0.1):\n",
        "\n",
        "  random.shuffle(data)\n",
        "\n",
        "  train, dev, test = data[:round(len(data) * train_size)], \\\n",
        "    data[round(len(data) * train_size):round(len(data) * (train_size + dev_size))], \\\n",
        "    data[round(len(data) * (train_size + dev_size)):]\n",
        "  \n",
        "  return train, dev, test\n",
        "\n",
        "all_data = list(positive_examples) + list(negative_examples)\n",
        "train, dev, test = shuffle_and_split_data(all_data)\n",
        "\n",
        "print(\"Train size {0:,}\".format(len(train)))\n",
        "print(\"Dev size {0:,}\".format(len(dev)))\n",
        "print(\"Test size {0:,}\".format(len(test)))\n",
        "\n",
        "MAX_WORD_LENGTH = max([len(w) for w in train] + [len(w) for w in dev])\n",
        "print (f'Maximum word length {MAX_WORD_LENGTH}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size 80\n",
            "Dev size 10\n",
            "Test size 10\n",
            "Maximum word length 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHDga9a0moeO",
        "colab_type": "text"
      },
      "source": [
        "Serialize data as numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5CPTd8Tms_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_data_as_np_arrays(dataset, dataset_name):\n",
        "  x, length, y = [], [], []\n",
        "\n",
        "  for w in dataset:\n",
        "    length.append(len(w))\n",
        "    y.append(1 if w in positive_examples else 0)\n",
        "    w_x = [alphabet[c] for c in w]\n",
        "    if len(w_x) < MAX_WORD_LENGTH:\n",
        "      w_x.extend([0] * (MAX_WORD_LENGTH - len(w_x)))\n",
        "    elif len(w_x) > MAX_WORD_LENGTH:\n",
        "      w_x[:MAX_WORD_LENGTH]\n",
        "    x.append(w_x)\n",
        "\n",
        "  numpy.save(DATA_DIR + \"/model/experiment1/\" + dataset_name + \".data.npy\", numpy.array(x))\n",
        "  numpy.save(DATA_DIR + \"/model/experiment1/\" + dataset_name + \".length.npy\", numpy.array(length))\n",
        "  numpy.save(DATA_DIR + \"/model/experiment1/\" + dataset_name + \".labels.npy\", numpy.array(y))\n",
        "\n",
        "save_data_as_np_arrays(train, \"train\")\n",
        "save_data_as_np_arrays(dev, \"dev\")\n",
        "save_data_as_np_arrays(test, \"test\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6v7Kw6MI3fY",
        "colab_type": "text"
      },
      "source": [
        "Serialize data as text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOTL6MfkI6qr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_data_as_tsv(filename, data):\n",
        "  POS, NEG = str(1), str(0)\n",
        "\n",
        "  with open(DATA_DIR + filename, \"w\") as f:\n",
        "    for w in data:\n",
        "      if w in positive_examples:\n",
        "        f.write(POS)\n",
        "      else:\n",
        "        f.write(NEG)\n",
        "      f.write(\"\\t\")\n",
        "      f.write(w)\n",
        "      f.write(\"\\n\")\n",
        "\n",
        "save_data_as_tsv(\"/model/experiment1/train.tsv\", train)\n",
        "save_data_as_tsv(\"/model/experiment1/dev.tsv\", dev)\n",
        "save_data_as_tsv(\"/model/experiment1/test.tsv\", test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfbObusawAuA",
        "colab_type": "text"
      },
      "source": [
        "Serialize data extension \n",
        "\n",
        "For each word we add all it's prefixes except epsilon and the word itself\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Oae2AzIRz6de",
        "outputId": "41a6d3d2-90aa-4825-b31f-b5b980e2ee17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# TODO remove reading data\n",
        "def extend_dataset(data, positive_examples, alphabet, base_path):\n",
        "  inv_alphabet = { v : k for k, v in alphabet.items() }\n",
        "  x, length, y = data\n",
        "\n",
        "  word_to_count = defaultdict(int)\n",
        "  for i in range(len(x)):\n",
        "    for j in range(1, length[i] + 1):\n",
        "      prefix = ''.join([inv_alphabet[c] for c in x[i][:j]])\n",
        "      word_to_count[prefix] += 1\n",
        "\n",
        "  assert sum(length) == sum([count for w, count in word_to_count.items()])\n",
        "\n",
        "  MAX_WORD_LENGTH = max(length)\n",
        "\n",
        "  x1, length1, y1, weights = [], [], [], []\n",
        "\n",
        "  for w, count in word_to_count.items():\n",
        "    w_x = [alphabet[c] for c in w]\n",
        "    if len(w_x) < MAX_WORD_LENGTH:\n",
        "      w_x.extend([0] * (MAX_WORD_LENGTH - len(w_x)))\n",
        "    elif len(w_x) > MAX_WORD_LENGTH:\n",
        "      w_x[:MAX_WORD_LENGTH]\n",
        "    x1.append(w_x)\n",
        "    length1.append(len(w))\n",
        "    y1.append(1 if w in positive_examples else 0)\n",
        "    weights.append(count)\n",
        "\n",
        "  print(\"Size {0:,}\".format(len(x)))\n",
        "  print(\"Extended size {0:,}\".format(len(x1)))\n",
        "  numpy.save(base_path + \".extended.data.npy\", numpy.array(x1))\n",
        "  numpy.save(base_path + \".extended.length.npy\", numpy.array(length1))\n",
        "  numpy.save(base_path + \".extended.labels.npy\", numpy.array(y1))\n",
        "  numpy.save(base_path + \".extended.weights.npy\", numpy.array(weights))\n",
        "\n",
        "def load_data(filename_data, filename_length, filename_labels):\n",
        "  return numpy.load(filename_data, allow_pickle=True), \\\n",
        "    numpy.load(filename_length, allow_pickle=True), \\\n",
        "    numpy.load(filename_labels, allow_pickle=True)\n",
        "\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/model/large/train.data.npy\",\n",
        "  DATA_DIR + \"/model/large/train.length.npy\",\n",
        "  DATA_DIR + \"/model/large/train.labels.npy\"\n",
        ")\n",
        "\n",
        "extend_dataset(train_data, positive_examples, alphabet, DATA_DIR + \"/model/large/train\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size 1,154,917\n",
            "Extended size 3,301,985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5NpCKj-0Uu3",
        "colab_type": "code",
        "outputId": "012e2a2e-338c-47ab-bb28-617f7e47788e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "x = numpy.array([[0, 2], [1, 1], [2, 0]]).T\n",
        "print (x)\n",
        "print (numpy.cov(x))\n",
        "\n",
        "x = numpy.array([[1692, 68], [1978, 102], [1884, 110], [2151, 112], [2519, 154]]).T\n",
        "print (x)\n",
        "print (numpy.cov(x))\n",
        "# vectors = []\n",
        "# with open(DATA_DIR + \"/kMeans/light/vectors.txt\", \"r\") as f:\n",
        "#   lines = f.readlines()\n",
        "#   for line in lines:\n",
        "#     line = line.strip()\n",
        "#     if line:\n",
        "#       vectors.append([float(p) for p in line.split()])\n",
        "\n",
        "# v = numpy.array(vectors).T\n",
        "# #print (v)\n",
        "# print (numpy.cov(v))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 2]\n",
            " [2 1 0]]\n",
            "[[ 1. -1.]\n",
            " [-1.  1.]]\n",
            "[[1692 1978 1884 2151 2519]\n",
            " [  68  102  110  112  154]]\n",
            "[[97732.7  9107.3]\n",
            " [ 9107.3   941.2]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}