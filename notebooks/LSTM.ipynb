{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nelly-hateva/tardis/blob/master/notebooks/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjJf48UmdOiw"
      },
      "source": [
        "Prerequisities: Access data in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXA25lCFc93U",
        "outputId": "bf31c26b-0cf7-47a3-bc36-f8fee2ea6e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "MOUNT_POINT = \"/content/drive/\"\n",
        "DATA_DIR = MOUNT_POINT + \"My Drive/Colab Notebooks/Thesis-Data\"\n",
        "drive.mount(MOUNT_POINT)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMxbQJztK4D"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgm-qk7bpN2s"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import numpy\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.utils.data as tud\n",
        "from collections import defaultdict, Counter\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lswi4u5DZ2XW"
      },
      "source": [
        "Set random seed and default dtype to double"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8LpjujPZu9K"
      },
      "source": [
        "def seed_torch(seed=666):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  numpy.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_torch()\n",
        "torch.set_default_dtype(torch.float64) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol9WEP5spjmO"
      },
      "source": [
        "Define Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rabbQj3Qpm5s"
      },
      "source": [
        "class NLDataset(tud.Dataset):\n",
        "  def __init__(self, dataset):\n",
        "\n",
        "    data, length, labels = dataset\n",
        "\n",
        "    self.data = torch.tensor(data).long().cuda()\n",
        "    self.length = torch.tensor(length).long().cuda()\n",
        "    self.labels = torch.tensor(labels).long().cuda()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "      'x': self.data[idx],\n",
        "      'length': self.length[idx],\n",
        "      'y': self.labels[idx]\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTeOjoPkddtR"
      },
      "source": [
        "Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36fG5vbJdXWs"
      },
      "source": [
        "class NLNN(nn.Module):\n",
        "\n",
        "  def __init__(self, params):\n",
        "    super(NLNN, self).__init__()\n",
        "\n",
        "    # + 1 because of the padding with 0s\n",
        "    num_embeddings = params['alphabet_size'] + 1\n",
        "\n",
        "    if 'embedding_size' in params:\n",
        "      self.embeddings = nn.Embedding(\n",
        "        num_embeddings,\n",
        "        params['embedding_size'],\n",
        "        padding_idx = 0\n",
        "      )\n",
        "      self.embeddings.weight.data.uniform_(-0.05, 0.05)\n",
        "      embedding_dim = params['embedding_size']\n",
        "    else:\n",
        "      # one hot encoding\n",
        "      self.embeddings = nn.Embedding(\n",
        "          num_embeddings,\n",
        "          num_embeddings,\n",
        "          padding_idx = 0\n",
        "      )\n",
        "      self.embeddings.weight.data = torch.eye(num_embeddings)\n",
        "      self.embeddings.weight.requires_grad = False\n",
        "      embedding_dim = num_embeddings\n",
        "\n",
        "    self.lstm = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=params['hidden_lstm_dim'],\n",
        "      batch_first=True,\n",
        "    )\n",
        "\n",
        "    self.linear = nn.Linear(\n",
        "      in_features=params['hidden_lstm_dim'], \n",
        "      out_features=2,\n",
        "      bias=True\n",
        "    )\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  # def forward(self, x, length):\n",
        "  #   if isinstance(input, PackedSequence):\n",
        "  #     return self.forward_packed(x, length)\n",
        "  #   else:\n",
        "  #     return self.forward_tensor(x, length)\n",
        "\n",
        "  # def forward_tensor(self, x, length):\n",
        "  #   pass\n",
        "\n",
        "  def forward(self, x, length):\n",
        "    embeddings = self.embeddings(x)\n",
        "\n",
        "    embeddings = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "      embeddings, length, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "    outputs, (ht, ct) = self.lstm(embeddings)\n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "    lstm_out = ht[-1,:,:] # get the last hidden state of the outmost layer\n",
        "\n",
        "    linear = self.linear(lstm_out)\n",
        "    return self.softmax(linear)\n",
        "\n",
        "  def lstm_hidden_states(self, x, length, *args):\n",
        "    embeddings = self.embeddings(x)\n",
        "\n",
        "    embeddings = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "      embeddings, length, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "\n",
        "    if len(args) == 2:\n",
        "      h_0, c_0 = args\n",
        "      outputs, (ht, ct) = self.lstm(embeddings, (h_0, c_0))\n",
        "    else:\n",
        "      outputs, (ht, ct) = self.lstm(embeddings)\n",
        "\n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "    return outputs, (ht, ct)\n",
        "\n",
        "  def linear_output(self, lstm_out):\n",
        "    linear = self.linear(lstm_out)\n",
        "    return linear\n",
        "\n",
        "  def vectors(self, x, length, h_0, c_0):\n",
        "    embeddings = self.embeddings(x)\n",
        "\n",
        "    embeddings = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "      embeddings, length, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "    outputs, (ht, ct) = self.lstm(embeddings, (h_0, c_0))\n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "    lstm_out = ht[-1,:,:] # get the last hidden state of the outmost layer\n",
        "\n",
        "    linear = self.linear(lstm_out)\n",
        "\n",
        "    return (ht, ct), linear\n",
        "\n",
        "  def classify(self, linear):\n",
        "    softmax = self.softmax(linear)\n",
        "    return softmax.argmax(dim=1).cpu().numpy()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_9KFhqgG1c6"
      },
      "source": [
        "Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uodxVTrDG4zF"
      },
      "source": [
        "def accuracy(predictions, refs):\n",
        "  assert(len(predictions) == len(refs))\n",
        "\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "  for prediction, label in zip(predictions, refs):\n",
        "    if label == 1:\n",
        "      if prediction == 1:\n",
        "        tp += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        fp += 1\n",
        "      else:\n",
        "        tn += 1\n",
        "\n",
        "  return tp, tn, fp, fn, (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "def evaluate_model(data_loader, model, set_name):\n",
        "  predictions, labels = [], []\n",
        "  for data in data_loader:\n",
        "    result = model(data['x'], data['length'])\n",
        "    argmax = result.argmax(dim=1).cpu().numpy()\n",
        "    predictions.extend(list(argmax))\n",
        "    labels.extend(list(data['y'].cpu().numpy()))\n",
        "\n",
        "  tp, tn, fp, fn, acc = accuracy(predictions, labels)\n",
        "\n",
        "  if tp == 0:\n",
        "    if fn == 0 and fp == 0:\n",
        "      pr, r, f1 = 1, 1, 1\n",
        "    else:\n",
        "      pr, r, f1 = 0, 0, 0\n",
        "  else:\n",
        "    pr = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    f1 = 2 * ((pr * r) / (pr + r))\n",
        "  print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "    set_name, tp, tn, fp, fn, pr, r, f1, acc\n",
        "  ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uirm5lNiHfZZ"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfgtsKPWx50s"
      },
      "source": [
        "def load_data(filename_data, filename_length, filename_labels):\n",
        "  return numpy.load(filename_data, allow_pickle=True), \\\n",
        "    numpy.load(filename_length, allow_pickle=True), \\\n",
        "    numpy.load(filename_labels, allow_pickle=True)\n",
        "\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/numeral-50/train.data.npy\",\n",
        "  DATA_DIR + \"/numeral-50/train.length.npy\",\n",
        "  DATA_DIR + \"/numeral-50/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/numeral-50/dev.data.npy\",\n",
        "  DATA_DIR + \"/numeral-50/dev.length.npy\",\n",
        "  DATA_DIR + \"/numeral-50/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/numeral-50/test.data.npy\",\n",
        "  DATA_DIR + \"/numeral-50/test.length.npy\",\n",
        "  DATA_DIR + \"/numeral-50/test.labels.npy\"\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnaYhnvOrzM0"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYOMQQCKr1rn"
      },
      "source": [
        "def train_model(\n",
        "    params=None, params_path=None, model_path=None,\n",
        "    train_loader=None, dev_loader=None, test_loader=None\n",
        "):\n",
        "  model = NLNN(params)\n",
        "  model.cuda()\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters()) #weight_decay=1e-5\n",
        "\n",
        "  trainable_parameters = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad\n",
        "  )\n",
        "  print(params)\n",
        "  print(\"\")\n",
        "  print(\"Trainable parameters {0:,}\".format(trainable_parameters))\n",
        "  print(\"\")\n",
        "\n",
        "  best_dev_accuracy, best_state_dict, best_epoch = 0, dict(), -1\n",
        "\n",
        "  for epoch in range(params['num_epochs']):\n",
        "\n",
        "    model.train() # set the model to training mode  \n",
        "    print('Epoch {}/{} : '.format(epoch + 1, params['num_epochs']))\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    for data in train_loader:\n",
        "      model.zero_grad()\n",
        "      predictions = model(data['x'], data['length'])\n",
        "      batch_loss = loss_function(predictions, data['y'])\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    model.eval() # set the model to eval mode\n",
        "\n",
        "    predictions_dev, labels_dev = [], []\n",
        "\n",
        "    for data in dev_loader:\n",
        "      dev_predictions = model(data['x'], data['length'])\n",
        "\n",
        "      argmax = dev_predictions.argmax(dim=1).cpu().numpy()\n",
        "      predictions_dev.extend(list(argmax))\n",
        "      labels_dev.extend(list(data['y'].cpu().numpy()))\n",
        "\n",
        "    _, _, _, _, dev_accuracy = accuracy(predictions_dev, labels_dev)\n",
        "\n",
        "    if dev_accuracy > best_dev_accuracy:\n",
        "      best_state_dict = copy.deepcopy(model.state_dict())\n",
        "      best_epoch = epoch\n",
        "\n",
        "    print(\n",
        "      \"dev accuracy:{}\\ttime:{:.2f}s\"\n",
        "      .format(dev_accuracy, time.time() - t0)\n",
        "    )\n",
        "\n",
        "  model.load_state_dict(best_state_dict)\n",
        "  if model_path:\n",
        "    print(\"Best dev epoch {}\".format(epoch))\n",
        "    torch.save(model.state_dict(), DATA_DIR + model_path)\n",
        "    if params_path:\n",
        "      with open(DATA_DIR + params_path, 'wb') as f:\n",
        "        pickle.dump(params, f)\n",
        "\n",
        "  model.eval()\n",
        "  evaluate_model(train_loader, model, \"train\")\n",
        "  evaluate_model(dev_loader, model, \"dev\")\n",
        "  evaluate_model(test_loader, model, \"test\")\n",
        "\n",
        "alphabet = {}\n",
        "with open(DATA_DIR + '/model/light/alphabet.dict', 'rb') as f:\n",
        "  alphabet = pickle.load(f)\n",
        "\n",
        "params = {\n",
        "  'batch_size': 5,\n",
        "  'alphabet_size': len(alphabet),\n",
        "  # 'embedding_size': 1,\n",
        "  'hidden_lstm_dim': 3,\n",
        "  'num_epochs': 2000\n",
        "}\n",
        "train_loader = tud.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_loader = tud.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_loader = tud.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "# train_model(\n",
        "#     params=params,\n",
        "#     params_path=\"/model/light/model.params\",\n",
        "#     model_path=\"/model/light/model.pt\",\n",
        "#     train_loader=train_loader, dev_loader=dev_loader, test_loader=test_loader\n",
        "# )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4EDGrCmDDAG",
        "outputId": "1637f8cf-187d-4474-82fa-0fb1e7d97aef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def load_model(params_path=None, model_path=None):\n",
        "  with open(DATA_DIR + params_path, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "  model = NLNN(params)\n",
        "  model.load_state_dict(torch.load(DATA_DIR + model_path))\n",
        "  model.to(torch.device(\"cuda\"))\n",
        "  model.eval()\n",
        "  return model, params\n",
        "\n",
        "model, _ = load_model(\n",
        "    params_path=\"/model/light/model.params\",\n",
        "    model_path=\"/model/light/model.pt\"\n",
        ")\n",
        "evaluate_model(train_loader, model, \"train\")\n",
        "evaluate_model(dev_loader, model, \"dev\")\n",
        "evaluate_model(test_loader, model, \"test\")\n",
        "\n",
        "inv_alphabet = { v : k for k, v in alphabet.items() }\n",
        "\n",
        "data['x'] = \n",
        "#   h_0 = torch.zeros(1, 1, 3).cuda()\n",
        "#   c_0 = torch.zeros(1, 1, 3).cuda()\n",
        "#   vec = model.vectors(data['x'], data['length'], h_0, c_0)\n",
        "\n",
        "#   result = model(data['x'], data['length'])\n",
        "#   prediction = result.argmax(dim=1).cpu().numpy()[0]\n",
        "\n",
        "#   label = data['y'].cpu().numpy()[0]\n",
        "#   if label == 0 and prediction == 0:\n",
        "#       # print (vec)\n",
        "#       # print (\"\".join([inv_alphabet[c] for c in data['x'][0].cpu().numpy()[:data['length'][0]]]))\n",
        "#       #break\n",
        "\n",
        "mht = torch.tensor([5.8730, -5.9212]).cuda()\n",
        "print(model.classify(mht.view(1, 2)))\n",
        "\n",
        "# mht = torch.tensor([0.7337597, 0.26021487, -0.25838602]).cuda()\n",
        "# print(model.classify(mht.view(1, 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train : TP : 39 TN : 37 FP : 3 FN : 1 Pr : 0.9285714285714286 R : 0.975 F1: 0.951219512195122 ACC : 0.95 \n",
            "dev : TP : 3 TN : 4 FP : 2 FN : 1 Pr : 0.6 R : 0.75 F1: 0.6666666666666665 ACC : 0.7 \n",
            "test : TP : 4 TN : 3 FP : 1 FN : 2 Pr : 0.8 R : 0.6666666666666666 F1: 0.7272727272727272 ACC : 0.7 \n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRruEe5Vy9tQ"
      },
      "source": [
        "Get the vectors for K-Means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thgJDOsnyzpn",
        "outputId": "29eab018-e8d2-4e0a-d112-44309534b8f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def lstm_vectors(model, params, data_loader, filter_negative=False):\n",
        "  hidden_size = params['hidden_lstm_dim']\n",
        "  total_samples, vectors = 0, defaultdict(int)\n",
        "\n",
        "  for data in data_loader:\n",
        "\n",
        "    if filter_negative:\n",
        "      # get only vectors from samples which are classfied as positive by the model\n",
        "      argmax = model(data['x'], data['length']).argmax(dim=1)\n",
        "      positive_samples = (argmax == 1).nonzero().reshape(-1)\n",
        "      data_x = torch.index_select(data['x'], 0, positive_samples)\n",
        "      data_length = data['length'][positive_samples]\n",
        "    else:\n",
        "      data_x = data['x']\n",
        "      data_length = data['length']\n",
        "\n",
        "    samples_in_batch = len(data_x)\n",
        "    total_samples += samples_in_batch\n",
        "\n",
        "    ones = torch.ones(samples_in_batch).cuda()\n",
        "    h_0 = torch.zeros(1, samples_in_batch, hidden_size).cuda()\n",
        "    c_0 = torch.zeros(1, samples_in_batch, hidden_size).cuda()\n",
        "\n",
        "    for j in range(data_length.max()):\n",
        "      (h_t, c_t), linear = model.vectors(data_x[:, j : j + 1], ones, h_0, c_0)\n",
        "      h_0, c_0 = h_t, c_t\n",
        "\n",
        "      concat = torch.cat((h_t[-1,:,:], c_t[-1,:,:], linear), 1)\n",
        "      # The words are padded with zero and we should filter out the tensors\n",
        "      y1 = torch.ones_like(data_length)\n",
        "      y2 = torch.zeros_like(data_length)\n",
        "      indices = torch.where((j < data_length), y1, y2).bool()\n",
        "      concat = concat[indices].cpu().detach().numpy()\n",
        "\n",
        "      for vector in concat:\n",
        "        vectors[tuple(vector)] += 1\n",
        "\n",
        "  initial_state = torch.zeros(hidden_size).cuda()\n",
        "  linear = model.linear_output(initial_state.view(1, 1, -1))\n",
        "  initial_state =  torch.cat((initial_state, initial_state, linear[-1, -1, :]), 0).cpu().detach().numpy()\n",
        "  initial_state = tuple(initial_state)\n",
        "  vectors[initial_state] += total_samples\n",
        "  return vectors, initial_state\n",
        "\n",
        "def truncate(n, decimals=14):\n",
        "  multiplier = 10 ** decimals\n",
        "  return int(n * multiplier) / multiplier\n",
        "\n",
        "def test_lstm_vectors(model, params, data_loader, decimals=14):\n",
        "  vectors, initial_state = lstm_vectors(model, params, data_loader)\n",
        "  truncated_vectors = {}\n",
        "  for vector, count in vectors.items():\n",
        "    truncated_vectors[tuple([truncate(v, decimals=decimals) for v in vector])] = count\n",
        "  initial_state = tuple([truncate(v, decimals=decimals) for v in initial_state])\n",
        "  vectors = truncated_vectors\n",
        "\n",
        "  hidden_size, expected_vectors_count, samples_count = 0, 0, 0\n",
        "  for data in data_loader:\n",
        "    # for one word with length n, we should have n + 1 vectors \n",
        "    expected_vectors_count += (torch.sum(data['length']).item() + len(data['length']))\n",
        "    samples_count += len(data['x'])\n",
        "\n",
        "    _, (ht, ct) = model.lstm_hidden_states(data['x'], data['length'])\n",
        "    linear = model.linear_output(ht)\n",
        "\n",
        "    hidden_size = ht.size()[2]\n",
        "\n",
        "    for i in range(len(data['x'])):\n",
        "      vector = tuple(torch.cat((ht[-1, i, :], ct[-1, i, :], linear[-1, i, :]), 0).cpu().detach().numpy())\n",
        "      vector = tuple([truncate(v, decimals=decimals) for v in vector])\n",
        "      assert vectors[vector] >= 1\n",
        "\n",
        "  vectors_count = sum([ count for vector, count in vectors.items()])\n",
        "  assert vectors_count == expected_vectors_count\n",
        "\n",
        "  # each sample starts with the vector at time step 0\n",
        "  start = torch.zeros(hidden_size).cuda()\n",
        "  linear = model.linear_output(start.view(1, 1, -1))\n",
        "  start = torch.cat((torch.zeros(hidden_size * 2).cuda(), linear[-1, -1, :]), dim=0).cpu().detach().numpy()\n",
        "  start = tuple([truncate(v, decimals=decimals) for v in start])\n",
        "  assert vectors[start] >= samples_count\n",
        "\n",
        "  return vectors, initial_state\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path=\"/model/light/model.params\",\n",
        "    model_path=\"/model/light/model.pt\"\n",
        ")\n",
        "\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/model/light/train.data.npy\",\n",
        "  DATA_DIR + \"/model/light/train.length.npy\",\n",
        "  DATA_DIR + \"/model/light/train.labels.npy\"\n",
        ")\n",
        "train_loader = tud.DataLoader(\n",
        "  NLDataset(train_data), batch_size=10, shuffle = False\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "#test_lstm_vectors(model, params, train_loader, decimals=9)\n",
        "vectors, initial_state = lstm_vectors(model, params, train_loader, filter_negative=True)\n",
        "\n",
        "print(\"Elapsed: {:.2f}s\".format(time.time() - t0))\n",
        "print(\"Number of unique vectors {0:,}\".format(len(vectors)))\n",
        "print(\"Number of total vectors {0:,}\".format(sum(vectors.values())))\n",
        "\n",
        "with open(DATA_DIR + \"/kMeans/light+dense/vectors-train-pos.txt\", \"w\") as f:\n",
        "  # intial state first\n",
        "  count = vectors.pop(initial_state)\n",
        "  f.write(str(count))\n",
        "  f.write(\"\\t\")\n",
        "  for i in range(len(initial_state)):\n",
        "    f.write(str(initial_state[i]))\n",
        "    f.write(\" \")\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "  for vector, count in vectors.items():\n",
        "    f.write(str(count))\n",
        "    f.write(\"\\t\")\n",
        "    for i in range(len(vector)):\n",
        "      f.write(str(vector[i]))\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed: 0.12s\n",
            "Number of unique vectors 102\n",
            "Number of total vectors 444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv_cI6JM8ISc"
      },
      "source": [
        "Covariance matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmds55_VQwuh"
      },
      "source": [
        "vectors = []\n",
        "with open(DATA_DIR + \"/kMeans/large/vectors-dev-9.txt\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      vec = line.split(\"\\t\")[1]\n",
        "      vectors.append([float(p) for p in vec.split(\" \")])\n",
        "\n",
        "numpy.savetxt(\n",
        "    DATA_DIR + \"/kMeans/large/vectors-dev-9-cov.txt\",\n",
        "    numpy.cov(numpy.array(vectors).T),\n",
        "    delimiter=',', fmt='%1.3f'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2gsu41wawYy"
      },
      "source": [
        "Load K centroids and build automata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCgRfLNa9uMP"
      },
      "source": [
        "def closest_centroid_naive(centroids, t):\n",
        "  min_dist, min_dist_centroid = float(\"inf\"), -1\n",
        "\n",
        "  for i in range(len(centroids)):\n",
        "    dist = numpy.sum([(x_i - y_i) ** 2 for (x_i, y_i) in zip(centroids[i], t)])\n",
        "    if dist < min_dist:\n",
        "      min_dist, min_dist_centroid = dist, i\n",
        "\n",
        "  return min_dist_centroid\n",
        "\n",
        "def closest_centroid(centroids, t):\n",
        "  subtraction = centroids - t.expand_as(centroids)\n",
        "  dist = torch.sum(subtraction * subtraction, dim=1)\n",
        "  return torch.argmin(dist, dim=0)\n",
        "\n",
        "def test_closest_centroid():\n",
        "  rand_dim = random.randint(64, 128)\n",
        "  rand_centroids_count = random.randint(1024, 2048)\n",
        "  rand_centroids = torch.rand(rand_centroids_count, rand_dim)\n",
        "  rand_vectors_count = random.randint(64, 128)\n",
        "\n",
        "  for i in range(rand_vectors_count):\n",
        "    rand_vector = torch.rand(rand_dim)\n",
        "\n",
        "    actual = closest_centroid(rand_centroids, rand_vector).item()\n",
        "    expected = closest_centroid_naive(rand_centroids.cpu().detach().numpy(), rand_vector.cpu().detach().numpy())\n",
        "    assert actual == expected\n",
        "\n",
        "  for i in range(rand_centroids_count):\n",
        "    actual = closest_centroid(rand_centroids, rand_centroids[i]).item()\n",
        "    assert actual == i\n",
        "\n",
        "#test_closest_centroid()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChaHDQiPRSd1"
      },
      "source": [
        "def read_centroids(file):\n",
        "  centroids = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        centroids.append([float(p) for p in line.split()])\n",
        "\n",
        "  print(\"Number of centroids: {0:,}\".format(len(centroids)))\n",
        "\n",
        "  return torch.tensor(centroids).cuda()\n",
        "\n",
        "def get_initial_state_centroid(assignmentsFile):\n",
        "  with open(assignmentsFile, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    return int(lines[0])\n",
        "\n",
        "def write_initial_state(f, initial_state_centroid):\n",
        "  f.write(str(initial_state_centroid))\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "def write_final_states(f, final_states):\n",
        "  for fs in final_states:\n",
        "    f.write(str(fs))\n",
        "    f.write(\" \")\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "def write_transitions(f, transitions):\n",
        "  for tr in transitions:\n",
        "    f.write(str(tr[0]) + \" \" + str(tr[1]) + \" \" + str(tr[2]))\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "def build_and_save_automaton(model, params, centroidsFile, assignmentsFile, automatonFile):\n",
        "  t0 = time.time()\n",
        "\n",
        "  centroids = read_centroids(centroidsFile)\n",
        "  transitions, final_states = automaton(model, params, centroids)\n",
        "\n",
        "  with open(automatonFile, \"w\") as f:\n",
        "      # initial state on the first line\n",
        "      write_initial_state(f, get_initial_state_centroid(assignmentsFile))\n",
        "      # final states separated with space on the second line\n",
        "      write_final_states(f, final_states)\n",
        "      # transitions, each on a separate line\n",
        "      write_transitions(f, transitions)\n",
        "\n",
        "  print(\"Number of tranitions {0:,}\".format(len(transitions)))\n",
        "  print(\"Number of final states {0:,}\".format(len(final_states)))\n",
        "  print(\"Elapsed: {:.2f}s\".format(time.time() - t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qhx_Qs26Bnf",
        "outputId": "b52ff3b7-077c-4d8f-b071-34fe72535c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "def automaton(model, params, centroids):\n",
        "\n",
        "  alphabet_size = params['alphabet_size']\n",
        "  hidden_lstm_dim = params['hidden_lstm_dim']\n",
        "\n",
        "  transitions = []\n",
        "  x = torch.tensor([[i] for i in range(1, alphabet_size + 1)]).cuda()\n",
        "  ones = torch.ones(alphabet_size).cuda()\n",
        "\n",
        "  for q1, centroid in enumerate(centroids):\n",
        "    h_t, c_t, _ = torch.split(centroid, hidden_lstm_dim, dim=0)\n",
        "    h_t = h_t.repeat(alphabet_size, 1).view(1, alphabet_size, hidden_lstm_dim)\n",
        "    c_t = c_t.repeat(alphabet_size, 1).view(1, alphabet_size, hidden_lstm_dim)\n",
        "\n",
        "    _, (h_t, c_t) = model.lstm_hidden_states(x, ones, h_t, c_t)\n",
        "    linear = model.linear_output(h_t)\n",
        "    concat = torch.cat((h_t[-1,:,:], c_t[-1,:,:], linear[-1,:,:]), 1)\n",
        "\n",
        "    for i in range(alphabet_size):\n",
        "      # TODO closest centroid to return vector with size alphabet_size\n",
        "      q2 = closest_centroid(centroids, concat[i])\n",
        "      transitions.append((q1, x[i][0].item(), q2.item()))\n",
        "\n",
        "  _, linear = torch.split(centroids, hidden_lstm_dim * 2, dim=1)\n",
        "  final_states = numpy.where(model.classify(linear) == 1)[0]\n",
        "\n",
        "  return transitions, final_states\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path = \"/model/light/model.params\",\n",
        "    model_path = \"/model/light/model.pt\"\n",
        ")\n",
        "\n",
        "# build_and_save_automaton(\n",
        "#     model, params,\n",
        "#     DATA_DIR + \"/kMeans/tardis/kmeans-standardized-euclidean-distance-centroids-k=34.txt\",\n",
        "#     DATA_DIR + \"/kMeans/tardis/kmeans-standardized-euclidean-distance-assignments-k=34.txt\",\n",
        "#     DATA_DIR + \"/kMeans/tardis/aut-standardized-euclidean-distance-k=34.txt\"\n",
        "# )\n",
        "build_and_save_automaton(\n",
        "    model, params,\n",
        "    DATA_DIR + \"/kMeans/tardis/kmeans-standardized-euclidean-distance-centroids-k=51.txt\",\n",
        "    DATA_DIR + \"/kMeans/tardis/kmeans-standardized-euclidean-distance-assignments-k=51.txt\",\n",
        "    DATA_DIR + \"/kMeans/tardis/aut-standardized-euclidean-distance-k=51.txt\"\n",
        ")\n",
        "build_and_save_automaton(\n",
        "    model, params,\n",
        "    DATA_DIR + \"/kMeans/tardis/kmeans-standardized-euclidean-distance-centroids-k=82.txt\",\n",
        "    DATA_DIR + \"/kMeans/tardis/kmeans-standardized-euclidean-distance-assignments-k=82.txt\",\n",
        "    DATA_DIR + \"/kMeans/tardis/aut-standardized-euclidean-distance-k=82.txt\"\n",
        ")\n",
        "build_and_save_automaton(\n",
        "    model, params,\n",
        "    DATA_DIR + \"/kMeans/tardis/kmeans-standardized-euclidean-distance-centroids-k=102.txt\",\n",
        "    DATA_DIR + \"/kMeans/tardis/kmeans-standardized-euclidean-distance-assignments-k=102.txt\",\n",
        "    DATA_DIR + \"/kMeans/tardis/aut-standardized-euclidean-distance-k=102.txt\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of centroids: 51\n",
            "Number of tranitions 663\n",
            "Number of final states 42\n",
            "Elapsed: 1.31s\n",
            "Number of centroids: 82\n",
            "Number of tranitions 1,066\n",
            "Number of final states 65\n",
            "Elapsed: 1.43s\n",
            "Number of centroids: 102\n",
            "Number of tranitions 1,326\n",
            "Number of final states 83\n",
            "Elapsed: 1.42s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09bOipdnfZC4"
      },
      "source": [
        "K Means Vectors Stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0_o_bfffbFT",
        "outputId": "2e7ce476-eeee-4306-b20d-86fcb1c41e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "def read_vectors(file):\n",
        "  vectors = []\n",
        "  counts = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        count, vector = line.split(\"\\t\")\n",
        "        vectors.append([float(p) for p in vector.split()])\n",
        "        counts.append(int(count))\n",
        "\n",
        "  return vectors, counts\n",
        "\n",
        "def vectors_distances_stats_2(vectors, counts, model, params, batch_size=100):\n",
        "  with torch.no_grad():\n",
        "    vectors = torch.tensor(vectors).cuda()\n",
        "    vectors.requires_grad = False\n",
        "\n",
        "    min_dist, min_i, min_j = float(\"inf\"), -1, -1\n",
        "    max_dist, max_i, max_j = -1, -1, -1\n",
        "    distances = []\n",
        "\n",
        "    for index, vector in enumerate(vectors):\n",
        "      if index == vectors.size()[0] - 1:\n",
        "        break\n",
        "\n",
        "      for j in range(index + 1, vectors.size()[0], batch_size):\n",
        "        vec = vectors[j:j + batch_size, :]\n",
        "        v = vectors[index].expand_as(vec)\n",
        "\n",
        "        substract = vec - v\n",
        "        distance = torch.sum(substract * substract, dim=1)\n",
        "\n",
        "        min_d = torch.min(distance).item()\n",
        "        if min_d < min_dist:\n",
        "          min_dist = min_d\n",
        "          min_i = index\n",
        "          min_j = torch.argmin(distance).item() + j\n",
        "\n",
        "        max_d = torch.max(distance).item()\n",
        "        if max_d > max_dist:\n",
        "          max_dist = max_d\n",
        "          max_i = index\n",
        "          max_j = torch.argmax(distance).item() + j\n",
        "\n",
        "        distances.extend(list(distance.cpu().detach().numpy()))\n",
        "\n",
        "    print(\"Min distance between 2 vectors {}\".format(math.sqrt(min_dist)))\n",
        "    print(\"{} {} {}\".format(\n",
        "        counts[min_i],\n",
        "        model.classify(vectors[min_i].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "        list(vectors[min_i].cpu().detach().numpy())\n",
        "    ))\n",
        "    print(\"{} {} {}\".format(\n",
        "        counts[min_j],\n",
        "        model.classify(vectors[min_j].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "        list(vectors[min_j].cpu().detach().numpy())\n",
        "    ))\n",
        "\n",
        "    print(\"Max distance between 2 vectors {}\".format(math.sqrt(max_dist)))\n",
        "    print(\"{} {} {}\".format(\n",
        "        counts[max_i],\n",
        "        model.classify(vectors[max_i].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "        list(vectors[max_i].cpu().detach().numpy())\n",
        "    ))\n",
        "    print(\"{} {} {}\".format(\n",
        "        counts[max_j],\n",
        "        model.classify(vectors[max_j].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "        list(vectors[max_j].cpu().detach().numpy())\n",
        "    ))\n",
        "\n",
        "    distances = [math.sqrt(d) for d in distances]\n",
        "    print(\"Mean distance between 2 vectors {}\".format(statistics.mean(distances)))\n",
        "    print(\"Pvariance distance between 2 vectors {}\".format(statistics.pvariance(distances)))\n",
        "    print(\"Pstandard deviation distance between 2 vectors {}\".format(statistics.pstdev(distances)))\n",
        "\n",
        "def vectors_distances_stats(vectors, counts, model, params):\n",
        "  vectors = torch.tensor(vectors).cuda()\n",
        "  vectors.requires_grad = False\n",
        "\n",
        "  min_dist, min_i, min_j = float(\"inf\"), -1, -1\n",
        "  max_dist, max_i, max_j = -1, -1, -1\n",
        "  distances = []\n",
        "\n",
        "  for index, vector in enumerate(vectors):\n",
        "    if index == vectors.size()[0] - 1:\n",
        "      break\n",
        "\n",
        "    vec = vectors[index + 1:, :]\n",
        "    v = vectors[index].expand_as(vec)\n",
        "\n",
        "    substract = vec - v\n",
        "    distance = torch.sum(substract * substract, dim=1)\n",
        "\n",
        "    min_d = torch.min(distance).item()\n",
        "    if min_d < min_dist:\n",
        "      min_dist = min_d\n",
        "      min_i = index\n",
        "      min_j = torch.argmin(distance).item() + index + 1\n",
        "\n",
        "    max_d = torch.max(distance).item()\n",
        "    if max_d > max_dist:\n",
        "      max_dist = max_d\n",
        "      max_i = index\n",
        "      max_j = torch.argmax(distance).item() +  index + 1\n",
        "\n",
        "    distances.extend(list(distance.cpu().detach().numpy()))\n",
        "\n",
        "  print(\"Min distance between 2 vectors {}\".format(math.sqrt(min_dist)))\n",
        "  print(\"{} {} {}\".format(\n",
        "      counts[min_i],\n",
        "      model.classify(vectors[min_i].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "      list(vectors[min_i].cpu().detach().numpy())\n",
        "  ))\n",
        "  print(\"{} {} {}\".format(\n",
        "      counts[min_j],\n",
        "      model.classify(vectors[min_j].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "      list(vectors[min_j].cpu().detach().numpy())\n",
        "  ))\n",
        "\n",
        "  print(\"Max distance between 2 vectors {}\".format(math.sqrt(max_dist)))\n",
        "  print(\"{} {} {}\".format(\n",
        "      counts[max_i],\n",
        "      model.classify(vectors[max_i].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "      list(vectors[max_i].cpu().detach().numpy())\n",
        "  ))\n",
        "  print(\"{} {} {}\".format(\n",
        "      counts[max_j],\n",
        "      model.classify(vectors[max_j].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "      list(vectors[max_j].cpu().detach().numpy())\n",
        "  ))\n",
        "\n",
        "  distances = [math.sqrt(d) for d in distances]\n",
        "  print(\"Mean distance between 2 vectors {}\".format(statistics.mean(distances)))\n",
        "  print(\"Pvariance distance between 2 vectors {}\".format(statistics.pvariance(distances)))\n",
        "  print(\"Pstandard deviation distance between 2 vectors {}\".format(statistics.pstdev(distances)))\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path = \"/model/large/model.params\",\n",
        "    model_path = \"/model/large/model.pt\"\n",
        ")\n",
        "vectors, counts = read_vectors(DATA_DIR + \"/kMeans/large/vectors-dev-9.txt\")\n",
        "#vectors, counts = read_vectors(DATA_DIR + \"/kMeans/light+dense/vectors-train-14.txt\")\n",
        "#vectors_distances_stats(vectors, counts, model, params)\n",
        "vectors_distances_stats_2(vectors, counts, model, params)\n",
        "# vectors_distances_stats(vectors, counts, model, params)\n",
        "# print(\"===\")\n",
        "#vectors_distances_stats_2(vectors, counts, model, params, batch_size=)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0806802d6f88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;31m#vectors, counts = read_vectors(DATA_DIR + \"/kMeans/light+dense/vectors-train-14.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;31m#vectors_distances_stats(vectors, counts, model, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m \u001b[0mvectors_distances_stats_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;31m# vectors_distances_stats(vectors, counts, model, params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# print(\"===\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-0806802d6f88>\u001b[0m in \u001b[0;36mvectors_distances_stats_2\u001b[0;34m(vectors, counts, model, params, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstract\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msubstract\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mmin_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmin_d\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0mmin_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin_d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbJkyOKKk2Oj"
      },
      "source": [
        "K Means Results Stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOulJhZg6-1v",
        "outputId": "775a729c-f837-4123-96d6-87ecb50e0e0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "def count_points_in_clusters(assignments):\n",
        "  count_points_in_clusters = numpy.array(list(Counter(assignments).values()))\n",
        "\n",
        "  print(\"Min points in cluster {}\".format(numpy.min(count_points_in_clusters)))\n",
        "  print(\"Max points in cluster {}\".format(numpy.max(count_points_in_clusters)))\n",
        "\n",
        "  print(\"Mean points in cluster {}\".format(numpy.mean(count_points_in_clusters)))\n",
        "  print(\"Variance points in cluster {}\".format(numpy.var(count_points_in_clusters)))\n",
        "  print(\"Standard deviation points in cluster {}\".format(numpy.std(count_points_in_clusters)))\n",
        "  print()\n",
        "\n",
        "def distances_to_centroids(centroids, assignments, vectors, use_variance=False):\n",
        "\n",
        "  if use_variance:\n",
        "    deviation = torch.tensor(numpy.std(vectors, axis=0)).cuda()\n",
        "\n",
        "  assignments = torch.tensor(assignments).cuda()\n",
        "  vectors = torch.tensor(vectors).cuda()\n",
        "\n",
        "  embedding = nn.Embedding(centroids.size()[0], centroids.size()[1])\n",
        "  embedding.weight.requires_grad = False\n",
        "  embedding.weight.data = centroids\n",
        " \n",
        "  substract = vectors - embedding(assignments)\n",
        "  if use_variance:\n",
        "    devision = substract / deviation\n",
        "  else:\n",
        "    devision = substract\n",
        "  distance = torch.sqrt(torch.sum(devision * devision, dim=1)).cpu().detach().numpy()\n",
        "\n",
        "  print(\"Min distance between centroid and vector {}\".format(numpy.min(distance)))\n",
        "  print(\"Max distance between centroid and vector {}\".format(numpy.max(distance)))\n",
        "\n",
        "  print(\"Mean distance between centroid and vector {}\".format(numpy.mean(distance)))\n",
        "  print(\"Variance distance between centroid and vector {}\".format(numpy.var(distance)))\n",
        "  print(\"Standard deviation distance between centroid and vector {}\".format(numpy.std(distance)))\n",
        "  print()\n",
        "\n",
        "def k_means_stats(centroids, assignments, vectors, use_variance=False):\n",
        "  count_points_in_clusters(assignments)\n",
        "  distances_to_centroids(centroids, assignments, vectors, use_variance=use_variance)\n",
        "\n",
        "def read_vectors(file):\n",
        "  vectors = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        count, vector = line.split(\"\\t\")\n",
        "        vectors.append([float(p) for p in vector.split()])\n",
        "\n",
        "  return vectors\n",
        "\n",
        "def read_assignments(file):\n",
        "  assignments = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        assignments.append(int(line))\n",
        "\n",
        "  return assignments\n",
        "\n",
        "assignments = read_assignments(DATA_DIR + \"/kMeans/light+dense/kmeans-standardized-euclidean-distance-assignments-k=34.txt\")\n",
        "centroids = read_centroids(DATA_DIR + \"/kMeans/light+dense/kmeans-standardized-euclidean-distance-centroids-k=34.txt\")\n",
        "vectors = read_vectors(DATA_DIR + \"/kMeans/light+dense/vectors-train-14.txt\")\n",
        "k_means_stats(centroids, assignments, vectors, use_variance=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of centroids: 34\n",
            "Min points in cluster 1\n",
            "Max points in cluster 20\n",
            "Mean points in cluster 8.088235294117647\n",
            "Variance points in cluster 22.845155709342556\n",
            "Standard deviation points in cluster 4.779660627005076\n",
            "\n",
            "Min distance between centroid and vector 0.0\n",
            "Max distance between centroid and vector 1.6243095065812312\n",
            "Mean distance between centroid and vector 0.6980171488806557\n",
            "Variance distance between centroid and vector 0.09505240878611079\n",
            "Standard deviation distance between centroid and vector 0.30830570670376956\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH9WnCfQ_dox"
      },
      "source": [
        "Properties of the covariance matrix of the vectors for clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9rEnzIP-wu7",
        "outputId": "8fd6b8d4-513a-454c-e5cd-880905907249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy\n",
        "\n",
        "def read_vectors(file):\n",
        "  vectors = []\n",
        "  counts = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        count, vector = line.split(\"\\t\")\n",
        "        vectors.append([float(p) for p in vector.split()])\n",
        "        counts.append(int(count))\n",
        "\n",
        "  return vectors, counts\n",
        "\n",
        "# vectors, counts = read_vectors(DATA_DIR + \"/kMeans/large/vectors-train.txt\")\n",
        "# covariance_matrix = numpy.cov(numpy.array(vectors).T)\n",
        "\n",
        "# print (\"Covariance matrix \", covariance_matrix)\n",
        "# print (\"Covariance matrix rank \", numpy.linalg.matrix_rank(covariance_matrix))\n",
        "#print (\"Vectors rank \", numpy.linalg.matrix_rank(numpy.array(vectors)))\n",
        "\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# ainv = numpy.linalg.inv(covariance_matrix)\n",
        "# print (ainv)\n",
        "# numpy.allclose(numpy.dot(covariance_matrix, ainv), numpy.eye(202))\n",
        "\n",
        "# print (numpy.linalg.det(covariance_matrix))\n",
        "# print(numpy.isclose( numpy.linalg.det(covariance_matrix), 0))\n",
        "# iv = [[1, 0.5, 0.5], [0.5, 1, 0.5], [0.5, 0.5, 1]]\n",
        "# distance.mahalanobis([2, 0, 0], [0, 1, 0], iv)\n",
        "a = numpy.array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]])\n",
        "print (numpy.linalg.inv(a))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.         0.         0.        ]\n",
            " [0.         0.5        0.         0.        ]\n",
            " [0.         0.         0.33333333 0.        ]\n",
            " [0.         0.         0.         0.25      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}