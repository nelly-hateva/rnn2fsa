{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thesis - Train NN",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjJf48UmdOiw",
        "colab_type": "text"
      },
      "source": [
        "Prerequisities: Access data in google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXA25lCFc93U",
        "colab_type": "code",
        "outputId": "83e15208-3c61-4475-c1a4-0c3444cb06cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "MOUNT_POINT = \"/content/drive/\"\n",
        "DATA_DIR = MOUNT_POINT + \"My Drive/Colab Notebooks/Thesis-Data/bg\"\n",
        "drive.mount(MOUNT_POINT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCMxbQJztK4D",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgm-qk7bpN2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import numpy\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.utils.data as tud\n",
        "from collections import defaultdict, Counter\n",
        "import statistics\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lswi4u5DZ2XW",
        "colab_type": "text"
      },
      "source": [
        "Set random seed and default dtype to double"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8LpjujPZu9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_torch(seed=666):\n",
        "  random.seed(seed)\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  numpy.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_torch()\n",
        "torch.set_default_dtype(torch.float64) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol9WEP5spjmO",
        "colab_type": "text"
      },
      "source": [
        "Define Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rabbQj3Qpm5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLDataset(tud.Dataset):\n",
        "  def __init__(self, dataset):\n",
        "\n",
        "    data, length, labels = dataset\n",
        "\n",
        "    self.data = torch.tensor(data).long().cuda()\n",
        "    self.length = torch.tensor(length).long().cuda()\n",
        "    self.labels = torch.tensor(labels).long().cuda()\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "      'x': self.data[idx],\n",
        "      'length': self.length[idx],\n",
        "      'y': self.labels[idx]\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTeOjoPkddtR",
        "colab_type": "text"
      },
      "source": [
        "Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36fG5vbJdXWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLNN(nn.Module):\n",
        "\n",
        "  def __init__(self, params):\n",
        "    super(NLNN, self).__init__()\n",
        "\n",
        "    # + 1 because of the padding with 0s\n",
        "    num_embeddings = params['alphabet_size'] + 1\n",
        "\n",
        "    if 'embedding_size' in params:\n",
        "      self.embeddings = nn.Embedding(\n",
        "        num_embeddings,\n",
        "        params['embedding_size'],\n",
        "        padding_idx = 0\n",
        "      )\n",
        "      self.embeddings.weight.data.uniform_(-0.05, 0.05)\n",
        "      embedding_dim = params['embedding_size']\n",
        "    else:\n",
        "      # one hot encoding\n",
        "      self.embeddings = nn.Embedding(\n",
        "          num_embeddings,\n",
        "          num_embeddings,\n",
        "          padding_idx = 0\n",
        "      )\n",
        "      self.embeddings.weight.data = torch.eye(num_embeddings)\n",
        "      self.embeddings.weight.requires_grad = False\n",
        "      embedding_dim = num_embeddings\n",
        "\n",
        "    self.lstm = nn.LSTM(\n",
        "      input_size=embedding_dim,\n",
        "      hidden_size=params['hidden_lstm_dim'],\n",
        "      batch_first=True,\n",
        "    )\n",
        "\n",
        "    self.linear = nn.Linear(\n",
        "      in_features=params['hidden_lstm_dim'], \n",
        "      out_features=2,\n",
        "      bias=True\n",
        "    )\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, x, length):\n",
        "    embeddings = self.embeddings(x)\n",
        "\n",
        "    embeddings = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "      embeddings, length, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "    outputs, (ht, ct) = self.lstm(embeddings)\n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "    lstm_out = ht[-1,:,:] # get the last hidden state of the outmost layer\n",
        "\n",
        "    linear = self.linear(lstm_out)\n",
        "    return self.softmax(linear)\n",
        "\n",
        "  def lstm_hidden_states(self, x, length, *args):\n",
        "    embeddings = self.embeddings(x)\n",
        "\n",
        "    embeddings = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "      embeddings, length, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "\n",
        "    if len(args) == 2:\n",
        "      h_0, c_0 = args\n",
        "      outputs, (ht, ct) = self.lstm(embeddings, (h_0, c_0))\n",
        "    else:\n",
        "      outputs, (ht, ct) = self.lstm(embeddings)\n",
        "\n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "    return outputs, (ht, ct)\n",
        "\n",
        "  def linear_output(self, lstm_out):\n",
        "    linear = self.linear(lstm_out)\n",
        "    return linear\n",
        "\n",
        "  def vectors(self, x, length, h_0, c_0):\n",
        "    embeddings = self.embeddings(x)\n",
        "\n",
        "    embeddings = torch.nn.utils.rnn.pack_padded_sequence(\n",
        "      embeddings, length, batch_first=True, enforce_sorted=False\n",
        "    )\n",
        "    outputs, (ht, ct) = self.lstm(embeddings, (h_0, c_0))\n",
        "    outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "    lstm_out = ht[-1,:,:] # get the last hidden state of the outmost layer\n",
        "\n",
        "    linear = self.linear(lstm_out)\n",
        "\n",
        "    return (ht, ct), linear\n",
        "\n",
        "  def classify(self, linear):\n",
        "    softmax = self.softmax(linear)\n",
        "    return softmax.argmax(dim=1).cpu().numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_9KFhqgG1c6",
        "colab_type": "text"
      },
      "source": [
        "Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uodxVTrDG4zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(predictions, refs):\n",
        "  assert(len(predictions) == len(refs))\n",
        "\n",
        "  tp, tn, fp, fn = 0, 0, 0, 0\n",
        "  for prediction, label in zip(predictions, refs):\n",
        "    if label == 1:\n",
        "      if prediction == 1:\n",
        "        tp += 1\n",
        "      else:\n",
        "        fn += 1\n",
        "    else:\n",
        "      if prediction == 1:\n",
        "        fp += 1\n",
        "      else:\n",
        "        tn += 1\n",
        "\n",
        "  return tp, tn, fp, fn, (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "def evaluate_model(data_loader, model, set_name):\n",
        "  predictions, labels = [], []\n",
        "  for data in data_loader:\n",
        "    result = model(data['x'], data['length'])\n",
        "    argmax = result.argmax(dim=1).cpu().numpy()\n",
        "    predictions.extend(list(argmax))\n",
        "    labels.extend(list(data['y'].cpu().numpy()))\n",
        "\n",
        "  tp, tn, fp, fn, acc = accuracy(predictions, labels)\n",
        "\n",
        "  if tp == 0:\n",
        "    if fn == 0 and fp == 0:\n",
        "      pr, r, f1 = 1, 1, 1\n",
        "    else:\n",
        "      pr, r, f1 = 0, 0, 0\n",
        "  else:\n",
        "    pr = tp / (tp + fp)\n",
        "    r = tp / (tp + fn)\n",
        "    f1 = 2 * ((pr * r) / (pr + r))\n",
        "  print(\"{} : TP : {} TN : {} FP : {} FN : {} Pr : {} R : {} F1: {} ACC : {} \".format(\n",
        "    set_name, tp, tn, fp, fn, pr, r, f1, acc\n",
        "  ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uirm5lNiHfZZ",
        "colab_type": "text"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfgtsKPWx50s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(filename_data, filename_length, filename_labels):\n",
        "  return numpy.load(filename_data, allow_pickle=True), \\\n",
        "    numpy.load(filename_length, allow_pickle=True), \\\n",
        "    numpy.load(filename_labels, allow_pickle=True)\n",
        "\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/model/light/train.data.npy\",\n",
        "  DATA_DIR + \"/model/light/train.length.npy\",\n",
        "  DATA_DIR + \"/model/light/train.labels.npy\"\n",
        ")\n",
        "dev_data = load_data(\n",
        "  DATA_DIR + \"/model/light/dev.data.npy\",\n",
        "  DATA_DIR + \"/model/light/dev.length.npy\",\n",
        "  DATA_DIR + \"/model/light/dev.labels.npy\"\n",
        ")\n",
        "test_data = load_data(\n",
        "  DATA_DIR + \"/model/light/test.data.npy\",\n",
        "  DATA_DIR + \"/model/light/test.length.npy\",\n",
        "  DATA_DIR + \"/model/light/test.labels.npy\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnaYhnvOrzM0",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYOMQQCKr1rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(\n",
        "    params=None, params_path=None, model_path=None,\n",
        "    train_loader=None, dev_loader=None, test_loader=None\n",
        "):\n",
        "  model = NLNN(params)\n",
        "  model.cuda()\n",
        "  loss_function = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(model.parameters()) #weight_decay=1e-5\n",
        "\n",
        "  trainable_parameters = sum(\n",
        "    p.numel() for p in model.parameters() if p.requires_grad\n",
        "  )\n",
        "  print(params)\n",
        "  print(\"\")\n",
        "  print(\"Trainable parameters {0:,}\".format(trainable_parameters))\n",
        "  print(\"\")\n",
        "\n",
        "  best_dev_accuracy, best_state_dict, best_epoch = 0, dict(), -1\n",
        "\n",
        "  for epoch in range(params['num_epochs']):\n",
        "\n",
        "    model.train() # set the model to training mode  \n",
        "    print('Epoch {}/{} : '.format(epoch + 1, params['num_epochs']))\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    for data in train_loader:\n",
        "      model.zero_grad()\n",
        "      predictions = model(data['x'], data['length'])\n",
        "      batch_loss = loss_function(predictions, data['y'])\n",
        "      batch_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    model.eval() # set the model to eval mode\n",
        "\n",
        "    predictions_dev, labels_dev = [], []\n",
        "\n",
        "    for data in dev_loader:\n",
        "      dev_predictions = model(data['x'], data['length'])\n",
        "\n",
        "      argmax = dev_predictions.argmax(dim=1).cpu().numpy()\n",
        "      predictions_dev.extend(list(argmax))\n",
        "      labels_dev.extend(list(data['y'].cpu().numpy()))\n",
        "\n",
        "    _, _, _, _, dev_accuracy = accuracy(predictions_dev, labels_dev)\n",
        "\n",
        "    if dev_accuracy > best_dev_accuracy:\n",
        "      best_state_dict = model.state_dict()\n",
        "      best_epoch = epoch\n",
        "\n",
        "    print(\n",
        "      \"dev accuracy:{}\\ttime:{:.2f}s\"\n",
        "      .format(dev_accuracy, time.time() - t0)\n",
        "    )\n",
        "\n",
        "  model.load_state_dict(best_state_dict)\n",
        "  if model_path:\n",
        "    print(\"Best dev epoch {}\".format(epoch))\n",
        "    torch.save(model.state_dict(), DATA_DIR + model_path)\n",
        "    if params_path:\n",
        "      with open(DATA_DIR + params_path, 'wb') as f:\n",
        "        pickle.dump(params, f)\n",
        "\n",
        "  model.eval()\n",
        "  evaluate_model(train_loader, model, \"train\")\n",
        "  evaluate_model(dev_loader, model, \"dev\")\n",
        "  evaluate_model(test_loader, model, \"test\")\n",
        "\n",
        "alphabet = {}\n",
        "with open(DATA_DIR + '/model/light/alphabet.dict', 'rb') as f:\n",
        "  alphabet = pickle.load(f)\n",
        "\n",
        "params = {\n",
        "  'batch_size': 5,\n",
        "  'alphabet_size': len(alphabet),\n",
        "  # 'embedding_size': 1,\n",
        "  'hidden_lstm_dim': 3,\n",
        "  'num_epochs': 2000\n",
        "}\n",
        "train_loader = tud.DataLoader(\n",
        "  NLDataset(train_data), batch_size=params[\"batch_size\"], shuffle = True\n",
        ")\n",
        "dev_loader = tud.DataLoader(\n",
        "  NLDataset(dev_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "test_loader = tud.DataLoader(\n",
        "  NLDataset(test_data), batch_size=params[\"batch_size\"]\n",
        ")\n",
        "\n",
        "# train_model(\n",
        "#     params=params,\n",
        "#     params_path=\"/model/light/model.params\",\n",
        "#     model_path=\"/model/light/model.pt\",\n",
        "#     train_loader=train_loader, dev_loader=dev_loader, test_loader=test_loader\n",
        "# )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4EDGrCmDDAG",
        "colab_type": "code",
        "outputId": "a3b9bb13-c721-4434-b150-abd0e7fa468a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "def load_model(params_path=None, model_path=None):\n",
        "  with open(DATA_DIR + params_path, 'rb') as f:\n",
        "    params = pickle.load(f)\n",
        "  model = NLNN(params)\n",
        "  model.load_state_dict(torch.load(DATA_DIR + model_path))\n",
        "  model.to(torch.device(\"cuda\"))\n",
        "  model.eval()\n",
        "  return model, params\n",
        "\n",
        "model, _ = load_model(\n",
        "    params_path=\"/model/light/model.params\",\n",
        "    model_path=\"/model/light/model.pt\"\n",
        ")\n",
        "evaluate_model(train_loader, model, \"train\")\n",
        "evaluate_model(dev_loader, model, \"dev\")\n",
        "evaluate_model(test_loader, model, \"test\")\n",
        "\n",
        "# inv_alphabet = { v : k for k, v in alphabet.items() }\n",
        "# for data in test_loader:\n",
        "#   result = model(data['x'], data['length'])\n",
        "#   o, (ht, ct) = model.lstm_hidden_states(data['x'], data['length'])\n",
        "#   prediction = result.argmax(dim=1).cpu().numpy()[0]\n",
        "#   label = data['y'].cpu().numpy()[0]\n",
        "#   if label == 1 and prediction == 1:\n",
        "#       print (o, ht, ct)\n",
        "#       print (\"\".join([inv_alphabet[c] for c in data['x'][0].cpu().numpy()[:data['length'][0]]]))\n",
        "\n",
        "# mht = torch.tensor([0.7338, 0.2602, -0.2584]).cuda()\n",
        "# print(model.classify(mht.view(1, 3)))\n",
        "\n",
        "# mht = torch.tensor([0.7337597, 0.26021487, -0.25838602]).cuda()\n",
        "# print(model.classify(mht.view(1, 3)))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-44e038373b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/model/light/model.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRruEe5Vy9tQ",
        "colab_type": "text"
      },
      "source": [
        "Get the vectors for K-Means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thgJDOsnyzpn",
        "colab_type": "code",
        "outputId": "4c1fbb89-71ec-4ac0-9af0-f17594d9d504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "def lstm_vectors(model, params, data_loader):\n",
        "  hidden_size = params['hidden_lstm_dim']\n",
        "  total_samples, vectors = 0, defaultdict(int)\n",
        "\n",
        "  for data in data_loader:\n",
        "    samples_in_batch = len(data['x'])\n",
        "    total_samples += samples_in_batch\n",
        "\n",
        "    ones = torch.ones(samples_in_batch).cuda()\n",
        "    h_0 = torch.zeros(1, samples_in_batch, hidden_size).cuda()\n",
        "    c_0 = torch.zeros(1, samples_in_batch, hidden_size).cuda()\n",
        "\n",
        "    for j in range(data['length'].max()):\n",
        "      (h_t, c_t), linear = model.vectors(data['x'][:, j : j + 1], ones, h_0, c_0)\n",
        "      h_0, c_0 = h_t, c_t\n",
        "\n",
        "      concat = torch.cat((h_t[-1,:,:], c_t[-1,:,:], linear), 1)\n",
        "      # The words are padded with zero and we should filter out the tensors\n",
        "      y1 = torch.ones_like(data['length'])\n",
        "      y2 = torch.zeros_like(data['length'])\n",
        "      indices = torch.where((j < data['length']), y1, y2).bool()\n",
        "      concat = concat[indices].cpu().detach().numpy()\n",
        "\n",
        "      for vector in concat:\n",
        "        vectors[tuple(vector)] += 1\n",
        "\n",
        "  initial_state = torch.zeros(hidden_size).cuda()\n",
        "  linear = model.linear_output(initial_state.view(1, 1, -1))\n",
        "  initial_state =  torch.cat((initial_state, initial_state, linear[-1, -1, :]), 0).cpu().detach().numpy()\n",
        "  vectors[tuple(initial_state)] += total_samples\n",
        "  return vectors, initial_state\n",
        "\n",
        "def truncate(n, decimals=14):\n",
        "  multiplier = 10 ** decimals\n",
        "  return int(n * multiplier) / multiplier\n",
        "\n",
        "def get_and_validate_lstm_vectors(model, params, data_loader, decimals=14):\n",
        "  vectors, initial_state = lstm_vectors(model, params, data_loader)\n",
        "  truncated_vectors = {}\n",
        "  for vector, count in vectors.items():\n",
        "    truncated_vectors[tuple([truncate(v, decimals=decimals) for v in vector])] = count\n",
        "  initial_state = tuple([truncate(v, decimals=decimals) for v in initial_state])\n",
        "  vectors = truncated_vectors\n",
        "\n",
        "  hidden_size, expected_vectors_count, samples_count = 0, 0, 0\n",
        "  for data in data_loader:\n",
        "    # for one word with length n, we should have n + 1 vectors \n",
        "    expected_vectors_count += (torch.sum(data['length']).item() + len(data['length']))\n",
        "    samples_count += len(data['x'])\n",
        "\n",
        "    _, (ht, ct) = model.lstm_hidden_states(data['x'], data['length'])\n",
        "    linear = model.linear_output(ht)\n",
        "\n",
        "    hidden_size = ht.size()[2]\n",
        "\n",
        "    for i in range(len(data['x'])):\n",
        "      vector = tuple(torch.cat((ht[-1, i, :], ct[-1, i, :], linear[-1, i, :]), 0).cpu().detach().numpy())\n",
        "      vector = tuple([truncate(v, decimals=decimals) for v in vector])\n",
        "      assert vectors[vector] >= 1\n",
        "\n",
        "  vectors_count = sum([ count for vector, count in vectors.items()])\n",
        "  assert vectors_count == expected_vectors_count\n",
        "\n",
        "  # each sample starts with the vector at time step 0\n",
        "  start = torch.zeros(hidden_size).cuda()\n",
        "  linear = model.linear_output(start.view(1, 1, -1))\n",
        "  start = torch.cat((torch.zeros(hidden_size * 2).cuda(), linear[-1, -1, :]), dim=0).cpu().detach().numpy()\n",
        "  start = tuple([truncate(v, decimals=decimals) for v in start])\n",
        "  assert vectors[start] >= samples_count\n",
        "\n",
        "  return vectors, initial_state\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path=\"/model/light/model.params\",\n",
        "    model_path=\"/model/light/model.pt\"\n",
        ")\n",
        "\n",
        "train_data = load_data(\n",
        "  DATA_DIR + \"/model/light/train.data.npy\",\n",
        "  DATA_DIR + \"/model/light/train.length.npy\",\n",
        "  DATA_DIR + \"/model/light/train.labels.npy\"\n",
        ")\n",
        "train_loader = tud.DataLoader(\n",
        "  NLDataset(train_data), batch_size=100, shuffle = False\n",
        ")\n",
        "\n",
        "t0 = time.time()\n",
        "#vectors = lstm_vectors(model, params, train_loader)\n",
        "vectors, initial_state = get_and_validate_lstm_vectors(model, params, train_loader, decimals=14)\n",
        "print(\"Elapsed: {:.2f}s\".format(time.time() - t0))\n",
        "print(\"Number of unique vectors {0:,}\".format(len(vectors)))\n",
        "print(\"Number of total vectors {0:,}\".format(sum(vectors.values())))\n",
        "\n",
        "with open(DATA_DIR + \"/kMeans/light+dense/vectors-train-14.txt\", \"w\") as f:\n",
        "  # intial state first\n",
        "  count = vectors.pop(initial_state)\n",
        "  f.write(str(count))\n",
        "  f.write(\"\\t\")\n",
        "  for i in range(len(initial_state)):\n",
        "    f.write(str(initial_state[i]))\n",
        "    f.write(\" \")\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "  for vector, count in vectors.items():\n",
        "    f.write(str(count))\n",
        "    f.write(\"\\t\")\n",
        "    for i in range(len(vector)):\n",
        "      f.write(str(vector[i]))\n",
        "      f.write(\" \")\n",
        "    f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed: 0.03s\n",
            "Number of unique vectors 275\n",
            "Number of total vectors 844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv_cI6JM8ISc",
        "colab_type": "text"
      },
      "source": [
        "Covariance matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmds55_VQwuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectors = []\n",
        "with open(DATA_DIR + \"/kMeans/light+dense/vectors-train-14.txt\", \"r\") as f:\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "      vec = line.split(\"\\t\")[1]\n",
        "      vectors.append([float(p) for p in vec.split(\" \")])\n",
        "\n",
        "numpy.savetxt(\n",
        "    DATA_DIR + \"/kMeans/light+dense/vectors-train-14-cov.txt\",\n",
        "    numpy.cov(numpy.array(vectors)),\n",
        "    delimiter=',', fmt='%1.3f'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2gsu41wawYy",
        "colab_type": "text"
      },
      "source": [
        "Load K centroids and build automata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCgRfLNa9uMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def closest_centroid_naive(centroids, t):\n",
        "  min_dist, min_dist_centroid = float(\"inf\"), -1\n",
        "\n",
        "  for i in range(len(centroids)):\n",
        "    dist = numpy.sum([(x_i - y_i) ** 2 for (x_i, y_i) in zip(centroids[i], t)])\n",
        "    if dist < min_dist:\n",
        "      min_dist, min_dist_centroid = dist, i\n",
        "\n",
        "  return min_dist_centroid\n",
        "\n",
        "def closest_centroid(centroids, t):\n",
        "  subtraction = centroids - t.repeat(centroids.size()[0], 1)\n",
        "  dist = torch.sum(subtraction * subtraction, dim=1)\n",
        "  return torch.argmin(dist, dim=0) # torch.min(dist)\n",
        "\n",
        "def test_closest_centroid():\n",
        "  rand_dim = random.randint(64, 128)\n",
        "  rand_centroids_count = random.randint(1024, 2048)\n",
        "  rand_centroids = torch.rand(rand_centroids_count, rand_dim)\n",
        "  rand_vectors_count = random.randint(64, 128)\n",
        "\n",
        "  for i in range(rand_vectors_count):\n",
        "    rand_vector = torch.rand(rand_dim)\n",
        "\n",
        "    actual = closest_centroid(rand_centroids, rand_vector).item()\n",
        "    expected = closest_centroid_naive(rand_centroids.cpu().detach().numpy(), rand_vector.cpu().detach().numpy())\n",
        "    assert actual == expected\n",
        "\n",
        "  for i in range(rand_centroids_count):\n",
        "    actual = closest_centroid(rand_centroids, rand_centroids[i]).item()\n",
        "    assert actual == i\n",
        "\n",
        "#test_closest_centroid()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChaHDQiPRSd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_centroids(file):\n",
        "  centroids = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        centroids.append([float(p) for p in line.split()])\n",
        "\n",
        "  print(\"Number of centroids: {0:,}\".format(len(centroids)))\n",
        "\n",
        "  return torch.tensor(centroids).cuda()\n",
        "\n",
        "def get_initial_state_centroid(assignmentsFile):\n",
        "  with open(assignmentsFile, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    return int(lines[0])\n",
        "\n",
        "def write_initial_state(f, initial_state_centroid):\n",
        "  f.write(str(initial_state_centroid))\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "def write_final_states(f, final_states):\n",
        "  for fs in final_states:\n",
        "    f.write(str(fs))\n",
        "    f.write(\" \")\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "def write_transitions(f, transitions):\n",
        "  for tr in transitions:\n",
        "    f.write(str(tr[0]) + \" \" + str(tr[1]) + \" \" + str(tr[2]))\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "def build_and_save_automaton(model, params, centroidsFile, assignmentsFile, automatonFile):\n",
        "  t0 = time.time()\n",
        "\n",
        "  centroids = read_centroids(centroidsFile)\n",
        "  transitions, final_states = automaton(model, params, centroids)\n",
        "\n",
        "  with open(automatonFile, \"w\") as f:\n",
        "      # initial state on the first line\n",
        "      write_initial_state(f, get_initial_state_centroid(assignmentsFile))\n",
        "      # final states separated with space on the second line\n",
        "      write_final_states(f, final_states)\n",
        "      # transitions, each on a separate line\n",
        "      write_transitions(f, transitions)\n",
        "\n",
        "  print(\"Number of tranitions {0:,}\".format(len(transitions)))\n",
        "  print(\"Number of final states {0:,}\".format(len(final_states)))\n",
        "  print(\"Elapsed: {:.2f}s\".format(time.time() - t0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qhx_Qs26Bnf",
        "colab_type": "code",
        "outputId": "6ac8e3ac-408f-4840-81c1-ee5d10c04629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "def automaton(model, params, centroids):\n",
        "\n",
        "  alphabet_size = params['alphabet_size']\n",
        "  hidden_lstm_dim = params['hidden_lstm_dim']\n",
        "\n",
        "  transitions = []\n",
        "  x = torch.tensor([[i] for i in range(1, alphabet_size + 1)]).cuda()\n",
        "  ones = torch.ones(alphabet_size).cuda()\n",
        "\n",
        "  for q1, centroid in enumerate(centroids):\n",
        "    h_t, c_t, _ = torch.split(centroid, hidden_lstm_dim, dim=0)\n",
        "    h_t = h_t.repeat(alphabet_size, 1).view(1, alphabet_size, hidden_lstm_dim)\n",
        "    c_t = c_t.repeat(alphabet_size, 1).view(1, alphabet_size, hidden_lstm_dim)\n",
        "\n",
        "    _, (h_t, c_t) = model.lstm_hidden_states(x, ones, h_t, c_t)\n",
        "    linear = model.linear_output(h_t)\n",
        "    concat = torch.cat((h_t[-1,:,:], c_t[-1,:,:], linear[-1,:,:]), 1)\n",
        "\n",
        "    for i in range(alphabet_size):\n",
        "      # TODO closest centroid to return vector with size alphabet_size\n",
        "      q2 = closest_centroid(centroids, concat[i])\n",
        "      transitions.append((q1, x[i][0].item(), q2.item()))\n",
        "\n",
        "  _, linear = torch.split(centroids, hidden_lstm_dim * 2, dim=1)\n",
        "  final_states = numpy.where(model.classify(linear) == 1)[0]\n",
        "\n",
        "  return transitions, final_states\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path = \"/model/light/model.params\",\n",
        "    model_path = \"/model/light/model.pt\"\n",
        ")\n",
        "\n",
        "build_and_save_automaton(\n",
        "    model, params,\n",
        "    DATA_DIR + \"/kMeans/light+dense/kmeans-centroids-k=275.txt\",\n",
        "    DATA_DIR + \"/kMeans/light+dense/kmeans-assignments-k=275.txt\",\n",
        "    DATA_DIR + \"/kMeans/light+dense/aut-k=275.txt\"\n",
        ")\n",
        "build_and_save_automaton(\n",
        "    model, params,\n",
        "    DATA_DIR + \"/kMeans/light+dense/kmeans-centroids-k=220.txt\",\n",
        "    DATA_DIR + \"/kMeans/light+dense/kmeans-assignments-k=220.txt\",\n",
        "    DATA_DIR + \"/kMeans/light+dense/aut-k=220.txt\"\n",
        ")\n",
        "build_and_save_automaton(\n",
        "    model, params,\n",
        "    DATA_DIR + \"/kMeans/light+dense/kmeans-centroids-k=137.txt\",\n",
        "    DATA_DIR + \"/kMeans/light+dense/kmeans-assignments-k=137.txt\",\n",
        "    DATA_DIR + \"/kMeans/light+dense/aut-k=137.txt\"\n",
        ")\n",
        "build_and_save_automaton(\n",
        "    model, params,\n",
        "    DATA_DIR + \"/kMeans/light+dense/kmeans-centroids-k=34.txt\",\n",
        "    DATA_DIR + \"/kMeans/light+dense/kmeans-assignments-k=34.txt\",\n",
        "    DATA_DIR + \"/kMeans/light+dense/aut-k=34.txt\"\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of centroids: 275\n",
            "Number of tranitions 3,575\n",
            "Number of final states 118\n",
            "Elapsed: 1.71s\n",
            "Number of centroids: 220\n",
            "Number of tranitions 2,860\n",
            "Number of final states 97\n",
            "Elapsed: 1.71s\n",
            "Number of centroids: 137\n",
            "Number of tranitions 1,781\n",
            "Number of final states 60\n",
            "Elapsed: 1.51s\n",
            "Number of centroids: 34\n",
            "Number of tranitions 442\n",
            "Number of final states 13\n",
            "Elapsed: 1.25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09bOipdnfZC4",
        "colab_type": "text"
      },
      "source": [
        "K Means Vectors Stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0_o_bfffbFT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "1e29d5cd-e05b-432b-dac9-436e86f5ab41"
      },
      "source": [
        "def read_vectors(file):\n",
        "  vectors = []\n",
        "  counts = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        count, vector = line.split(\"\\t\")\n",
        "        vectors.append([float(p) for p in vector.split()])\n",
        "        counts.append(int(count))\n",
        "\n",
        "  return vectors, counts\n",
        "\n",
        "def vectors_distances_stats(vectors, counts, model, params):\n",
        "  vectors = torch.tensor(vectors).cuda()\n",
        "  vectors.requires_grad = False\n",
        "\n",
        "  min_dist, min_i, min_j = float(\"inf\"), -1, -1\n",
        "  max_dist, max_i, max_j = -1, -1, -1\n",
        "  distances = []\n",
        "\n",
        "  for index, vector in enumerate(vectors):\n",
        "    if index == vectors.size()[0] - 1:\n",
        "      break\n",
        "\n",
        "    v = vector.clone()\n",
        "    vec = vectors[index + 1:, :]\n",
        "    vr = v.repeat(vec.size()[0], 1)\n",
        "\n",
        "    substract = vec - v\n",
        "    distance = torch.sum(substract * substract, dim=1)\n",
        "\n",
        "    min_d = torch.min(distance).item()\n",
        "    if min_d < min_dist:\n",
        "      min_dist = min_d\n",
        "      min_i = index\n",
        "      min_j = torch.argmin(distance).item() +  index + 1\n",
        "\n",
        "    max_d = torch.max(distance).item()\n",
        "    if max_d > max_dist:\n",
        "      max_dist = max_d\n",
        "      max_i = index\n",
        "      max_j = torch.argmax(distance).item() +  index + 1\n",
        "\n",
        "    distances.extend(list(distance.cpu().detach().numpy()))\n",
        "\n",
        "  print(\"Min distance between 2 vectors {}\".format(math.sqrt(min_dist)))\n",
        "  print(\"{} {} {}\".format(\n",
        "      counts[min_i],\n",
        "      model.classify(vectors[min_i].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "      list(vectors[min_i].cpu().detach().numpy())\n",
        "  ))\n",
        "  print(\"{} {} {}\".format(\n",
        "      counts[min_j],\n",
        "      model.classify(vectors[min_j].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "      list(vectors[min_j].cpu().detach().numpy())\n",
        "  ))\n",
        "\n",
        "  print(\"Max distance between 2 vectors {}\".format(math.sqrt(max_dist)))\n",
        "  print(\"{} {} {}\".format(\n",
        "      counts[max_i],\n",
        "      model.classify(vectors[max_i].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "      list(vectors[max_i].cpu().detach().numpy())\n",
        "  ))\n",
        "  print(\"{} {} {}\".format(\n",
        "      counts[max_j],\n",
        "      model.classify(vectors[max_j].split(2 * params['hidden_lstm_dim'])[1].view(1, 2))[0],\n",
        "      list(vectors[max_j].cpu().detach().numpy())\n",
        "  ))\n",
        "\n",
        "  distances = [math.sqrt(d) for d in distances]\n",
        "  print(\"Mean distance between 2 vectors {}\".format(statistics.mean(distances)))\n",
        "  print(\"Pvariance distance between 2 vectors {}\".format(statistics.pvariance(distances)))\n",
        "  print(\"Pstandard deviation distance between 2 vectors {}\".format(statistics.pstdev(distances)))\n",
        "\n",
        "model, params = load_model(\n",
        "    params_path = \"/model/light/model.params\",\n",
        "    model_path = \"/model/light/model.pt\"\n",
        ")\n",
        "vectors, counts = read_vectors(DATA_DIR + \"/kMeans/light+dense/vectors-train-14.txt\")\n",
        "vectors_distances_stats(vectors, counts, model, params)\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min distance between 2 vectors 0.07543317705400461\n",
            "1 0 [-0.11563141532165, 0.23888565613838, 0.93588505098126, -0.2087566898479, 0.42763257910031, 1.71088485441823, 8.6240765680341, -8.852301277923]\n",
            "1 0 [-0.134067898784, 0.22963834869523, 0.93469144923071, -0.24753898289186, 0.40476308233132, 1.7008349085364, 8.57853715017196, -8.81972210098049]\n",
            "Max distance between 2 vectors 28.654250766726786\n",
            "1 0 [0.82989520048775, 0.67207647285873, 0.9936942295665, 1.32699361771358, 1.25506608403218, 2.91521117365019, 10.86224494159302, -10.44765909636646]\n",
            "1 1 [-0.27483088396374, -0.45895874323433, -0.87158380838291, -0.32831196665657, -0.60252525647277, -1.39595878776956, -9.95047796261411, 8.4505117456059]\n",
            "Mean distance between 2 vectors 11.553862778240614\n",
            "Pvariance distance between 2 vectors 70.52928229746466\n",
            "Pstandard deviation distance between 2 vectors 8.398171366283535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbJkyOKKk2Oj",
        "colab_type": "text"
      },
      "source": [
        "K Means Results Stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOulJhZg6-1v",
        "colab_type": "code",
        "outputId": "cdc0e2d1-4ade-4fc5-8699-0e4b7e4bcc6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "def count_points_in_clusters(assignments):\n",
        "  count_points_in_clusters = Counter(assignments).values()\n",
        "\n",
        "  print(\"Min points in cluster {}\".format(min(count_points_in_clusters)))\n",
        "  print(\"Max points in cluster {}\".format(max(count_points_in_clusters)))\n",
        "\n",
        "  print(\"Mean points in cluster {}\".format(statistics.mean(count_points_in_clusters)))\n",
        "  print(\"Pvariance points in cluster {}\".format(statistics.pvariance(count_points_in_clusters)))\n",
        "  print(\"Pstandard deviation points in cluster {}\".format(statistics.pstdev(count_points_in_clusters)))\n",
        "  print()\n",
        "\n",
        "def distances_to_centroids(centroids, assignments, vectors):\n",
        "\n",
        "  assignments = torch.tensor(assignments).cuda()\n",
        "  vectors = torch.tensor(vectors).cuda()\n",
        "\n",
        "  embedding = nn.Embedding(centroids.size()[0], centroids.size()[1])\n",
        "  embedding.weight.requires_grad = False\n",
        "  embedding.weight.data = centroids\n",
        " \n",
        "  substract = vectors - embedding(assignments)\n",
        "  distance = torch.sum(substract * substract, dim=1)\n",
        "  distance = torch.sqrt(distance)\n",
        "  distance = distance.cpu().detach().numpy()\n",
        "\n",
        "  print(\"Min distance between centroid and vector {}\".format(numpy.min(distance)))\n",
        "  print(\"Max distance between centroid and vector {}\".format(numpy.max(distance)))\n",
        "\n",
        "  distance = list(distance)\n",
        "\n",
        "  print(\"Mean distance between centroid and vector {}\".format(statistics.mean(distance)))\n",
        "  print(\"Pvariance distance between centroid and vector {}\".format(statistics.pvariance(distance)))\n",
        "  print(\"Pstandard deviation distance between centroid and vector {}\".format(statistics.pstdev(distance)))\n",
        "  print()\n",
        "\n",
        "def k_means_stats(centroids, assignments, vectors):\n",
        "  count_points_in_clusters(assignments)\n",
        "  distances_to_centroids(centroids, assignments, vectors)\n",
        "\n",
        "def read_vectors(file):\n",
        "  vectors = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        count, vector = line.split(\"\\t\")\n",
        "        vectors.append([float(p) for p in vector.split()])\n",
        "\n",
        "  return vectors\n",
        "\n",
        "def read_assignments(file):\n",
        "  assignments = []\n",
        "  with open(file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      line = line.strip()\n",
        "      if line:\n",
        "        assignments.append(int(line))\n",
        "\n",
        "  return assignments\n",
        "\n",
        "assignments = read_assignments(DATA_DIR + \"/kMeans/light+dense/kmeans-assignments-k=34.txt\")\n",
        "centroids = read_centroids(DATA_DIR + \"/kMeans/light+dense/kmeans-centroids-k=34.txt\")\n",
        "vectors = read_vectors(DATA_DIR + \"/kMeans/light+dense/vectors-train-14.txt\")\n",
        "k_means_stats(centroids, assignments, vectors)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of centroids: 34\n",
            "Min points in cluster 1\n",
            "Max points in cluster 18\n",
            "Mean points in cluster 8.088235294117647\n",
            "Pvariance points in cluster 15.96280276816609\n",
            "Pstandard deviation points in cluster 3.9953476404646056\n",
            "\n",
            "Min distance between centroid and vector 1.3877787807814457e-17\n",
            "Max distance between centroid and vector 2.138036695208876\n",
            "Mean distance between centroid and vector 0.7126564690609364\n",
            "Pvariance distance between centroid and vector 0.12651805259193896\n",
            "Pstandard deviation distance between centroid and vector 0.3556937623742353\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}